[
  {
    "objectID": "posts/20240517-interesting-etymologies/index.html",
    "href": "posts/20240517-interesting-etymologies/index.html",
    "title": "Interesting Etymologies Running List",
    "section": "",
    "text": "This is a running list of etymologies that I think are neat."
  },
  {
    "objectID": "posts/20240517-interesting-etymologies/index.html#temujin",
    "href": "posts/20240517-interesting-etymologies/index.html#temujin",
    "title": "Interesting Etymologies Running List",
    "section": "Temujin",
    "text": "Temujin\nGenghis Khan was just a title; his real name was Temujin. Temujin is a Mongolic name that was probably borrowed from the Proto-Turkic *temürči meaning “blacksmith”. The -ci suffix denotes a person who works with something, in this case *temür, or iron. Modern Turkish’s “demirci”, also meaning “blacksmith”, is a descendent of this word and a cognate to Temujin. These names and their friends were and are still common throughout the region. For example, Timur (also known as Tamerlame), of the Timurid Empire, shares the same root—Timur, like Turkish demir, means iron."
  },
  {
    "objectID": "posts/20240517-interesting-etymologies/index.html#ordu",
    "href": "posts/20240517-interesting-etymologies/index.html#ordu",
    "title": "Interesting Etymologies Running List",
    "section": "Ordu",
    "text": "Ordu\nOrdu is the Turkish word for “army”. It actually has an English cognate, “horde”! Quite a few European languages seem to have picked this word up via contact with Central Asians. The most likely source appears to be Proto-Turkic, which makes sense given the role they played in Central Asian empires. But there’s another cognate to ordu that’s even more interesting: Urdu, the national language of Pakistan. The Mughal Empire of India established Persian as the official language of the empire, and the Hindustani dialect spoken in the military camps came to be referred to as “Zaban-e-Urdu” in Persian, or “language of the army”, eventually being shortened to just “Urdu”."
  },
  {
    "objectID": "posts/20240517-interesting-etymologies/index.html#rickshaw",
    "href": "posts/20240517-interesting-etymologies/index.html#rickshaw",
    "title": "Interesting Etymologies Running List",
    "section": "Rickshaw",
    "text": "Rickshaw\nRickshaws are small, hand-drawn passenger carts that were (and in some places, are) popular in Asia. It comes from the Japanese word 人力車 jinrikisha. But when it entered the English language, it actually already had a cognate, a word that came from the same root: wheel.\nFive thousand years, a group called the Yamnaya used carts and wagons on the Ukrainian Steppe. This group is generally accepted to be the source of the Indo-European Languages—a language family encompassing languages as diverse as English, Persian, and Hindi. Their word for wheel was probably something like *kʷékʷlos. This evolved over time to be something like *hwehwlą, in Proto-Germanic, the most recent common ancestor of languages like German, English, and Norwegian. This eventually became *hweōl in Old English and wheel today. That was journey that the word *kʷékʷlos took westward to arrive in English as wheel. But how did it arrive in Japanese as jinrikishaw?\nNot all of the Yamnaya people went west into Europe. Some went south into Iran and India, others went east to the edge of Mongolia. We call the earliest of these people the Afanasievo Culture, but we only know them through their archeological remains. One of the things we know about them, however, is that they used carts as early as ~3500 BCE. They are also likely to be the source of a group known as the Tocharians, that lived in the Tarim Basin in modern day Xinjiang, China. They spoke a family of languages known as Tocharian and were producing manuscripts in their languages as late as 500-800 AD. Between the Afanasievans and the Tocharians, there was almost certainly a continuous, cart-using Indo-European presence on the Chinese periphery for roughly 4000 years, a time period including the earliest known presence of wheeled vehicles in China.\nSome of these Indo-European speakers are likely the source of the Old Chinese word for wheeled vehicles, 車, which would have been pronounced *kʰlja around 1200 BC. This evolved into something like *t͡ɕʰia in Middle Chinese. That was borrowed into Japanese as 人力車 jinrikisha, or “human-power cart”, recognizably the same word we use in English today and a cognate with “wheel”.\nTestPAT"
  },
  {
    "objectID": "posts/20230719-getting-around-istanbul/index.html",
    "href": "posts/20230719-getting-around-istanbul/index.html",
    "title": "Getting Around Istanbul",
    "section": "",
    "text": "Summary:\n\nThe metro and ferries are the best public transit options\nGoogle Maps works, but Moovit has much better coverage for ferry routes\nAvoid relying on taxis unless absolutely necessary\nUber calls taxis and you will likely have an excessive wait\nAvoid minibuses (light blue buses on Google Maps)\n\nIf you don’t like my guide, you can read more about getting to and from the airport here\nIf you don’t like my guide, you can read more about public transit here\n\n\nGeneral Information\nIstanbul has a very well-developed public transit system. It spans the Bosphorous Strait to connect both sides of city and connects both passenger airports (IST and SAW) to the city center. Public transit, especially the ferries and metro, are the best way to get around the city due to rush hour traffic. The metro system covers all of the main touristic areas of the city and is easy to use. On weeknights most public transit shuts down around midnight, so plan on staying near your hotel if you want to go out late. Google Maps does a good job of providing routes and even has several privately owned airport bus companies listed. Almost all of the public transit can be accessed using an IstanbulKart that can be purchased from and reloaded at yellow kiosks in metro stations. In tourist areas, you’re more likely to find kiosks that have an English option. Each connection you use will incur a new charge, so make sure your IstanbulKart is loaded up. And importantly, most IstanbulKart machines cannot take bills larger than 100 TL! Each leg of a journey will probably cost ~10TL\n\n\nModes of Transit\n\nMetro\n\n\n\nIstanbul metro map\n\n\nThe metro system in Istanbul is clean and easy to use. It functions just like the metro lines in most major cities. Expect trains every 5-10 min in most cases. Trains run from 6:00 AM to midnight, but there is uninterrupted 24-hour service with trains every 30 min Friday and Saturday night. This should be one of your primary ways of getting around the city. There is only one metro line that crosses the Bosphorous Strait, the Marmaray (gray line). This line is more expensive.\n\n\nFerries\nFerries in Istanbul are fully integrated into the public transit network, so you can use your IstanbulKart to board them. They are very convenient, and should also be one of your primary modes of transportation. They are automatically included in Google Maps routes and marked with a blue boat sign. At many of the ferry stations, there will be several wharfs with different ferries. For the busiest ferries, there can even be two docks running the same route at staggered intervals. Make sure you are getting on the right one by checking the signs! Public ferry schedules can be found here: https://sehirhatlari.istanbul/en/timetables . There are also private ferries (which can still be accessed using your IstanbulKart), but these are not included in Google Maps. Roughly half of the ferries moving on major routes are operated by private companies. If you see a ferry on Google Maps that’s running on the :00 and :30 marks, there is probably another dock nearby with a private ferry that leaves on the :15 and :45 marks. Private ferries are included in the Moovit app.\n\n\nBuses\nThere are three types of buses in Istanbul that might be included in your route.\n\nStandard city buses: These are big yellow and black buses, and they have a yellow bus symbol on Google Maps. You pay when you get on. Most buses will stop running before midnight.\nMetrobuses: These have a beige bus symbol on Google Maps. These buses have their own lanes and move pretty quickly. You pay when you enter the station as if it were a metro.\nMinibuses: These are tiny death traps with no ventilation. They are marked with a light blue bus symbol on Google Maps. I recommend not using them because they only take cash and you have to tell them your destination.\n\n\n\nTaxis\nAvoid taxis unless absolutely necessary. They are notorious for scamming people, often do not speak English, and will not know how to get to your destination. Many taxis are cash only. If you have to take a taxi, make sure the meter is running as soon as you get in the cab, and pull up the route on your phone so that you see if they make any detours. There may be additional fees associated with crossing the bridges, but these should be small (~20 TL).\n\n\nGetting to/from the Airport\nBoth Istanbul airports (IST and SAW) are a ways outside the city, so it can take some time to get to the city center where your hotels probably are.\nThere are several ways you can get to/from the airport. The easiest way is going to be getting your hotel to call you an airport shuttle. Note that the wedding venue (A11 Hotel Bosphorus) provides an airport shuttle. You also might be able to save yourself a few dollars by booking a shuttle yourself with a private shuttle service such as https://airporttransfer.vip/ . This should cost ~€40. I’ve seen good reviews for this service, but I haven’t used it personally. Driving directly to your hotel should take ~45 min, but traffic can add up to an hour if you’re landing during rush hour.\nAnother way is to take an airport bus. There are private companies that offer reliable service between the airport and popular destinations in Istanbul. These run 24/7, but at 30-60 min intervals. The most popular is Havaist, which leaves from the -2 floor of the IST airport https://istanbul-international-airport.com/transportation/bus/ . If you are staying in Uskudar, you can take the Havaist bus to Kadikoy, followed by the metro from Kadikoy to Uskudar. This is two stops, but requires changing lines at the first stop. The total cost of this trip will be ~$7, not including buying an IstanbulKart to use the public transit. You can also take the Havaist bus to Besiktas and take a ferry from Besiktas to Uskudar.\nThe airports also connects directly to the metro system. You can connect to either the historic areas of Istanbul or Uskudar, but it will require several transfers, and take slightly longer than the Havaist bus. You can get to Uskudar by taking the M11-M7-M2-Marmaray route. Most other areas of the city—such as Galata and Eminonu—are accessible via the M2 line without crossing the Bosphorous using the Marmaray. Some of the stations are quite large, and it may not be desirable to lug around a ton of bags."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Daniel Geiszler",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nRetrieval Agumented Generation with DeepSeek\n\n\n\n\n\n\nmachine learning\n\n\npython\n\n\n\n\n\n\n\n\n\nJan 25, 2025\n\n\n6 min\n\n\n\n\n\n\n\nMaking Rolling Blog Posts\n\n\n\n\n\n\nprogramming\n\n\ncontinuous integration\n\n\n\n\n\n\n\n\n\nDec 21, 2024\n\n\n1 min\n\n\n\n\n\n\n\nMachine Learning Basics\n\n\n\n\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\nDec 19, 2024\n\n\n1 min\n\n\n\n\n\n\n\nLearning Pyro for Better Content Sorting\n\n\n\n\n\n\nmachine learning\n\n\npython\n\n\n\n\n\n\n\n\n\nDec 15, 2024\n\n\n10 min\n\n\n\n\n\n\n\nInteresting Etymologies Running List\n\n\n\n\n\n\nlinguistics\n\n\n\n\n\n\n\n\n\nMay 16, 2024\n\n\n1 min\n\n\n\n\n\n\n\nGetting Around Istanbul\n\n\n\n\n\n\nIstanbul\n\n\nTurkey\n\n\npublic transit\n\n\n\n\n\n\n\n\n\nJul 19, 2023\n\n\n6 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Daniel Geiszler",
    "section": "",
    "text": "Hi, I’m Danny 🧬\nI’m an EMBO Postdoctoral Fellow at Istanbul’s Koç University, trying to figure out how to maximize the PTM information we can get out of DIA data using machine learning and Bayesian statistics. My field of study is computational proteomics, with a specialization in post-translational modifications, especially open searches, glycosylation, and chemoproteomics.\nI did my Bioinformatics PhD at the University of Michigan in Alexey Nesvizhskii’s lab where I developed PTM-Shepherd and contributed to FragPipe, as well as helping in several PTM-related statistics and data science projects.\nIf you’re ever up for a friendly chat or potential collaboration, don’t hesitate to reach out."
  },
  {
    "objectID": "posts/20240517-interesting-etymologies/etymologies.html",
    "href": "posts/20240517-interesting-etymologies/etymologies.html",
    "title": "Daniel Geiszler",
    "section": "",
    "text": "This is a running list of etymologies that I think are neat."
  },
  {
    "objectID": "posts/20240517-interesting-etymologies/etymologies.html#temujin",
    "href": "posts/20240517-interesting-etymologies/etymologies.html#temujin",
    "title": "Daniel Geiszler",
    "section": "Temujin",
    "text": "Temujin\nGenghis Khan was just a title; his real name was Temujin. Temujin is a Mongolic name that was probably borrowed from the Proto-Turkic *temürči meaning “blacksmith”. The -ci suffix denotes a person who works with something, in this case *temür, or iron. Modern Turkish’s “demirci”, also meaning “blacksmith”, is a descendent of this word and a cognate to Temujin. These names and their friends were and are still common throughout the region. For example, Timur (also known as Tamerlame), of the Timurid Empire, shares the same root—Timur, like Turkish demir, means iron."
  },
  {
    "objectID": "posts/20240517-interesting-etymologies/etymologies.html#ordu",
    "href": "posts/20240517-interesting-etymologies/etymologies.html#ordu",
    "title": "Daniel Geiszler",
    "section": "Ordu",
    "text": "Ordu\nOrdu is the Turkish word for “army”. It actually has an English cognate, “horde”! Quite a few European languages seem to have picked this word up via contact with Central Asians. The most likely source appears to be Proto-Turkic, which makes sense given the role they played in Central Asian empires. But there’s another cognate to ordu that’s even more interesting: Urdu, the national language of Pakistan. The Mughal Empire of India established Persian as the official language of the empire, and the Hindustani dialect spoken in the military camps came to be referred to as “Zaban-e-Urdu” in Persian, or “language of the army”, eventually being shortened to just “Urdu”."
  },
  {
    "objectID": "posts/20240517-interesting-etymologies/etymologies.html#rickshaw",
    "href": "posts/20240517-interesting-etymologies/etymologies.html#rickshaw",
    "title": "Daniel Geiszler",
    "section": "Rickshaw",
    "text": "Rickshaw\nRickshaws are small, hand-drawn passenger carts that were (and in some places, are) popular in Asia. It comes from the Japanese word 人力車 jinrikisha. But when it entered the English language, it actually already had a cognate, a word that came from the same root: wheel.\nFive thousand years, a group called the Yamnaya used carts and wagons on the Ukrainian Steppe. This group is generally accepted to be the source of the Indo-European Languages—a language family encompassing languages as diverse as English, Persian, and Hindi. Their word for wheel was probably something like *kʷékʷlos. This evolved over time to be something like *hwehwlą, in Proto-Germanic, the most recent common ancestor of languages like German, English, and Norwegian. This eventually became *hweōl in Old English and wheel today. That was journey that the word *kʷékʷlos took westward to arrive in English as wheel. But how did it arrive in Japanese as jinrikishaw?\nNot all of the Yamnaya people went west into Europe. Some went south into Iran and India, others went east to the edge of Mongolia. We call the earliest of these people the Afanasievo Culture, but we only know them through their archeological remains. One of the things we know about them, however, is that they used carts as early as ~3500 BCE. They are also likely to be the source of a group known as the Tocharians, that lived in the Tarim Basin in modern day Xinjiang, China. They spoke a family of languages known as Tocharian and were producing manuscripts in their languages as late as 500-800 AD. Between the Afanasievans and the Tocharians, there was almost certainly a continuous, cart-using Indo-European presence on the Chinese periphery for roughly 4000 years, a time period including the earliest known presence of wheeled vehicles in China.\nSome of these Indo-European speakers are likely the source of the Old Chinese word for wheeled vehicles, 車, which would have been pronounced *kʰlja around 1200 BC. This evolved into something like *t͡ɕʰia in Middle Chinese. That was borrowed into Japanese as 人力車 jinrikisha, or “human-power cart”, recognizably the same word we use in English today and a cognate with “wheel”."
  },
  {
    "objectID": "dashboards.html",
    "href": "dashboards.html",
    "title": "Daniel Geiszler",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTraining Dashboard\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2024\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "dashboards/20241201-training-dashboard/index.html",
    "href": "dashboards/20241201-training-dashboard/index.html",
    "title": "Training Dashboard",
    "section": "",
    "text": "This is a dashboard I made to track progress in the gym. Because this website is hosted on github-pages, which only allows static content, the dashboard itself automatically deploys on Render whenever there are updates and is embedded here in an iframe. If the formatting here is poor, it can be visited directly. Render will also shut down any apps that are inactive, so it may take a minute to start up once you open this page.\nIf you’re interested in using this yourself, just follow these steps:\n\nRecord your workouts in Google Sheets. I record my workouts here. This sheet contains the minimum columns required for the dashboard to function.\nPublish your workouts to the web. Go to File &gt; Share &gt; Publish to the web on your Google Sheet. Choose Entire Document and select Comma-separated values (.csv) as the format. Click Publish and confirm.\nWhen you open the dashboard, copy-paste your link into the data url field and click “Load Data”.\n\nFeel free to suggest features or make changes yourself. Enjoy!"
  },
  {
    "objectID": "posts/20241215-testing-pyro/index.html",
    "href": "posts/20241215-testing-pyro/index.html",
    "title": "Learning Pyro for Better Content Sorting",
    "section": "",
    "text": "I’ve been looking for a reason to learn to use the Pyro library in Python for Markov Chain Monte Carlo (MCMC) simulations, and I’ve finally found one. This is me documenting how to learn to use it for future reference, and hopefully you’ll be able to get something out of it too.\nHave you ever sorted content on a website by “Top Ranked” or something similar, only do be inundated with a bunch of posts with a 100% rating but only a tiny number of reviews? It can make it frustrating or impossible to find “the best” of something when it’s hiding below hundreds or thousands of other things. If you’re going to rank by the average score for a piece of content, it’s inevitable that you’ll end up with some items near the top of the list that shouldn’t be. This happens because low sample sizes (e.g., a small number of reviews or upvotes) lead to a wide variety of estimates of the content’s “true” rating.\nThankfully, lots of smart companies have come up with better ways to rank their content! Reddit, for example, used to use a confidence interval around the score of the of a post (see the cpdef double _confidence function) to account for uncertainty in its “true” score. This has the effect of penalizing upvoted posts that have a low score with only a handful of upvotes, but it will also boost downvoted posts in the same situation. As the number of votes increases, the confidence in the post’s true rating also increases.\nOther companies seem to have an issue with this. For example, Google Maps has pretty limited options on what you can sort by, and instead only allows you to filter based on certain ranges of average review score. This still leaves the top results polluted by new, fake, or closed restaurants when you’re going out to eat. This isn’t a good system, and I frequently find myself with bad results. You have to go to an entirely different website to get decent results.\nFinding a better estimate of a restaurant’s “true rating” is a good use case for Pyro, a probabilistic programming language thats uses PyTorch. Pyro make it easy to find a restaurant’s “true rating” (posterior probability) given its current ratings (likelihood) and some assumptions about what its true rating is likely to be before any reviews are given (prior probability). These problems can be solved using MCMC simulations to estimate the posterior probability. PyTorch is supposed to make these sorts of problems easy, so I’m going to use it to estimate the posterior probabilities of some content and learn how to use Pyro along the way.\nInstalling PyTorch isn’t as simple as most other python packages, so I recommend looking at their website to do it (https://pytorch.org/). Beyond that, you’ll also need pandas and the pyro package (https://pyro.ai/).\n\n# import packages\n\nimport torch\nimport pyro\nimport pyro.distributions as dist\nimport pandas as pd\n\nfrom pyro.infer import MCMC, NUTS, SVI, Trace_ELBO\nfrom pyro.optim import Adam\n\nIf you installed PyTorch correctly and you have an nVidia GPU, you can run this to see if your GPU is available for PyTorch to use. This workbook should still work even if this isn’t true, instead running the simulations on your CPU. The first few will run on the default device, but I’ll switch to my GPU later.\n\nif torch.cuda.is_available():\n    print(\"CUDA is available. GPU is ready to be used.\")\nelse:\n    print(\"CUDA is not available. Running on CPU.\")\n\nCUDA is available. GPU is ready to be used.\n\n\nAs a toy example, let’s consider a restaurant or some other piece of content that has a single upvote in an upvote/downvote rating system. This will run on the default device regardless of your GPU setup.\n\n# Data: 1 thumb up out of 1 observation\ndata = torch.tensor([1.])\n\nWe first need to define a model that PyTorch will simulate.\n\nThe pyro.sample function here is given two arguments: the name of your sample and the prior distribution. In this case, we use the Beta(1,1) distribution as our prior. This draws a sample from the Beta(1,1) distribution for our prior. The beta distribution is a common prior when estimating probabilities because of both the posterior and the prior are defined over [0,1]. It is also a conjugate prior for the Binomial distribution, which is the distribution for a series of up-down votes. The Beta(1,1) is a uniform distribution, which indicates we have no information about the prior probability. In practice, Betas tend to be used more than Uniforms due to their flexibility, but the results should be the same regardless. It is also easy to interpret in our case because the Beta(a,b) parameters a and b correspond to pseudocounts for upvotes and downvotes for our content. This will come in handy later.\nThe pyro.plate function indicates that all of the observations are IID, and again takes two arguments: a name and the number of observations. We then use the pyro.sample function again, but this time the probability of success (in this case, an upvoted restaurants) is given by the the probability from the first step, and our observed data is used as our evidence.\n\n\n# Define the model\ndef beta_model(data):\n    # Prior distribution for the probability of a thumb up\n    prob_thumb_up = pyro.sample(\"prob_thumb_up\", dist.Beta(1, 1))\n    # Observing the data\n    with pyro.plate(\"data\", len(data)):\n        pyro.sample(\"obs\", dist.Bernoulli(prob_thumb_up), obs=data)\n\nTo simulate our posterior, we use NUTS (No U-turn Sampler). We then run our MCMC method to extract all of the posterior simulations. The NUTS sampler starts off slow and not giving useful information at the beginning, so we throw out the first warmup_steps.\n\n# Run sampling\nnuts_kernel = NUTS(beta_model)\nmcmc = MCMC(nuts_kernel, num_samples=100, warmup_steps=200)\nmcmc.run(data)\n\nWarmup:   0%|          | 0/300 [00:00, ?it/s]Warmup:   6%|▋         | 19/300 [00:00, 190.00it/s, step size=1.25e+00, acc. prob=0.751]Warmup:  15%|█▌        | 45/300 [00:00, 228.50it/s, step size=1.65e+00, acc. prob=0.777]Warmup:  25%|██▍       | 74/300 [00:00, 256.06it/s, step size=1.22e+00, acc. prob=0.782]Warmup:  36%|███▌      | 108/300 [00:00, 287.31it/s, step size=4.48e-01, acc. prob=0.777]Warmup:  48%|████▊     | 145/300 [00:00, 309.93it/s, step size=1.46e+00, acc. prob=0.783]Warmup:  59%|█████▊    | 176/300 [00:00, 306.81it/s, step size=6.46e-01, acc. prob=0.781]Sample:  69%|██████▉   | 208/300 [00:00, 309.12it/s, step size=1.04e+00, acc. prob=0.915]Sample:  83%|████████▎ | 249/300 [00:00, 338.71it/s, step size=1.04e+00, acc. prob=0.892]Sample:  97%|█████████▋| 290/300 [00:00, 357.34it/s, step size=1.04e+00, acc. prob=0.897]Sample: 100%|██████████| 300/300 [00:00, 317.22it/s, step size=1.04e+00, acc. prob=0.899]\n\n\nNow that our 100 samples have run in ~1 second, we can see what our posterior distribution looks like.\n\n# Extract samples\nsamples = mcmc.get_samples()\nprob_thumb_up_samples = samples['prob_thumb_up']\nprint(samples)\n\n# Make histogram of samples\nsamples_df = pd.DataFrame(samples)\nsamples_df.hist()\n\n{'prob_thumb_up': tensor([0.3754, 0.6458, 0.6215, 0.6673, 0.6825, 0.6825, 0.9285, 0.9551, 0.4132,\n        0.5960, 0.6367, 0.8789, 0.9242, 0.9909, 0.9961, 0.8353, 0.7794, 0.4894,\n        0.4894, 0.7527, 0.9925, 0.9846, 0.9204, 0.7989, 0.7989, 0.7631, 0.8936,\n        0.5362, 0.8810, 0.2876, 0.2743, 0.4360, 0.2989, 0.2143, 0.3124, 0.5030,\n        0.6797, 0.7506, 0.7535, 0.3841, 0.4743, 0.4743, 0.6214, 0.6214, 0.8456,\n        0.6840, 0.6840, 0.7425, 0.9353, 0.8336, 0.8336, 0.8424, 0.7697, 0.6447,\n        0.6352, 0.6352, 0.9111, 0.8721, 0.8918, 0.8699, 0.9374, 0.4371, 0.1400,\n        0.2007, 0.3737, 0.8435, 0.7996, 0.2103, 0.6420, 0.9197, 0.9983, 0.9518,\n        0.6668, 0.5557, 0.9482, 0.9872, 0.7521, 0.1814, 0.2036, 0.6258, 0.2069,\n        0.3711, 0.2595, 0.4180, 0.5293, 0.9503, 0.8273, 0.3834, 0.6513, 0.6072,\n        0.5900, 0.5900, 0.6927, 0.5713, 0.6686, 0.8526, 0.4008, 0.1775, 0.6810,\n        0.5947])}\n\n\narray([[&lt;Axes: title={'center': 'prob_thumb_up'}&gt;]], dtype=object)\n\n\n\n\n\n\n\n\n\nOur probability distribution looks like it’s multimodal, when it should be unimodal. That probably means the model didn’t converge. Let’s try upping the number of samples we use to see if we can get a better behaved distribution.\n\n# Run sampling\nnuts_kernel = NUTS(beta_model)\nmcmc = MCMC(nuts_kernel, num_samples=10000, warmup_steps=200)\nmcmc.run(data)\n\n# Extract samples\nsamples = mcmc.get_samples()\nprob_thumb_up_samples = samples['prob_thumb_up']\nprint(samples)\n\n# Make histogram of samples\nsamples_df = pd.DataFrame(samples)\nsamples_df.hist()\n\nWarmup:   0%|          | 0/10200 [00:00, ?it/s]Warmup:   0%|          | 27/10200 [00:00, 264.69it/s, step size=2.01e+00, acc. prob=0.770]Warmup:   1%|          | 58/10200 [00:00, 290.19it/s, step size=3.27e+00, acc. prob=0.786]Warmup:   1%|          | 91/10200 [00:00, 306.86it/s, step size=1.99e+00, acc. prob=0.787]Warmup:   1%|▏         | 133/10200 [00:00, 348.50it/s, step size=4.65e-01, acc. prob=0.782]Warmup:   2%|▏         | 168/10200 [00:00, 335.00it/s, step size=2.79e+00, acc. prob=0.780]Sample:   2%|▏         | 211/10200 [00:00, 365.04it/s, step size=1.62e+00, acc. prob=0.838]Sample:   2%|▏         | 250/10200 [00:00, 373.00it/s, step size=1.62e+00, acc. prob=0.869]Sample:   3%|▎         | 290/10200 [00:00, 379.08it/s, step size=1.62e+00, acc. prob=0.890]Sample:   3%|▎         | 330/10200 [00:00, 384.61it/s, step size=1.62e+00, acc. prob=0.890]Sample:   4%|▎         | 376/10200 [00:01, 405.12it/s, step size=1.62e+00, acc. prob=0.901]Sample:   4%|▍         | 418/10200 [00:01, 409.61it/s, step size=1.62e+00, acc. prob=0.897]Sample:   5%|▍         | 462/10200 [00:01, 415.74it/s, step size=1.62e+00, acc. prob=0.901]Sample:   5%|▍         | 504/10200 [00:01, 407.25it/s, step size=1.62e+00, acc. prob=0.897]Sample:   5%|▌         | 545/10200 [00:01, 400.98it/s, step size=1.62e+00, acc. prob=0.899]Sample:   6%|▌         | 587/10200 [00:01, 405.88it/s, step size=1.62e+00, acc. prob=0.893]Sample:   6%|▌         | 628/10200 [00:01, 398.88it/s, step size=1.62e+00, acc. prob=0.890]Sample:   7%|▋         | 668/10200 [00:01, 381.39it/s, step size=1.62e+00, acc. prob=0.891]Sample:   7%|▋         | 707/10200 [00:01, 376.34it/s, step size=1.62e+00, acc. prob=0.894]Sample:   7%|▋         | 749/10200 [00:01, 386.58it/s, step size=1.62e+00, acc. prob=0.892]Sample:   8%|▊         | 793/10200 [00:02, 400.26it/s, step size=1.62e+00, acc. prob=0.889]Sample:   8%|▊         | 834/10200 [00:02, 393.90it/s, step size=1.62e+00, acc. prob=0.887]Sample:   9%|▊         | 874/10200 [00:02, 395.66it/s, step size=1.62e+00, acc. prob=0.887]Sample:   9%|▉         | 915/10200 [00:02, 399.25it/s, step size=1.62e+00, acc. prob=0.888]Sample:   9%|▉         | 955/10200 [00:02, 395.89it/s, step size=1.62e+00, acc. prob=0.888]Sample:  10%|▉         | 1002/10200 [00:02, 416.54it/s, step size=1.62e+00, acc. prob=0.889]Sample:  10%|█         | 1044/10200 [00:02, 405.60it/s, step size=1.62e+00, acc. prob=0.889]Sample:  11%|█         | 1085/10200 [00:02, 405.69it/s, step size=1.62e+00, acc. prob=0.887]Sample:  11%|█         | 1126/10200 [00:02, 397.68it/s, step size=1.62e+00, acc. prob=0.889]Sample:  11%|█▏        | 1166/10200 [00:03, 394.01it/s, step size=1.62e+00, acc. prob=0.889]Sample:  12%|█▏        | 1206/10200 [00:03, 381.33it/s, step size=1.62e+00, acc. prob=0.889]Sample:  12%|█▏        | 1245/10200 [00:03, 378.52it/s, step size=1.62e+00, acc. prob=0.890]Sample:  13%|█▎        | 1291/10200 [00:03, 400.77it/s, step size=1.62e+00, acc. prob=0.892]Sample:  13%|█▎        | 1332/10200 [00:03, 397.67it/s, step size=1.62e+00, acc. prob=0.893]Sample:  13%|█▎        | 1372/10200 [00:03, 397.19it/s, step size=1.62e+00, acc. prob=0.893]Sample:  14%|█▍        | 1419/10200 [00:03, 418.31it/s, step size=1.62e+00, acc. prob=0.893]Sample:  14%|█▍        | 1467/10200 [00:03, 435.98it/s, step size=1.62e+00, acc. prob=0.893]Sample:  15%|█▍        | 1511/10200 [00:03, 428.88it/s, step size=1.62e+00, acc. prob=0.892]Sample:  15%|█▌        | 1554/10200 [00:03, 422.55it/s, step size=1.62e+00, acc. prob=0.893]Sample:  16%|█▌        | 1597/10200 [00:04, 413.15it/s, step size=1.62e+00, acc. prob=0.893]Sample:  16%|█▌        | 1639/10200 [00:04, 412.72it/s, step size=1.62e+00, acc. prob=0.893]Sample:  16%|█▋        | 1681/10200 [00:04, 408.30it/s, step size=1.62e+00, acc. prob=0.894]Sample:  17%|█▋        | 1722/10200 [00:04, 400.66it/s, step size=1.62e+00, acc. prob=0.893]Sample:  17%|█▋        | 1763/10200 [00:04, 398.74it/s, step size=1.62e+00, acc. prob=0.893]Sample:  18%|█▊        | 1803/10200 [00:04, 399.10it/s, step size=1.62e+00, acc. prob=0.894]Sample:  18%|█▊        | 1845/10200 [00:04, 402.87it/s, step size=1.62e+00, acc. prob=0.893]Sample:  19%|█▊        | 1889/10200 [00:04, 411.36it/s, step size=1.62e+00, acc. prob=0.894]Sample:  19%|█▉        | 1931/10200 [00:04, 413.90it/s, step size=1.62e+00, acc. prob=0.894]Sample:  19%|█▉        | 1973/10200 [00:04, 398.03it/s, step size=1.62e+00, acc. prob=0.895]Sample:  20%|█▉        | 2016/10200 [00:05, 407.23it/s, step size=1.62e+00, acc. prob=0.895]Sample:  20%|██        | 2057/10200 [00:05, 406.21it/s, step size=1.62e+00, acc. prob=0.895]Sample:  21%|██        | 2101/10200 [00:05, 412.50it/s, step size=1.62e+00, acc. prob=0.895]Sample:  21%|██        | 2143/10200 [00:05, 412.59it/s, step size=1.62e+00, acc. prob=0.894]Sample:  21%|██▏       | 2185/10200 [00:05, 388.49it/s, step size=1.62e+00, acc. prob=0.894]Sample:  22%|██▏       | 2225/10200 [00:05, 387.36it/s, step size=1.62e+00, acc. prob=0.894]Sample:  22%|██▏       | 2264/10200 [00:05, 377.98it/s, step size=1.62e+00, acc. prob=0.894]Sample:  23%|██▎       | 2305/10200 [00:05, 385.98it/s, step size=1.62e+00, acc. prob=0.894]Sample:  23%|██▎       | 2345/10200 [00:05, 388.91it/s, step size=1.62e+00, acc. prob=0.893]Sample:  23%|██▎       | 2385/10200 [00:06, 382.12it/s, step size=1.62e+00, acc. prob=0.893]Sample:  24%|██▍       | 2425/10200 [00:06, 387.26it/s, step size=1.62e+00, acc. prob=0.893]Sample:  24%|██▍       | 2466/10200 [00:06, 393.28it/s, step size=1.62e+00, acc. prob=0.893]Sample:  25%|██▍       | 2506/10200 [00:06, 380.72it/s, step size=1.62e+00, acc. prob=0.893]Sample:  25%|██▍       | 2546/10200 [00:06, 385.25it/s, step size=1.62e+00, acc. prob=0.893]Sample:  25%|██▌       | 2585/10200 [00:06, 380.00it/s, step size=1.62e+00, acc. prob=0.893]Sample:  26%|██▌       | 2627/10200 [00:06, 391.60it/s, step size=1.62e+00, acc. prob=0.893]Sample:  26%|██▌       | 2667/10200 [00:06, 385.03it/s, step size=1.62e+00, acc. prob=0.893]Sample:  27%|██▋       | 2706/10200 [00:06, 384.66it/s, step size=1.62e+00, acc. prob=0.893]Sample:  27%|██▋       | 2749/10200 [00:06, 394.45it/s, step size=1.62e+00, acc. prob=0.893]Sample:  27%|██▋       | 2797/10200 [00:07, 419.48it/s, step size=1.62e+00, acc. prob=0.894]Sample:  28%|██▊       | 2840/10200 [00:07, 409.83it/s, step size=1.62e+00, acc. prob=0.893]Sample:  28%|██▊       | 2882/10200 [00:07, 409.22it/s, step size=1.62e+00, acc. prob=0.893]Sample:  29%|██▊       | 2926/10200 [00:07, 416.97it/s, step size=1.62e+00, acc. prob=0.893]Sample:  29%|██▉       | 2969/10200 [00:07, 419.56it/s, step size=1.62e+00, acc. prob=0.894]Sample:  30%|██▉       | 3015/10200 [00:07, 427.52it/s, step size=1.62e+00, acc. prob=0.895]Sample:  30%|██▉       | 3058/10200 [00:07, 411.31it/s, step size=1.62e+00, acc. prob=0.894]Sample:  30%|███       | 3100/10200 [00:07, 401.12it/s, step size=1.62e+00, acc. prob=0.895]Sample:  31%|███       | 3141/10200 [00:07, 372.85it/s, step size=1.62e+00, acc. prob=0.894]Sample:  31%|███       | 3179/10200 [00:08, 367.77it/s, step size=1.62e+00, acc. prob=0.894]Sample:  32%|███▏      | 3217/10200 [00:08, 366.55it/s, step size=1.62e+00, acc. prob=0.895]Sample:  32%|███▏      | 3254/10200 [00:08, 352.66it/s, step size=1.62e+00, acc. prob=0.895]Sample:  32%|███▏      | 3290/10200 [00:08, 349.84it/s, step size=1.62e+00, acc. prob=0.895]Sample:  33%|███▎      | 3326/10200 [00:08, 340.25it/s, step size=1.62e+00, acc. prob=0.895]Sample:  33%|███▎      | 3361/10200 [00:08, 334.33it/s, step size=1.62e+00, acc. prob=0.895]Sample:  33%|███▎      | 3395/10200 [00:08, 322.38it/s, step size=1.62e+00, acc. prob=0.894]Sample:  34%|███▎      | 3428/10200 [00:08, 324.49it/s, step size=1.62e+00, acc. prob=0.894]Sample:  34%|███▍      | 3468/10200 [00:08, 343.01it/s, step size=1.62e+00, acc. prob=0.894]Sample:  34%|███▍      | 3509/10200 [00:09, 362.22it/s, step size=1.62e+00, acc. prob=0.893]Sample:  35%|███▍      | 3548/10200 [00:09, 368.17it/s, step size=1.62e+00, acc. prob=0.893]Sample:  35%|███▌      | 3585/10200 [00:09, 363.85it/s, step size=1.62e+00, acc. prob=0.893]Sample:  36%|███▌      | 3623/10200 [00:09, 366.43it/s, step size=1.62e+00, acc. prob=0.893]Sample:  36%|███▌      | 3665/10200 [00:09, 378.48it/s, step size=1.62e+00, acc. prob=0.893]Sample:  36%|███▋      | 3703/10200 [00:09, 371.31it/s, step size=1.62e+00, acc. prob=0.893]Sample:  37%|███▋      | 3742/10200 [00:09, 375.11it/s, step size=1.62e+00, acc. prob=0.893]Sample:  37%|███▋      | 3784/10200 [00:09, 387.12it/s, step size=1.62e+00, acc. prob=0.893]Sample:  37%|███▋      | 3823/10200 [00:09, 379.06it/s, step size=1.62e+00, acc. prob=0.893]Sample:  38%|███▊      | 3865/10200 [00:09, 388.72it/s, step size=1.62e+00, acc. prob=0.893]Sample:  38%|███▊      | 3904/10200 [00:10, 369.64it/s, step size=1.62e+00, acc. prob=0.894]Sample:  39%|███▊      | 3942/10200 [00:10, 355.84it/s, step size=1.62e+00, acc. prob=0.893]Sample:  39%|███▉      | 3978/10200 [00:10, 335.44it/s, step size=1.62e+00, acc. prob=0.893]Sample:  39%|███▉      | 4014/10200 [00:10, 339.37it/s, step size=1.62e+00, acc. prob=0.893]Sample:  40%|███▉      | 4049/10200 [00:10, 339.49it/s, step size=1.62e+00, acc. prob=0.893]Sample:  40%|████      | 4084/10200 [00:10, 340.54it/s, step size=1.62e+00, acc. prob=0.893]Sample:  40%|████      | 4119/10200 [00:10, 337.02it/s, step size=1.62e+00, acc. prob=0.893]Sample:  41%|████      | 4153/10200 [00:10, 266.50it/s, step size=1.62e+00, acc. prob=0.893]Sample:  41%|████      | 4189/10200 [00:11, 287.93it/s, step size=1.62e+00, acc. prob=0.893]Sample:  41%|████▏     | 4224/10200 [00:11, 302.37it/s, step size=1.62e+00, acc. prob=0.894]Sample:  42%|████▏     | 4260/10200 [00:11, 316.29it/s, step size=1.62e+00, acc. prob=0.893]Sample:  42%|████▏     | 4299/10200 [00:11, 335.70it/s, step size=1.62e+00, acc. prob=0.894]Sample:  43%|████▎     | 4341/10200 [00:11, 357.48it/s, step size=1.62e+00, acc. prob=0.893]Sample:  43%|████▎     | 4378/10200 [00:11, 357.99it/s, step size=1.62e+00, acc. prob=0.893]Sample:  43%|████▎     | 4420/10200 [00:11, 372.65it/s, step size=1.62e+00, acc. prob=0.894]Sample:  44%|████▎     | 4460/10200 [00:11, 373.01it/s, step size=1.62e+00, acc. prob=0.894]Sample:  44%|████▍     | 4498/10200 [00:11, 368.64it/s, step size=1.62e+00, acc. prob=0.894]Sample:  44%|████▍     | 4536/10200 [00:11, 354.53it/s, step size=1.62e+00, acc. prob=0.894]Sample:  45%|████▍     | 4574/10200 [00:12, 359.59it/s, step size=1.62e+00, acc. prob=0.894]Sample:  45%|████▌     | 4616/10200 [00:12, 375.56it/s, step size=1.62e+00, acc. prob=0.894]Sample:  46%|████▌     | 4660/10200 [00:12, 390.82it/s, step size=1.62e+00, acc. prob=0.894]Sample:  46%|████▌     | 4700/10200 [00:12, 385.63it/s, step size=1.62e+00, acc. prob=0.894]Sample:  46%|████▋     | 4740/10200 [00:12, 386.44it/s, step size=1.62e+00, acc. prob=0.894]Sample:  47%|████▋     | 4781/10200 [00:12, 392.15it/s, step size=1.62e+00, acc. prob=0.894]Sample:  47%|████▋     | 4821/10200 [00:12, 383.22it/s, step size=1.62e+00, acc. prob=0.894]Sample:  48%|████▊     | 4860/10200 [00:12, 365.14it/s, step size=1.62e+00, acc. prob=0.893]Sample:  48%|████▊     | 4897/10200 [00:12, 359.43it/s, step size=1.62e+00, acc. prob=0.893]Sample:  48%|████▊     | 4938/10200 [00:13, 373.72it/s, step size=1.62e+00, acc. prob=0.893]Sample:  49%|████▉     | 4980/10200 [00:13, 385.92it/s, step size=1.62e+00, acc. prob=0.893]Sample:  49%|████▉     | 5019/10200 [00:13, 374.54it/s, step size=1.62e+00, acc. prob=0.893]Sample:  50%|████▉     | 5057/10200 [00:13, 366.65it/s, step size=1.62e+00, acc. prob=0.893]Sample:  50%|████▉     | 5094/10200 [00:13, 362.43it/s, step size=1.62e+00, acc. prob=0.893]Sample:  50%|█████     | 5133/10200 [00:13, 368.98it/s, step size=1.62e+00, acc. prob=0.893]Sample:  51%|█████     | 5174/10200 [00:13, 377.58it/s, step size=1.62e+00, acc. prob=0.893]Sample:  51%|█████     | 5217/10200 [00:13, 390.56it/s, step size=1.62e+00, acc. prob=0.892]Sample:  52%|█████▏    | 5257/10200 [00:13, 386.54it/s, step size=1.62e+00, acc. prob=0.892]Sample:  52%|█████▏    | 5296/10200 [00:13, 387.54it/s, step size=1.62e+00, acc. prob=0.892]Sample:  52%|█████▏    | 5335/10200 [00:14, 377.19it/s, step size=1.62e+00, acc. prob=0.892]Sample:  53%|█████▎    | 5373/10200 [00:14, 373.24it/s, step size=1.62e+00, acc. prob=0.892]Sample:  53%|█████▎    | 5419/10200 [00:14, 398.39it/s, step size=1.62e+00, acc. prob=0.892]Sample:  54%|█████▎    | 5463/10200 [00:14, 408.56it/s, step size=1.62e+00, acc. prob=0.892]Sample:  54%|█████▍    | 5504/10200 [00:14, 405.95it/s, step size=1.62e+00, acc. prob=0.892]Sample:  54%|█████▍    | 5545/10200 [00:14, 407.14it/s, step size=1.62e+00, acc. prob=0.892]Sample:  55%|█████▍    | 5586/10200 [00:14, 404.40it/s, step size=1.62e+00, acc. prob=0.892]Sample:  55%|█████▌    | 5627/10200 [00:14, 404.86it/s, step size=1.62e+00, acc. prob=0.892]Sample:  56%|█████▌    | 5668/10200 [00:14, 395.84it/s, step size=1.62e+00, acc. prob=0.892]Sample:  56%|█████▌    | 5708/10200 [00:14, 386.90it/s, step size=1.62e+00, acc. prob=0.892]Sample:  56%|█████▋    | 5747/10200 [00:15, 386.67it/s, step size=1.62e+00, acc. prob=0.892]Sample:  57%|█████▋    | 5790/10200 [00:15, 396.56it/s, step size=1.62e+00, acc. prob=0.892]Sample:  57%|█████▋    | 5830/10200 [00:15, 389.59it/s, step size=1.62e+00, acc. prob=0.892]Sample:  58%|█████▊    | 5876/10200 [00:15, 407.67it/s, step size=1.62e+00, acc. prob=0.892]Sample:  58%|█████▊    | 5917/10200 [00:15, 405.42it/s, step size=1.62e+00, acc. prob=0.892]Sample:  58%|█████▊    | 5963/10200 [00:15, 420.17it/s, step size=1.62e+00, acc. prob=0.892]Sample:  59%|█████▉    | 6006/10200 [00:15, 421.46it/s, step size=1.62e+00, acc. prob=0.892]Sample:  59%|█████▉    | 6049/10200 [00:15, 409.55it/s, step size=1.62e+00, acc. prob=0.892]Sample:  60%|█████▉    | 6091/10200 [00:15, 386.82it/s, step size=1.62e+00, acc. prob=0.893]Sample:  60%|██████    | 6130/10200 [00:16, 373.11it/s, step size=1.62e+00, acc. prob=0.893]Sample:  60%|██████    | 6171/10200 [00:16, 382.77it/s, step size=1.62e+00, acc. prob=0.893]Sample:  61%|██████    | 6210/10200 [00:16, 376.29it/s, step size=1.62e+00, acc. prob=0.893]Sample:  61%|██████▏   | 6248/10200 [00:16, 369.98it/s, step size=1.62e+00, acc. prob=0.893]Sample:  62%|██████▏   | 6290/10200 [00:16, 382.04it/s, step size=1.62e+00, acc. prob=0.893]Sample:  62%|██████▏   | 6330/10200 [00:16, 382.81it/s, step size=1.62e+00, acc. prob=0.893]Sample:  62%|██████▎   | 6375/10200 [00:16, 399.90it/s, step size=1.62e+00, acc. prob=0.893]Sample:  63%|██████▎   | 6416/10200 [00:16, 401.27it/s, step size=1.62e+00, acc. prob=0.893]Sample:  63%|██████▎   | 6458/10200 [00:16, 406.29it/s, step size=1.62e+00, acc. prob=0.893]Sample:  64%|██████▎   | 6500/10200 [00:16, 408.95it/s, step size=1.62e+00, acc. prob=0.893]Sample:  64%|██████▍   | 6541/10200 [00:17, 404.48it/s, step size=1.62e+00, acc. prob=0.894]Sample:  65%|██████▍   | 6582/10200 [00:17, 404.25it/s, step size=1.62e+00, acc. prob=0.894]Sample:  65%|██████▍   | 6624/10200 [00:17, 404.13it/s, step size=1.62e+00, acc. prob=0.894]Sample:  65%|██████▌   | 6666/10200 [00:17, 408.36it/s, step size=1.62e+00, acc. prob=0.894]Sample:  66%|██████▌   | 6707/10200 [00:17, 406.43it/s, step size=1.62e+00, acc. prob=0.894]Sample:  66%|██████▌   | 6750/10200 [00:17, 410.97it/s, step size=1.62e+00, acc. prob=0.894]Sample:  67%|██████▋   | 6792/10200 [00:17, 395.57it/s, step size=1.62e+00, acc. prob=0.894]Sample:  67%|██████▋   | 6832/10200 [00:17, 388.73it/s, step size=1.62e+00, acc. prob=0.893]Sample:  67%|██████▋   | 6871/10200 [00:17, 386.88it/s, step size=1.62e+00, acc. prob=0.893]Sample:  68%|██████▊   | 6912/10200 [00:18, 391.31it/s, step size=1.62e+00, acc. prob=0.893]Sample:  68%|██████▊   | 6955/10200 [00:18, 402.22it/s, step size=1.62e+00, acc. prob=0.893]Sample:  69%|██████▊   | 7001/10200 [00:18, 416.09it/s, step size=1.62e+00, acc. prob=0.893]Sample:  69%|██████▉   | 7043/10200 [00:18, 410.61it/s, step size=1.62e+00, acc. prob=0.894]Sample:  69%|██████▉   | 7085/10200 [00:18, 393.50it/s, step size=1.62e+00, acc. prob=0.893]Sample:  70%|██████▉   | 7125/10200 [00:18, 389.62it/s, step size=1.62e+00, acc. prob=0.894]Sample:  70%|███████   | 7168/10200 [00:18, 397.76it/s, step size=1.62e+00, acc. prob=0.894]Sample:  71%|███████   | 7212/10200 [00:18, 406.48it/s, step size=1.62e+00, acc. prob=0.894]Sample:  71%|███████   | 7253/10200 [00:18, 384.11it/s, step size=1.62e+00, acc. prob=0.894]Sample:  71%|███████▏  | 7292/10200 [00:18, 378.33it/s, step size=1.62e+00, acc. prob=0.894]Sample:  72%|███████▏  | 7331/10200 [00:19, 371.14it/s, step size=1.62e+00, acc. prob=0.894]Sample:  72%|███████▏  | 7372/10200 [00:19, 379.37it/s, step size=1.62e+00, acc. prob=0.894]Sample:  73%|███████▎  | 7411/10200 [00:19, 378.17it/s, step size=1.62e+00, acc. prob=0.894]Sample:  73%|███████▎  | 7449/10200 [00:19, 365.05it/s, step size=1.62e+00, acc. prob=0.894]Sample:  73%|███████▎  | 7489/10200 [00:19, 371.80it/s, step size=1.62e+00, acc. prob=0.894]Sample:  74%|███████▍  | 7531/10200 [00:19, 385.61it/s, step size=1.62e+00, acc. prob=0.894]Sample:  74%|███████▍  | 7574/10200 [00:19, 397.13it/s, step size=1.62e+00, acc. prob=0.894]Sample:  75%|███████▍  | 7618/10200 [00:19, 406.61it/s, step size=1.62e+00, acc. prob=0.894]Sample:  75%|███████▌  | 7659/10200 [00:19, 404.05it/s, step size=1.62e+00, acc. prob=0.894]Sample:  76%|███████▌  | 7703/10200 [00:20, 413.38it/s, step size=1.62e+00, acc. prob=0.894]Sample:  76%|███████▌  | 7745/10200 [00:20, 411.69it/s, step size=1.62e+00, acc. prob=0.894]Sample:  76%|███████▋  | 7787/10200 [00:20, 409.25it/s, step size=1.62e+00, acc. prob=0.894]Sample:  77%|███████▋  | 7837/10200 [00:20, 433.35it/s, step size=1.62e+00, acc. prob=0.894]Sample:  77%|███████▋  | 7881/10200 [00:20, 427.75it/s, step size=1.62e+00, acc. prob=0.894]Sample:  78%|███████▊  | 7924/10200 [00:20, 419.82it/s, step size=1.62e+00, acc. prob=0.894]Sample:  78%|███████▊  | 7967/10200 [00:20, 400.67it/s, step size=1.62e+00, acc. prob=0.894]Sample:  79%|███████▊  | 8008/10200 [00:20, 402.17it/s, step size=1.62e+00, acc. prob=0.894]Sample:  79%|███████▉  | 8049/10200 [00:20, 398.63it/s, step size=1.62e+00, acc. prob=0.894]Sample:  79%|███████▉  | 8089/10200 [00:20, 397.95it/s, step size=1.62e+00, acc. prob=0.894]Sample:  80%|███████▉  | 8129/10200 [00:21, 392.84it/s, step size=1.62e+00, acc. prob=0.894]Sample:  80%|████████  | 8169/10200 [00:21, 392.05it/s, step size=1.62e+00, acc. prob=0.894]Sample:  80%|████████  | 8210/10200 [00:21, 396.96it/s, step size=1.62e+00, acc. prob=0.894]Sample:  81%|████████  | 8250/10200 [00:21, 395.53it/s, step size=1.62e+00, acc. prob=0.894]Sample:  81%|████████▏ | 8290/10200 [00:21, 389.63it/s, step size=1.62e+00, acc. prob=0.894]Sample:  82%|████████▏ | 8332/10200 [00:21, 397.35it/s, step size=1.62e+00, acc. prob=0.894]Sample:  82%|████████▏ | 8374/10200 [00:21, 404.00it/s, step size=1.62e+00, acc. prob=0.894]Sample:  82%|████████▎ | 8415/10200 [00:21, 386.35it/s, step size=1.62e+00, acc. prob=0.894]Sample:  83%|████████▎ | 8456/10200 [00:21, 389.77it/s, step size=1.62e+00, acc. prob=0.894]Sample:  83%|████████▎ | 8499/10200 [00:22, 400.26it/s, step size=1.62e+00, acc. prob=0.894]Sample:  84%|████████▎ | 8540/10200 [00:22, 397.67it/s, step size=1.62e+00, acc. prob=0.894]Sample:  84%|████████▍ | 8581/10200 [00:22, 397.32it/s, step size=1.62e+00, acc. prob=0.894]Sample:  85%|████████▍ | 8623/10200 [00:22, 401.59it/s, step size=1.62e+00, acc. prob=0.893]Sample:  85%|████████▍ | 8664/10200 [00:22, 399.98it/s, step size=1.62e+00, acc. prob=0.893]Sample:  85%|████████▌ | 8708/10200 [00:22, 409.31it/s, step size=1.62e+00, acc. prob=0.894]Sample:  86%|████████▌ | 8753/10200 [00:22, 421.26it/s, step size=1.62e+00, acc. prob=0.894]Sample:  86%|████████▌ | 8796/10200 [00:22, 414.05it/s, step size=1.62e+00, acc. prob=0.894]Sample:  87%|████████▋ | 8838/10200 [00:22, 409.70it/s, step size=1.62e+00, acc. prob=0.894]Sample:  87%|████████▋ | 8880/10200 [00:22, 401.12it/s, step size=1.62e+00, acc. prob=0.894]Sample:  88%|████████▊ | 8930/10200 [00:23, 427.69it/s, step size=1.62e+00, acc. prob=0.894]Sample:  88%|████████▊ | 8973/10200 [00:23, 412.68it/s, step size=1.62e+00, acc. prob=0.894]Sample:  88%|████████▊ | 9015/10200 [00:23, 399.82it/s, step size=1.62e+00, acc. prob=0.894]Sample:  89%|████████▉ | 9056/10200 [00:23, 388.44it/s, step size=1.62e+00, acc. prob=0.894]Sample:  89%|████████▉ | 9098/10200 [00:23, 393.97it/s, step size=1.62e+00, acc. prob=0.894]Sample:  90%|████████▉ | 9140/10200 [00:23, 397.96it/s, step size=1.62e+00, acc. prob=0.894]Sample:  90%|█████████ | 9185/10200 [00:23, 410.54it/s, step size=1.62e+00, acc. prob=0.894]Sample:  90%|█████████ | 9227/10200 [00:23, 410.24it/s, step size=1.62e+00, acc. prob=0.894]Sample:  91%|█████████ | 9273/10200 [00:23, 422.27it/s, step size=1.62e+00, acc. prob=0.894]Sample:  91%|█████████▏| 9317/10200 [00:24, 426.21it/s, step size=1.62e+00, acc. prob=0.894]Sample:  92%|█████████▏| 9360/10200 [00:24, 397.68it/s, step size=1.62e+00, acc. prob=0.893]Sample:  92%|█████████▏| 9403/10200 [00:24, 403.91it/s, step size=1.62e+00, acc. prob=0.893]Sample:  93%|█████████▎| 9444/10200 [00:24, 399.28it/s, step size=1.62e+00, acc. prob=0.893]Sample:  93%|█████████▎| 9485/10200 [00:24, 391.17it/s, step size=1.62e+00, acc. prob=0.893]Sample:  93%|█████████▎| 9529/10200 [00:24, 404.61it/s, step size=1.62e+00, acc. prob=0.893]Sample:  94%|█████████▍| 9570/10200 [00:24, 395.16it/s, step size=1.62e+00, acc. prob=0.893]Sample:  94%|█████████▍| 9610/10200 [00:24, 396.55it/s, step size=1.62e+00, acc. prob=0.893]Sample:  95%|█████████▍| 9654/10200 [00:24, 405.64it/s, step size=1.62e+00, acc. prob=0.893]Sample:  95%|█████████▌| 9695/10200 [00:24, 405.82it/s, step size=1.62e+00, acc. prob=0.893]Sample:  95%|█████████▌| 9736/10200 [00:25, 399.80it/s, step size=1.62e+00, acc. prob=0.894]Sample:  96%|█████████▌| 9777/10200 [00:25, 400.96it/s, step size=1.62e+00, acc. prob=0.894]Sample:  96%|█████████▋| 9822/10200 [00:25, 412.91it/s, step size=1.62e+00, acc. prob=0.894]Sample:  97%|█████████▋| 9864/10200 [00:25, 397.47it/s, step size=1.62e+00, acc. prob=0.894]Sample:  97%|█████████▋| 9906/10200 [00:25, 403.80it/s, step size=1.62e+00, acc. prob=0.894]Sample:  98%|█████████▊| 9947/10200 [00:25, 404.02it/s, step size=1.62e+00, acc. prob=0.894]Sample:  98%|█████████▊| 9988/10200 [00:25, 398.77it/s, step size=1.62e+00, acc. prob=0.894]Sample:  98%|█████████▊| 10028/10200 [00:25, 395.45it/s, step size=1.62e+00, acc. prob=0.894]Sample:  99%|█████████▊| 10068/10200 [00:25, 391.07it/s, step size=1.62e+00, acc. prob=0.894]Sample:  99%|█████████▉| 10108/10200 [00:26, 385.07it/s, step size=1.62e+00, acc. prob=0.894]Sample:  99%|█████████▉| 10147/10200 [00:26, 384.27it/s, step size=1.62e+00, acc. prob=0.894]Sample: 100%|█████████▉| 10186/10200 [00:26, 384.22it/s, step size=1.62e+00, acc. prob=0.894]Sample: 100%|██████████| 10200/10200 [00:26, 388.54it/s, step size=1.62e+00, acc. prob=0.894]\n\n\n{'prob_thumb_up': tensor([0.2494, 0.4574, 0.4574,  ..., 0.4894, 0.7621, 0.7621])}\n\n\narray([[&lt;Axes: title={'center': 'prob_thumb_up'}&gt;]], dtype=object)\n\n\n\n\n\n\n\n\n\nThis distribution looks much more regular. From this, we can calculate the confidence interval for our restaurant’s “true rating”.\n\n# Compute confidence interval\nconfidence_interval = torch.quantile(prob_thumb_up_samples, torch.tensor([0.025, 0.975]))\n\nprint(f\"Estimated probability of thumb up: {prob_thumb_up_samples.mean().item():.4f}\")\nprint(f\"95% confidence interval: {confidence_interval.tolist()}\")\n\nEstimated probability of thumb up: 0.6661\n95% confidence interval: [0.15158675611019135, 0.9878186583518982]\n\n\nWe should also probably check to make sure that our estimates are valid. This particular example is trivial because of our choice of distribution and prior. Because we used a conjugate prior (Beta) for our observed data (Binomial/Bernoulli), we have a closed form solution for our expected result. I mentioned before that the a and b parameters for the Beta(a,b) distribution are pseudocounts for successes (thumbs up) and failures (thumbs down), respectively. That means that if our prior distribution has the form Beta(1,1) and we have observed 1 additional success, our posterior distribution has the form Beta(2,1).\nYou can see slide 12 of David Maracek’s excellent lecture on Beta-Bernoulli distributions to see what this looks like, or you can draw the Beta(2,1) distribution yourself using this applet.\nThe shape of our simulated posterior looks almost exactly like our expected output. Success!\nHow a content rating system wants to use this information would be up to them. In Reddit’s case, it looks like they used an 80% bound to rank their posts. If you want your ranking to be much stricter, you could use the lower bound of the credible interval, which would push new entries to the bottom of the ranking. You could also increase the number of pseudocounts in your Beta prior to something like the mean value of upvotes and downvotes, which may be more appropriate if you want new content to be ranked “average” until proven otherwise.\nIn some cases, it may be more appropriate to do this in a regression framework so that you can control for other variables. I’m going to repeat this analysis using logistic regression, which models the log-odds of a success. I’ll also do it on the GPU (although for small samples and simple models this may not be necessary due to the overhead incurred by moving data to your GPU).\nFirst we’ll check to see if we can use our GPU. If we can, we’ll create our data on our GPU. All of the data needs to be in one place for this to work and it will default to your CPU, so you’ll need to explicitly put everything on your GPU. A GPU will actually be a bit slower for this use case due to the overhead incurred, but it’s a good reference for larger models.\n\n# Check for CUDA availability and prepare data\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    # Create data tensor on GPU\n    data = torch.tensor([1.]).to(device)\n    print(\"Data moved to GPU.\")\nelse:\n    device = torch.device(\"cpu\")\n    data = torch.tensor([1.]).to(device)\n    print(\"CUDA not available, running on CPU.\")\n    \n# Validate devices\ntorch.cuda.get_device_name(torch.cuda.current_device())\n\nData moved to GPU.\n\n\n'NVIDIA GeForce RTX 3050 Laptop GPU'\n\n\nNext we’ll define our model. This model is going to look different from the previous one. In our logistic regression, the intercept will give us the log-odds of our restaurant’s true probability of thumbs up. This parameter is no longer bounded on [0,1], which means the Beta distribution is no longer appropriate. There are a few options here. * A normal distribution with mean 0 (p = 0.5 means the odds are (0.5) / (1 - 0.5) = 1, and log(1) = 0. This is a common choice, but choosing the variance is tricky because there is no closed form solution, so we would have to pick one based on how informative we want it to be. * A Cauchy distribution with mean 0 (for the same reason) and scale 2.5. This is based on the recommendation of Gelman et al (2008) (http://www.stat.columbia.edu/~gelman/research/published/priors11.pdf). There is still no way to properly estimate the variance of the intercept, so our choice is again going to affect our outcomes.\n\ndef logistic_model(data):  \n    # Create parameters for the distribution on the correct device\n    mean = torch.tensor(0., device=device)  # Ensure mean is a tensor on GPU\n    scale = torch.tensor(2.5, device=device)  # Ensure standard deviation is a tensor on GPU\n    \n    # Create the Cauchy distribution with parameters on the correct device\n    prior_dist = dist.Cauchy(mean, scale)\n    \n    # Sample intercept from the distribution\n    intercept = pyro.sample(\"intercept\", prior_dist)\n    \n    # Observing the data with Bernoulli likelihood\n    with pyro.plate(\"data_plate\", len(data)):\n        pyro.sample(\"obs\", dist.Bernoulli(logits=intercept), obs=data)\n\nThen we repeat our sampling. I repeated this a few times before settling on 2000 samples for decent convergence.\n\n#NUTS sampler\nnuts_kernel = NUTS(logistic_model)\nmcmc = MCMC(nuts_kernel, num_samples=2000, warmup_steps=200)\nmcmc.run(data)\n\nWarmup:   0%|          | 0/2200 [00:00, ?it/s]Warmup:   0%|          | 1/2200 [00:00,  5.85it/s, step size=2.84e+01, acc. prob=0.993]Warmup:   0%|          | 9/2200 [00:00, 39.26it/s, step size=5.64e+00, acc. prob=0.755]Warmup:   1%|          | 14/2200 [00:00, 39.59it/s, step size=4.16e+00, acc. prob=0.764]Warmup:   1%|          | 19/2200 [00:00, 36.82it/s, step size=4.07e+00, acc. prob=0.772]Warmup:   1%|          | 27/2200 [00:00, 45.20it/s, step size=1.69e+00, acc. prob=0.767]Warmup:   1%|▏         | 32/2200 [00:00, 30.57it/s, step size=2.52e+00, acc. prob=0.776]Warmup:   2%|▏         | 40/2200 [00:01, 40.26it/s, step size=4.15e+00, acc. prob=0.784]Warmup:   2%|▏         | 53/2200 [00:01, 49.08it/s, step size=3.20e+00, acc. prob=0.785]Warmup:   3%|▎         | 59/2200 [00:01, 41.82it/s, step size=5.02e+00, acc. prob=0.789]Warmup:   3%|▎         | 67/2200 [00:01, 46.31it/s, step size=4.19e+00, acc. prob=0.789]Warmup:   4%|▎         | 77/2200 [00:01, 56.37it/s, step size=8.66e+00, acc. prob=0.795]Warmup:   4%|▍         | 85/2200 [00:01, 61.27it/s, step size=1.92e+01, acc. prob=0.800]Warmup:   4%|▍         | 92/2200 [00:01, 55.54it/s, step size=1.73e+00, acc. prob=0.786]Warmup:   5%|▍         | 102/2200 [00:02, 64.68it/s, step size=6.58e-02, acc. prob=0.778]Warmup:   5%|▌         | 110/2200 [00:02, 58.45it/s, step size=3.03e-01, acc. prob=0.786]Warmup:   5%|▌         | 119/2200 [00:02, 62.69it/s, step size=2.33e-01, acc. prob=0.786]Warmup:   6%|▌         | 127/2200 [00:02, 64.13it/s, step size=7.26e-02, acc. prob=0.783]Warmup:   6%|▌         | 135/2200 [00:02, 67.45it/s, step size=4.17e-02, acc. prob=0.782]Warmup:   6%|▋         | 143/2200 [00:02, 69.80it/s, step size=9.40e-02, acc. prob=0.785]Warmup:   7%|▋         | 151/2200 [00:02, 61.78it/s, step size=1.17e+00, acc. prob=0.774]Warmup:   7%|▋         | 159/2200 [00:02, 63.30it/s, step size=4.65e-01, acc. prob=0.776]Warmup:   8%|▊         | 170/2200 [00:03, 69.77it/s, step size=3.14e-01, acc. prob=0.776]Warmup:   8%|▊         | 179/2200 [00:03, 74.09it/s, step size=4.07e-01, acc. prob=0.777]Warmup:   8%|▊         | 187/2200 [00:03, 62.42it/s, step size=3.11e-01, acc. prob=0.777]Warmup:   9%|▉         | 201/2200 [00:03, 80.38it/s, step size=4.39e-01, acc. prob=1.000]Sample:  10%|▉         | 210/2200 [00:03, 77.26it/s, step size=4.39e-01, acc. prob=0.948]Sample:  10%|▉         | 219/2200 [00:03, 70.39it/s, step size=4.39e-01, acc. prob=0.952]Sample:  10%|█         | 231/2200 [00:03, 81.01it/s, step size=4.39e-01, acc. prob=0.953]Sample:  11%|█         | 240/2200 [00:04, 79.10it/s, step size=4.39e-01, acc. prob=0.946]Sample:  11%|█▏        | 249/2200 [00:04, 59.81it/s, step size=4.39e-01, acc. prob=0.942]Sample:  12%|█▏        | 256/2200 [00:04, 46.50it/s, step size=4.39e-01, acc. prob=0.946]Sample:  12%|█▏        | 262/2200 [00:04, 46.30it/s, step size=4.39e-01, acc. prob=0.945]Sample:  12%|█▏        | 272/2200 [00:04, 56.92it/s, step size=4.39e-01, acc. prob=0.943]Sample:  13%|█▎        | 281/2200 [00:04, 63.88it/s, step size=4.39e-01, acc. prob=0.949]Sample:  13%|█▎        | 291/2200 [00:04, 71.15it/s, step size=4.39e-01, acc. prob=0.950]Sample:  14%|█▎        | 301/2200 [00:05, 78.19it/s, step size=4.39e-01, acc. prob=0.949]Sample:  14%|█▍        | 310/2200 [00:05, 77.04it/s, step size=4.39e-01, acc. prob=0.949]Sample:  15%|█▍        | 321/2200 [00:05, 83.51it/s, step size=4.39e-01, acc. prob=0.953]Sample:  15%|█▌        | 330/2200 [00:05, 80.71it/s, step size=4.39e-01, acc. prob=0.947]Sample:  15%|█▌        | 339/2200 [00:05, 50.00it/s, step size=4.39e-01, acc. prob=0.950]Sample:  16%|█▌        | 346/2200 [00:05, 47.14it/s, step size=4.39e-01, acc. prob=0.949]Sample:  16%|█▌        | 355/2200 [00:06, 54.32it/s, step size=4.39e-01, acc. prob=0.946]Sample:  16%|█▋        | 362/2200 [00:06, 55.18it/s, step size=4.39e-01, acc. prob=0.944]Sample:  17%|█▋        | 369/2200 [00:06, 55.28it/s, step size=4.39e-01, acc. prob=0.943]Sample:  17%|█▋        | 376/2200 [00:06, 45.76it/s, step size=4.39e-01, acc. prob=0.941]Sample:  17%|█▋        | 382/2200 [00:06, 42.53it/s, step size=4.39e-01, acc. prob=0.942]Sample:  18%|█▊        | 387/2200 [00:06, 41.75it/s, step size=4.39e-01, acc. prob=0.943]Sample:  18%|█▊        | 395/2200 [00:06, 48.05it/s, step size=4.39e-01, acc. prob=0.945]Sample:  18%|█▊        | 401/2200 [00:07, 50.44it/s, step size=4.39e-01, acc. prob=0.945]Sample:  19%|█▊        | 409/2200 [00:07, 56.44it/s, step size=4.39e-01, acc. prob=0.946]Sample:  19%|█▉        | 418/2200 [00:07, 63.38it/s, step size=4.39e-01, acc. prob=0.946]Sample:  19%|█▉        | 425/2200 [00:07, 53.83it/s, step size=4.39e-01, acc. prob=0.945]Sample:  20%|█▉        | 432/2200 [00:07, 44.45it/s, step size=4.39e-01, acc. prob=0.945]Sample:  20%|█▉        | 437/2200 [00:07, 35.65it/s, step size=4.39e-01, acc. prob=0.944]Sample:  20%|██        | 446/2200 [00:08, 45.50it/s, step size=4.39e-01, acc. prob=0.945]Sample:  21%|██        | 454/2200 [00:08, 51.66it/s, step size=4.39e-01, acc. prob=0.945]Sample:  21%|██        | 462/2200 [00:08, 56.94it/s, step size=4.39e-01, acc. prob=0.946]Sample:  21%|██▏       | 469/2200 [00:08, 55.64it/s, step size=4.39e-01, acc. prob=0.946]Sample:  22%|██▏       | 476/2200 [00:08, 55.36it/s, step size=4.39e-01, acc. prob=0.947]Sample:  22%|██▏       | 482/2200 [00:08, 56.15it/s, step size=4.39e-01, acc. prob=0.947]Sample:  22%|██▏       | 490/2200 [00:08, 60.88it/s, step size=4.39e-01, acc. prob=0.947]Sample:  23%|██▎       | 497/2200 [00:09, 41.18it/s, step size=4.39e-01, acc. prob=0.948]Sample:  23%|██▎       | 503/2200 [00:09, 42.70it/s, step size=4.39e-01, acc. prob=0.948]Sample:  23%|██▎       | 512/2200 [00:09, 52.04it/s, step size=4.39e-01, acc. prob=0.948]Sample:  24%|██▎       | 519/2200 [00:09, 47.54it/s, step size=4.39e-01, acc. prob=0.948]Sample:  24%|██▍       | 529/2200 [00:09, 58.19it/s, step size=4.39e-01, acc. prob=0.947]Sample:  24%|██▍       | 537/2200 [00:09, 58.56it/s, step size=4.39e-01, acc. prob=0.947]Sample:  25%|██▍       | 544/2200 [00:09, 51.41it/s, step size=4.39e-01, acc. prob=0.948]Sample:  25%|██▌       | 550/2200 [00:09, 51.93it/s, step size=4.39e-01, acc. prob=0.948]Sample:  25%|██▌       | 556/2200 [00:10, 52.71it/s, step size=4.39e-01, acc. prob=0.949]Sample:  26%|██▌       | 563/2200 [00:10, 55.48it/s, step size=4.39e-01, acc. prob=0.947]Sample:  26%|██▌       | 569/2200 [00:10, 49.20it/s, step size=4.39e-01, acc. prob=0.945]Sample:  26%|██▌       | 575/2200 [00:10, 45.69it/s, step size=4.39e-01, acc. prob=0.946]Sample:  27%|██▋       | 587/2200 [00:10, 61.82it/s, step size=4.39e-01, acc. prob=0.945]Sample:  27%|██▋       | 597/2200 [00:10, 68.72it/s, step size=4.39e-01, acc. prob=0.944]Sample:  28%|██▊       | 606/2200 [00:10, 71.22it/s, step size=4.39e-01, acc. prob=0.944]Sample:  28%|██▊       | 614/2200 [00:10, 65.69it/s, step size=4.39e-01, acc. prob=0.945]Sample:  28%|██▊       | 624/2200 [00:11, 72.58it/s, step size=4.39e-01, acc. prob=0.943]Sample:  29%|██▊       | 632/2200 [00:11, 71.36it/s, step size=4.39e-01, acc. prob=0.943]Sample:  29%|██▉       | 640/2200 [00:11, 60.66it/s, step size=4.39e-01, acc. prob=0.943]Sample:  29%|██▉       | 648/2200 [00:11, 64.90it/s, step size=4.39e-01, acc. prob=0.944]Sample:  30%|██▉       | 657/2200 [00:11, 70.73it/s, step size=4.39e-01, acc. prob=0.944]Sample:  30%|███       | 666/2200 [00:11, 75.39it/s, step size=4.39e-01, acc. prob=0.944]Sample:  31%|███       | 674/2200 [00:11, 71.77it/s, step size=4.39e-01, acc. prob=0.944]Sample:  31%|███       | 682/2200 [00:11, 72.61it/s, step size=4.39e-01, acc. prob=0.944]Sample:  31%|███▏      | 690/2200 [00:12, 74.62it/s, step size=4.39e-01, acc. prob=0.944]Sample:  32%|███▏      | 698/2200 [00:12, 74.05it/s, step size=4.39e-01, acc. prob=0.944]Sample:  32%|███▏      | 706/2200 [00:12, 54.84it/s, step size=4.39e-01, acc. prob=0.944]Sample:  32%|███▏      | 713/2200 [00:12, 55.15it/s, step size=4.39e-01, acc. prob=0.943]Sample:  33%|███▎      | 720/2200 [00:12, 52.98it/s, step size=4.39e-01, acc. prob=0.943]Sample:  33%|███▎      | 726/2200 [00:12, 49.36it/s, step size=4.39e-01, acc. prob=0.944]Sample:  33%|███▎      | 732/2200 [00:13, 37.30it/s, step size=4.39e-01, acc. prob=0.944]Sample:  34%|███▎      | 737/2200 [00:13, 37.19it/s, step size=4.39e-01, acc. prob=0.944]Sample:  34%|███▎      | 742/2200 [00:13, 33.80it/s, step size=4.39e-01, acc. prob=0.944]Sample:  34%|███▍      | 746/2200 [00:13, 31.58it/s, step size=4.39e-01, acc. prob=0.945]Sample:  34%|███▍      | 755/2200 [00:13, 43.32it/s, step size=4.39e-01, acc. prob=0.945]Sample:  35%|███▍      | 763/2200 [00:13, 50.55it/s, step size=4.39e-01, acc. prob=0.945]Sample:  35%|███▌      | 771/2200 [00:13, 57.19it/s, step size=4.39e-01, acc. prob=0.945]Sample:  35%|███▌      | 778/2200 [00:13, 54.12it/s, step size=4.39e-01, acc. prob=0.945]Sample:  36%|███▌      | 784/2200 [00:14, 33.10it/s, step size=4.39e-01, acc. prob=0.946]Sample:  36%|███▌      | 789/2200 [00:14, 27.62it/s, step size=4.39e-01, acc. prob=0.945]Sample:  36%|███▌      | 796/2200 [00:14, 34.25it/s, step size=4.39e-01, acc. prob=0.946]Sample:  37%|███▋      | 807/2200 [00:14, 47.44it/s, step size=4.39e-01, acc. prob=0.945]Sample:  37%|███▋      | 818/2200 [00:14, 59.66it/s, step size=4.39e-01, acc. prob=0.944]Sample:  38%|███▊      | 826/2200 [00:15, 59.92it/s, step size=4.39e-01, acc. prob=0.944]Sample:  38%|███▊      | 834/2200 [00:15, 57.86it/s, step size=4.39e-01, acc. prob=0.943]Sample:  38%|███▊      | 841/2200 [00:15, 56.15it/s, step size=4.39e-01, acc. prob=0.942]Sample:  39%|███▊      | 848/2200 [00:15, 58.88it/s, step size=4.39e-01, acc. prob=0.942]Sample:  39%|███▉      | 858/2200 [00:15, 66.65it/s, step size=4.39e-01, acc. prob=0.942]Sample:  39%|███▉      | 866/2200 [00:15, 69.34it/s, step size=4.39e-01, acc. prob=0.942]Sample:  40%|███▉      | 876/2200 [00:15, 77.04it/s, step size=4.39e-01, acc. prob=0.943]Sample:  40%|████      | 885/2200 [00:15, 75.82it/s, step size=4.39e-01, acc. prob=0.942]Sample:  41%|████      | 893/2200 [00:16, 64.33it/s, step size=4.39e-01, acc. prob=0.942]Sample:  41%|████      | 903/2200 [00:16, 70.45it/s, step size=4.39e-01, acc. prob=0.941]Sample:  41%|████▏     | 911/2200 [00:16, 69.53it/s, step size=4.39e-01, acc. prob=0.942]Sample:  42%|████▏     | 919/2200 [00:16, 71.11it/s, step size=4.39e-01, acc. prob=0.942]Sample:  42%|████▏     | 927/2200 [00:16, 68.96it/s, step size=4.39e-01, acc. prob=0.942]Sample:  42%|████▎     | 935/2200 [00:16, 67.00it/s, step size=4.39e-01, acc. prob=0.942]Sample:  43%|████▎     | 942/2200 [00:16, 54.34it/s, step size=4.39e-01, acc. prob=0.942]Sample:  43%|████▎     | 953/2200 [00:16, 66.44it/s, step size=4.39e-01, acc. prob=0.942]Sample:  44%|████▎     | 961/2200 [00:17, 66.97it/s, step size=4.39e-01, acc. prob=0.942]Sample:  44%|████▍     | 969/2200 [00:17, 66.57it/s, step size=4.39e-01, acc. prob=0.942]Sample:  44%|████▍     | 978/2200 [00:17, 71.07it/s, step size=4.39e-01, acc. prob=0.941]Sample:  45%|████▍     | 986/2200 [00:17, 58.90it/s, step size=4.39e-01, acc. prob=0.941]Sample:  45%|████▌     | 993/2200 [00:17, 57.41it/s, step size=4.39e-01, acc. prob=0.941]Sample:  45%|████▌     | 1000/2200 [00:17, 58.18it/s, step size=4.39e-01, acc. prob=0.941]Sample:  46%|████▌     | 1007/2200 [00:17, 57.10it/s, step size=4.39e-01, acc. prob=0.941]Sample:  46%|████▌     | 1014/2200 [00:18, 58.92it/s, step size=4.39e-01, acc. prob=0.941]Sample:  47%|████▋     | 1024/2200 [00:18, 67.87it/s, step size=4.39e-01, acc. prob=0.941]Sample:  47%|████▋     | 1032/2200 [00:18, 66.51it/s, step size=4.39e-01, acc. prob=0.941]Sample:  47%|████▋     | 1041/2200 [00:18, 68.86it/s, step size=4.39e-01, acc. prob=0.941]Sample:  48%|████▊     | 1048/2200 [00:18, 68.98it/s, step size=4.39e-01, acc. prob=0.942]Sample:  48%|████▊     | 1055/2200 [00:18, 65.10it/s, step size=4.39e-01, acc. prob=0.941]Sample:  48%|████▊     | 1062/2200 [00:18, 53.10it/s, step size=4.39e-01, acc. prob=0.942]Sample:  49%|████▊     | 1068/2200 [00:19, 43.09it/s, step size=4.39e-01, acc. prob=0.942]Sample:  49%|████▉     | 1077/2200 [00:19, 52.75it/s, step size=4.39e-01, acc. prob=0.942]Sample:  49%|████▉     | 1085/2200 [00:19, 58.58it/s, step size=4.39e-01, acc. prob=0.942]Sample:  50%|████▉     | 1092/2200 [00:19, 53.51it/s, step size=4.39e-01, acc. prob=0.941]Sample:  50%|████▉     | 1098/2200 [00:19, 54.27it/s, step size=4.39e-01, acc. prob=0.941]Sample:  50%|█████     | 1105/2200 [00:19, 57.14it/s, step size=4.39e-01, acc. prob=0.941]Sample:  51%|█████     | 1113/2200 [00:19, 62.72it/s, step size=4.39e-01, acc. prob=0.941]Sample:  51%|█████     | 1120/2200 [00:19, 51.76it/s, step size=4.39e-01, acc. prob=0.940]Sample:  51%|█████     | 1126/2200 [00:20, 29.82it/s, step size=4.39e-01, acc. prob=0.941]Sample:  51%|█████▏    | 1131/2200 [00:21, 16.46it/s, step size=4.39e-01, acc. prob=0.941]Sample:  52%|█████▏    | 1135/2200 [00:21, 16.83it/s, step size=4.39e-01, acc. prob=0.941]Sample:  52%|█████▏    | 1139/2200 [00:21, 18.87it/s, step size=4.39e-01, acc. prob=0.941]Sample:  52%|█████▏    | 1145/2200 [00:21, 24.21it/s, step size=4.39e-01, acc. prob=0.941]Sample:  52%|█████▏    | 1150/2200 [00:21, 28.07it/s, step size=4.39e-01, acc. prob=0.941]Sample:  52%|█████▎    | 1155/2200 [00:21, 32.03it/s, step size=4.39e-01, acc. prob=0.942]Sample:  53%|█████▎    | 1160/2200 [00:21, 34.84it/s, step size=4.39e-01, acc. prob=0.942]Sample:  53%|█████▎    | 1167/2200 [00:21, 39.58it/s, step size=4.39e-01, acc. prob=0.942]Sample:  54%|█████▎    | 1178/2200 [00:22, 55.24it/s, step size=4.39e-01, acc. prob=0.942]Sample:  54%|█████▍    | 1185/2200 [00:22, 57.04it/s, step size=4.39e-01, acc. prob=0.942]Sample:  54%|█████▍    | 1192/2200 [00:22, 51.79it/s, step size=4.39e-01, acc. prob=0.942]Sample:  54%|█████▍    | 1198/2200 [00:22, 44.64it/s, step size=4.39e-01, acc. prob=0.942]Sample:  55%|█████▍    | 1203/2200 [00:22, 44.88it/s, step size=4.39e-01, acc. prob=0.942]Sample:  55%|█████▍    | 1208/2200 [00:22, 41.06it/s, step size=4.39e-01, acc. prob=0.943]Sample:  55%|█████▌    | 1216/2200 [00:22, 49.24it/s, step size=4.39e-01, acc. prob=0.943]Sample:  56%|█████▌    | 1223/2200 [00:23, 53.27it/s, step size=4.39e-01, acc. prob=0.943]Sample:  56%|█████▌    | 1235/2200 [00:23, 68.55it/s, step size=4.39e-01, acc. prob=0.942]Sample:  56%|█████▋    | 1243/2200 [00:23, 68.20it/s, step size=4.39e-01, acc. prob=0.942]Sample:  57%|█████▋    | 1251/2200 [00:23, 67.54it/s, step size=4.39e-01, acc. prob=0.943]Sample:  57%|█████▋    | 1259/2200 [00:23, 70.81it/s, step size=4.39e-01, acc. prob=0.942]Sample:  58%|█████▊    | 1267/2200 [00:23, 67.34it/s, step size=4.39e-01, acc. prob=0.942]Sample:  58%|█████▊    | 1274/2200 [00:23, 59.29it/s, step size=4.39e-01, acc. prob=0.943]Sample:  58%|█████▊    | 1281/2200 [00:23, 53.48it/s, step size=4.39e-01, acc. prob=0.943]Sample:  59%|█████▊    | 1292/2200 [00:24, 63.28it/s, step size=4.39e-01, acc. prob=0.943]Sample:  59%|█████▉    | 1299/2200 [00:24, 64.77it/s, step size=4.39e-01, acc. prob=0.943]Sample:  59%|█████▉    | 1308/2200 [00:24, 70.15it/s, step size=4.39e-01, acc. prob=0.944]Sample:  60%|█████▉    | 1317/2200 [00:24, 73.27it/s, step size=4.39e-01, acc. prob=0.944]Sample:  60%|██████    | 1326/2200 [00:24, 75.94it/s, step size=4.39e-01, acc. prob=0.944]Sample:  61%|██████    | 1335/2200 [00:24, 78.71it/s, step size=4.39e-01, acc. prob=0.944]Sample:  61%|██████    | 1345/2200 [00:24, 78.11it/s, step size=4.39e-01, acc. prob=0.944]Sample:  62%|██████▏   | 1353/2200 [00:24, 68.84it/s, step size=4.39e-01, acc. prob=0.944]Sample:  62%|██████▏   | 1361/2200 [00:24, 66.47it/s, step size=4.39e-01, acc. prob=0.943]Sample:  62%|██████▏   | 1369/2200 [00:25, 68.83it/s, step size=4.39e-01, acc. prob=0.943]Sample:  63%|██████▎   | 1380/2200 [00:25, 75.04it/s, step size=4.39e-01, acc. prob=0.944]Sample:  63%|██████▎   | 1388/2200 [00:25, 63.21it/s, step size=4.39e-01, acc. prob=0.943]Sample:  63%|██████▎   | 1395/2200 [00:25, 56.04it/s, step size=4.39e-01, acc. prob=0.943]Sample:  64%|██████▎   | 1402/2200 [00:25, 58.08it/s, step size=4.39e-01, acc. prob=0.944]Sample:  64%|██████▍   | 1410/2200 [00:25, 63.34it/s, step size=4.39e-01, acc. prob=0.943]Sample:  64%|██████▍   | 1417/2200 [00:25, 63.26it/s, step size=4.39e-01, acc. prob=0.943]Sample:  65%|██████▍   | 1424/2200 [00:26, 55.32it/s, step size=4.39e-01, acc. prob=0.943]Sample:  65%|██████▌   | 1433/2200 [00:26, 62.48it/s, step size=4.39e-01, acc. prob=0.943]Sample:  66%|██████▌   | 1442/2200 [00:26, 65.94it/s, step size=4.39e-01, acc. prob=0.943]Sample:  66%|██████▌   | 1451/2200 [00:26, 72.08it/s, step size=4.39e-01, acc. prob=0.943]Sample:  66%|██████▋   | 1460/2200 [00:26, 75.90it/s, step size=4.39e-01, acc. prob=0.943]Sample:  67%|██████▋   | 1469/2200 [00:26, 76.91it/s, step size=4.39e-01, acc. prob=0.943]Sample:  67%|██████▋   | 1477/2200 [00:26, 65.48it/s, step size=4.39e-01, acc. prob=0.943]Sample:  68%|██████▊   | 1487/2200 [00:26, 71.89it/s, step size=4.39e-01, acc. prob=0.943]Sample:  68%|██████▊   | 1495/2200 [00:26, 72.49it/s, step size=4.39e-01, acc. prob=0.943]Sample:  68%|██████▊   | 1503/2200 [00:27, 61.10it/s, step size=4.39e-01, acc. prob=0.943]Sample:  69%|██████▊   | 1510/2200 [00:27, 53.16it/s, step size=4.39e-01, acc. prob=0.943]Sample:  69%|██████▉   | 1522/2200 [00:27, 66.61it/s, step size=4.39e-01, acc. prob=0.943]Sample:  70%|██████▉   | 1530/2200 [00:27, 64.08it/s, step size=4.39e-01, acc. prob=0.943]Sample:  70%|██████▉   | 1539/2200 [00:27, 65.56it/s, step size=4.39e-01, acc. prob=0.943]Sample:  70%|███████   | 1547/2200 [00:27, 68.53it/s, step size=4.39e-01, acc. prob=0.944]Sample:  71%|███████   | 1555/2200 [00:27, 69.16it/s, step size=4.39e-01, acc. prob=0.944]Sample:  71%|███████   | 1563/2200 [00:28, 70.88it/s, step size=4.39e-01, acc. prob=0.944]Sample:  71%|███████▏  | 1572/2200 [00:28, 73.94it/s, step size=4.39e-01, acc. prob=0.944]Sample:  72%|███████▏  | 1581/2200 [00:28, 77.88it/s, step size=4.39e-01, acc. prob=0.944]Sample:  72%|███████▏  | 1589/2200 [00:28, 71.84it/s, step size=4.39e-01, acc. prob=0.944]Sample:  73%|███████▎  | 1600/2200 [00:28, 80.10it/s, step size=4.39e-01, acc. prob=0.944]Sample:  73%|███████▎  | 1609/2200 [00:28, 78.47it/s, step size=4.39e-01, acc. prob=0.944]Sample:  74%|███████▎  | 1617/2200 [00:28, 76.22it/s, step size=4.39e-01, acc. prob=0.944]Sample:  74%|███████▍  | 1625/2200 [00:28, 75.01it/s, step size=4.39e-01, acc. prob=0.945]Sample:  74%|███████▍  | 1634/2200 [00:28, 78.67it/s, step size=4.39e-01, acc. prob=0.945]Sample:  75%|███████▍  | 1642/2200 [00:29, 76.88it/s, step size=4.39e-01, acc. prob=0.945]Sample:  75%|███████▌  | 1650/2200 [00:29, 77.12it/s, step size=4.39e-01, acc. prob=0.945]Sample:  75%|███████▌  | 1658/2200 [00:29, 69.27it/s, step size=4.39e-01, acc. prob=0.945]Sample:  76%|███████▌  | 1666/2200 [00:29, 67.47it/s, step size=4.39e-01, acc. prob=0.945]Sample:  76%|███████▌  | 1675/2200 [00:29, 72.04it/s, step size=4.39e-01, acc. prob=0.945]Sample:  77%|███████▋  | 1684/2200 [00:29, 75.28it/s, step size=4.39e-01, acc. prob=0.945]Sample:  77%|███████▋  | 1693/2200 [00:29, 77.62it/s, step size=4.39e-01, acc. prob=0.945]Sample:  77%|███████▋  | 1701/2200 [00:29, 76.59it/s, step size=4.39e-01, acc. prob=0.945]Sample:  78%|███████▊  | 1712/2200 [00:29, 82.77it/s, step size=4.39e-01, acc. prob=0.945]Sample:  78%|███████▊  | 1721/2200 [00:30, 64.86it/s, step size=4.39e-01, acc. prob=0.945]Sample:  79%|███████▊  | 1729/2200 [00:30, 62.83it/s, step size=4.39e-01, acc. prob=0.945]Sample:  79%|███████▉  | 1738/2200 [00:30, 69.02it/s, step size=4.39e-01, acc. prob=0.945]Sample:  79%|███████▉  | 1746/2200 [00:30, 68.68it/s, step size=4.39e-01, acc. prob=0.945]Sample:  80%|███████▉  | 1754/2200 [00:30, 58.15it/s, step size=4.39e-01, acc. prob=0.945]Sample:  80%|████████  | 1762/2200 [00:30, 62.57it/s, step size=4.39e-01, acc. prob=0.945]Sample:  81%|████████  | 1772/2200 [00:30, 68.82it/s, step size=4.39e-01, acc. prob=0.945]Sample:  81%|████████  | 1785/2200 [00:31, 83.75it/s, step size=4.39e-01, acc. prob=0.945]Sample:  82%|████████▏ | 1795/2200 [00:31, 86.65it/s, step size=4.39e-01, acc. prob=0.945]Sample:  82%|████████▏ | 1805/2200 [00:31, 81.08it/s, step size=4.39e-01, acc. prob=0.944]Sample:  82%|████████▏ | 1814/2200 [00:31, 60.84it/s, step size=4.39e-01, acc. prob=0.944]Sample:  83%|████████▎ | 1822/2200 [00:31, 50.35it/s, step size=4.39e-01, acc. prob=0.944]Sample:  83%|████████▎ | 1828/2200 [00:31, 46.00it/s, step size=4.39e-01, acc. prob=0.944]Sample:  84%|████████▎ | 1837/2200 [00:32, 51.24it/s, step size=4.39e-01, acc. prob=0.944]Sample:  84%|████████▍ | 1845/2200 [00:32, 52.91it/s, step size=4.39e-01, acc. prob=0.944]Sample:  84%|████████▍ | 1851/2200 [00:32, 53.18it/s, step size=4.39e-01, acc. prob=0.944]Sample:  84%|████████▍ | 1857/2200 [00:32, 51.32it/s, step size=4.39e-01, acc. prob=0.944]Sample:  85%|████████▍ | 1863/2200 [00:32, 48.61it/s, step size=4.39e-01, acc. prob=0.944]Sample:  85%|████████▍ | 1868/2200 [00:32, 46.64it/s, step size=4.39e-01, acc. prob=0.944]Sample:  85%|████████▌ | 1876/2200 [00:32, 54.58it/s, step size=4.39e-01, acc. prob=0.944]Sample:  86%|████████▌ | 1883/2200 [00:32, 56.37it/s, step size=4.39e-01, acc. prob=0.944]Sample:  86%|████████▌ | 1892/2200 [00:33, 64.47it/s, step size=4.39e-01, acc. prob=0.944]Sample:  86%|████████▋ | 1900/2200 [00:33, 68.47it/s, step size=4.39e-01, acc. prob=0.944]Sample:  87%|████████▋ | 1908/2200 [00:33, 66.37it/s, step size=4.39e-01, acc. prob=0.944]Sample:  87%|████████▋ | 1916/2200 [00:33, 68.70it/s, step size=4.39e-01, acc. prob=0.944]Sample:  87%|████████▋ | 1923/2200 [00:33, 62.01it/s, step size=4.39e-01, acc. prob=0.944]Sample:  88%|████████▊ | 1930/2200 [00:33, 59.96it/s, step size=4.39e-01, acc. prob=0.944]Sample:  88%|████████▊ | 1937/2200 [00:33, 57.76it/s, step size=4.39e-01, acc. prob=0.944]Sample:  88%|████████▊ | 1943/2200 [00:33, 55.92it/s, step size=4.39e-01, acc. prob=0.944]Sample:  89%|████████▊ | 1950/2200 [00:34, 56.63it/s, step size=4.39e-01, acc. prob=0.944]Sample:  89%|████████▉ | 1957/2200 [00:34, 59.18it/s, step size=4.39e-01, acc. prob=0.944]Sample:  89%|████████▉ | 1964/2200 [00:34, 58.72it/s, step size=4.39e-01, acc. prob=0.944]Sample:  90%|████████▉ | 1973/2200 [00:34, 66.49it/s, step size=4.39e-01, acc. prob=0.945]Sample:  90%|█████████ | 1985/2200 [00:34, 78.13it/s, step size=4.39e-01, acc. prob=0.944]Sample:  91%|█████████ | 1993/2200 [00:34, 68.62it/s, step size=4.39e-01, acc. prob=0.944]Sample:  91%|█████████ | 2001/2200 [00:34, 46.55it/s, step size=4.39e-01, acc. prob=0.944]Sample:  91%|█████████ | 2007/2200 [00:35, 46.88it/s, step size=4.39e-01, acc. prob=0.945]Sample:  92%|█████████▏| 2014/2200 [00:35, 47.18it/s, step size=4.39e-01, acc. prob=0.945]Sample:  92%|█████████▏| 2020/2200 [00:35, 45.28it/s, step size=4.39e-01, acc. prob=0.945]Sample:  92%|█████████▏| 2028/2200 [00:35, 51.77it/s, step size=4.39e-01, acc. prob=0.944]Sample:  93%|█████████▎| 2038/2200 [00:35, 62.83it/s, step size=4.39e-01, acc. prob=0.945]Sample:  93%|█████████▎| 2047/2200 [00:35, 69.15it/s, step size=4.39e-01, acc. prob=0.945]Sample:  93%|█████████▎| 2055/2200 [00:35, 67.08it/s, step size=4.39e-01, acc. prob=0.945]Sample:  94%|█████████▍| 2063/2200 [00:35, 65.83it/s, step size=4.39e-01, acc. prob=0.945]Sample:  94%|█████████▍| 2071/2200 [00:36, 68.58it/s, step size=4.39e-01, acc. prob=0.945]Sample:  94%|█████████▍| 2079/2200 [00:36, 69.40it/s, step size=4.39e-01, acc. prob=0.945]Sample:  95%|█████████▍| 2087/2200 [00:36, 67.87it/s, step size=4.39e-01, acc. prob=0.945]Sample:  95%|█████████▌| 2094/2200 [00:36, 64.18it/s, step size=4.39e-01, acc. prob=0.945]Sample:  96%|█████████▌| 2101/2200 [00:36, 64.33it/s, step size=4.39e-01, acc. prob=0.945]Sample:  96%|█████████▌| 2108/2200 [00:36, 63.71it/s, step size=4.39e-01, acc. prob=0.945]Sample:  96%|█████████▌| 2115/2200 [00:36, 59.46it/s, step size=4.39e-01, acc. prob=0.945]Sample:  96%|█████████▋| 2122/2200 [00:36, 54.91it/s, step size=4.39e-01, acc. prob=0.945]Sample:  97%|█████████▋| 2130/2200 [00:36, 60.95it/s, step size=4.39e-01, acc. prob=0.946]Sample:  97%|█████████▋| 2137/2200 [00:37, 61.55it/s, step size=4.39e-01, acc. prob=0.946]Sample:  97%|█████████▋| 2144/2200 [00:37, 55.15it/s, step size=4.39e-01, acc. prob=0.946]Sample:  98%|█████████▊| 2150/2200 [00:37, 51.98it/s, step size=4.39e-01, acc. prob=0.946]Sample:  98%|█████████▊| 2161/2200 [00:37, 65.42it/s, step size=4.39e-01, acc. prob=0.946]Sample:  99%|█████████▊| 2168/2200 [00:37, 53.79it/s, step size=4.39e-01, acc. prob=0.946]Sample:  99%|█████████▉| 2176/2200 [00:37, 59.38it/s, step size=4.39e-01, acc. prob=0.946]Sample:  99%|█████████▉| 2186/2200 [00:37, 68.50it/s, step size=4.39e-01, acc. prob=0.946]Sample: 100%|█████████▉| 2194/2200 [00:38, 58.85it/s, step size=4.39e-01, acc. prob=0.946]Sample: 100%|██████████| 2200/2200 [00:38, 57.65it/s, step size=4.39e-01, acc. prob=0.946]\n\n\nAnd again check our model convergence. To plot these, we’ll also need to transfer these back to our CPU. We expect a normal distribution for the intercept parameter if the model converged. There are better ways to check the convergence such as Effective Sample Size (ESS) or Gelman-Rubin convergence, but they require running multiple chains which is finnicky on Windows.\n\n# Check model convergence\nintercept_df = pd.DataFrame({'intercept' : mcmc.get_samples()['intercept'].to('cpu')})\nintercept_df.hist()\n\narray([[&lt;Axes: title={'center': 'intercept'}&gt;]], dtype=object)\n\n\n\n\n\n\n\n\n\nThat doesn’t look very normal! Probably because we only have one sample. It’s possible that a distribution with smaller tails, like the normal, will work better here. Let’s try again with the normal distribution.\n\ndef logistic_model(data):  \n    # Create parameters for the distribution on the correct device\n    mean = torch.tensor(0., device=device)  # Ensure mean is a tensor on GPU\n    scale = torch.tensor(2.5, device=device)  # Ensure standard deviation is a tensor on GPU\n    \n    # Create the Normal distribution with parameters on the correct device\n    prior_dist = dist.Normal(mean, scale)\n    \n    # Sample intercept from the distribution\n    intercept = pyro.sample(\"intercept\", prior_dist)\n    \n    # Observing the data with Bernoulli likelihood\n    with pyro.plate(\"data_plate\", len(data)):\n        pyro.sample(\"obs\", dist.Bernoulli(logits=intercept), obs=data)\n\n\n#NUTS sampler\nnuts_kernel = NUTS(logistic_model)\nmcmc = MCMC(nuts_kernel, num_samples=2000, warmup_steps=200)\nmcmc.run(data)\n\nWarmup:   0%|          | 0/2200 [00:00, ?it/s]Warmup:   0%|          | 9/2200 [00:00, 76.19it/s, step size=2.56e+00, acc. prob=0.703]Warmup:   1%|          | 18/2200 [00:00, 79.19it/s, step size=2.06e+00, acc. prob=0.746]Warmup:   1%|          | 26/2200 [00:00, 78.87it/s, step size=4.48e-01, acc. prob=0.739]Warmup:   2%|▏         | 34/2200 [00:00, 68.26it/s, step size=2.94e+00, acc. prob=0.771]Warmup:   2%|▏         | 47/2200 [00:00, 83.55it/s, step size=8.53e-01, acc. prob=0.766]Warmup:   3%|▎         | 63/2200 [00:00, 105.58it/s, step size=5.31e+00, acc. prob=0.785]Warmup:   4%|▎         | 82/2200 [00:00, 130.01it/s, step size=1.48e+00, acc. prob=0.780]Warmup:   4%|▍         | 96/2200 [00:00, 126.58it/s, step size=4.48e+00, acc. prob=0.788]Warmup:   5%|▍         | 109/2200 [00:01, 106.03it/s, step size=1.79e+00, acc. prob=0.776]Warmup:   6%|▌         | 121/2200 [00:01, 104.75it/s, step size=2.01e-01, acc. prob=0.771]Warmup:   6%|▌         | 132/2200 [00:01, 97.08it/s, step size=3.17e+00, acc. prob=0.780] Warmup:   7%|▋         | 147/2200 [00:01, 109.33it/s, step size=1.41e+00, acc. prob=0.779]Warmup:   7%|▋         | 159/2200 [00:01, 111.26it/s, step size=2.33e+00, acc. prob=0.773]Warmup:   8%|▊         | 171/2200 [00:01, 101.01it/s, step size=4.57e-01, acc. prob=0.772]Warmup:   8%|▊         | 182/2200 [00:01, 91.58it/s, step size=9.20e-01, acc. prob=0.774] Warmup:   9%|▉         | 195/2200 [00:01, 100.74it/s, step size=5.34e+00, acc. prob=0.779]Sample:   9%|▉         | 206/2200 [00:02, 100.79it/s, step size=1.23e+00, acc. prob=0.838]Sample:  10%|▉         | 217/2200 [00:02, 97.41it/s, step size=1.23e+00, acc. prob=0.871] Sample:  10%|█         | 231/2200 [00:02, 106.95it/s, step size=1.23e+00, acc. prob=0.898]Sample:  11%|█         | 243/2200 [00:02, 108.94it/s, step size=1.23e+00, acc. prob=0.875]Sample:  12%|█▏        | 255/2200 [00:02, 111.10it/s, step size=1.23e+00, acc. prob=0.878]Sample:  12%|█▏        | 267/2200 [00:02, 106.99it/s, step size=1.23e+00, acc. prob=0.876]Sample:  13%|█▎        | 278/2200 [00:02, 107.22it/s, step size=1.23e+00, acc. prob=0.861]Sample:  13%|█▎        | 293/2200 [00:02, 118.56it/s, step size=1.23e+00, acc. prob=0.878]Sample:  14%|█▍        | 306/2200 [00:02, 121.13it/s, step size=1.23e+00, acc. prob=0.882]Sample:  14%|█▍        | 319/2200 [00:03, 117.59it/s, step size=1.23e+00, acc. prob=0.881]Sample:  15%|█▌        | 333/2200 [00:03, 123.54it/s, step size=1.23e+00, acc. prob=0.887]Sample:  16%|█▌        | 346/2200 [00:03, 119.40it/s, step size=1.23e+00, acc. prob=0.886]Sample:  16%|█▋        | 359/2200 [00:03, 111.44it/s, step size=1.23e+00, acc. prob=0.883]Sample:  17%|█▋        | 371/2200 [00:03, 104.77it/s, step size=1.23e+00, acc. prob=0.883]Sample:  17%|█▋        | 382/2200 [00:03, 105.04it/s, step size=1.23e+00, acc. prob=0.883]Sample:  18%|█▊        | 393/2200 [00:03, 104.13it/s, step size=1.23e+00, acc. prob=0.883]Sample:  18%|█▊        | 404/2200 [00:03, 101.04it/s, step size=1.23e+00, acc. prob=0.887]Sample:  19%|█▉        | 416/2200 [00:03, 105.62it/s, step size=1.23e+00, acc. prob=0.886]Sample:  19%|█▉        | 427/2200 [00:04, 104.79it/s, step size=1.23e+00, acc. prob=0.889]Sample:  20%|█▉        | 438/2200 [00:04, 103.49it/s, step size=1.23e+00, acc. prob=0.891]Sample:  20%|██        | 449/2200 [00:04, 99.19it/s, step size=1.23e+00, acc. prob=0.891] Sample:  21%|██        | 461/2200 [00:04, 102.37it/s, step size=1.23e+00, acc. prob=0.894]Sample:  21%|██▏       | 472/2200 [00:04, 97.22it/s, step size=1.23e+00, acc. prob=0.892] Sample:  22%|██▏       | 486/2200 [00:04, 107.85it/s, step size=1.23e+00, acc. prob=0.895]Sample:  23%|██▎       | 499/2200 [00:04, 113.65it/s, step size=1.23e+00, acc. prob=0.898]Sample:  23%|██▎       | 511/2200 [00:04, 105.33it/s, step size=1.23e+00, acc. prob=0.899]Sample:  24%|██▎       | 522/2200 [00:04, 105.74it/s, step size=1.23e+00, acc. prob=0.900]Sample:  24%|██▍       | 533/2200 [00:05, 105.46it/s, step size=1.23e+00, acc. prob=0.901]Sample:  25%|██▍       | 548/2200 [00:05, 115.18it/s, step size=1.23e+00, acc. prob=0.896]Sample:  26%|██▌       | 561/2200 [00:05, 119.00it/s, step size=1.23e+00, acc. prob=0.899]Sample:  26%|██▌       | 573/2200 [00:05, 112.84it/s, step size=1.23e+00, acc. prob=0.900]Sample:  27%|██▋       | 585/2200 [00:05, 109.64it/s, step size=1.23e+00, acc. prob=0.900]Sample:  27%|██▋       | 597/2200 [00:05, 110.97it/s, step size=1.23e+00, acc. prob=0.902]Sample:  28%|██▊       | 610/2200 [00:05, 113.74it/s, step size=1.23e+00, acc. prob=0.903]Sample:  28%|██▊       | 626/2200 [00:05, 124.29it/s, step size=1.23e+00, acc. prob=0.903]Sample:  29%|██▉       | 639/2200 [00:05, 120.14it/s, step size=1.23e+00, acc. prob=0.902]Sample:  30%|██▉       | 652/2200 [00:06, 116.41it/s, step size=1.23e+00, acc. prob=0.902]Sample:  30%|███       | 664/2200 [00:06, 115.95it/s, step size=1.23e+00, acc. prob=0.902]Sample:  31%|███       | 678/2200 [00:06, 121.63it/s, step size=1.23e+00, acc. prob=0.902]Sample:  31%|███▏      | 692/2200 [00:06, 123.34it/s, step size=1.23e+00, acc. prob=0.903]Sample:  32%|███▏      | 705/2200 [00:06, 113.09it/s, step size=1.23e+00, acc. prob=0.902]Sample:  33%|███▎      | 719/2200 [00:06, 119.32it/s, step size=1.23e+00, acc. prob=0.903]Sample:  33%|███▎      | 732/2200 [00:06, 116.49it/s, step size=1.23e+00, acc. prob=0.904]Sample:  34%|███▍      | 744/2200 [00:06, 116.19it/s, step size=1.23e+00, acc. prob=0.904]Sample:  34%|███▍      | 757/2200 [00:06, 118.99it/s, step size=1.23e+00, acc. prob=0.905]Sample:  35%|███▌      | 770/2200 [00:07, 120.07it/s, step size=1.23e+00, acc. prob=0.907]Sample:  36%|███▌      | 783/2200 [00:07, 114.28it/s, step size=1.23e+00, acc. prob=0.904]Sample:  36%|███▋      | 802/2200 [00:07, 132.12it/s, step size=1.23e+00, acc. prob=0.903]Sample:  37%|███▋      | 816/2200 [00:07, 124.29it/s, step size=1.23e+00, acc. prob=0.904]Sample:  38%|███▊      | 829/2200 [00:07, 124.85it/s, step size=1.23e+00, acc. prob=0.903]Sample:  38%|███▊      | 845/2200 [00:07, 134.52it/s, step size=1.23e+00, acc. prob=0.902]Sample:  39%|███▉      | 859/2200 [00:07, 136.03it/s, step size=1.23e+00, acc. prob=0.901]Sample:  40%|███▉      | 873/2200 [00:07, 131.90it/s, step size=1.23e+00, acc. prob=0.901]Sample:  40%|████      | 887/2200 [00:07, 121.79it/s, step size=1.23e+00, acc. prob=0.902]Sample:  41%|████      | 900/2200 [00:08, 118.56it/s, step size=1.23e+00, acc. prob=0.902]Sample:  42%|████▏     | 915/2200 [00:08, 124.18it/s, step size=1.23e+00, acc. prob=0.902]Sample:  42%|████▏     | 928/2200 [00:08, 124.08it/s, step size=1.23e+00, acc. prob=0.902]Sample:  43%|████▎     | 941/2200 [00:08, 121.34it/s, step size=1.23e+00, acc. prob=0.903]Sample:  43%|████▎     | 955/2200 [00:08, 124.10it/s, step size=1.23e+00, acc. prob=0.904]Sample:  44%|████▍     | 968/2200 [00:08, 123.98it/s, step size=1.23e+00, acc. prob=0.904]Sample:  45%|████▍     | 981/2200 [00:08, 121.59it/s, step size=1.23e+00, acc. prob=0.904]Sample:  45%|████▌     | 994/2200 [00:08, 115.52it/s, step size=1.23e+00, acc. prob=0.905]Sample:  46%|████▌     | 1006/2200 [00:08, 115.79it/s, step size=1.23e+00, acc. prob=0.906]Sample:  46%|████▋     | 1020/2200 [00:09, 121.85it/s, step size=1.23e+00, acc. prob=0.906]Sample:  47%|████▋     | 1033/2200 [00:09, 119.91it/s, step size=1.23e+00, acc. prob=0.907]Sample:  48%|████▊     | 1046/2200 [00:09, 122.06it/s, step size=1.23e+00, acc. prob=0.908]Sample:  48%|████▊     | 1059/2200 [00:09, 124.32it/s, step size=1.23e+00, acc. prob=0.909]Sample:  49%|████▊     | 1072/2200 [00:09, 120.06it/s, step size=1.23e+00, acc. prob=0.908]Sample:  49%|████▉     | 1087/2200 [00:09, 126.73it/s, step size=1.23e+00, acc. prob=0.909]Sample:  50%|█████     | 1100/2200 [00:09, 124.82it/s, step size=1.23e+00, acc. prob=0.909]Sample:  51%|█████     | 1114/2200 [00:09, 128.77it/s, step size=1.23e+00, acc. prob=0.909]Sample:  51%|█████     | 1127/2200 [00:09, 125.51it/s, step size=1.23e+00, acc. prob=0.909]Sample:  52%|█████▏    | 1140/2200 [00:10, 124.30it/s, step size=1.23e+00, acc. prob=0.908]Sample:  52%|█████▏    | 1153/2200 [00:10, 118.56it/s, step size=1.23e+00, acc. prob=0.908]Sample:  53%|█████▎    | 1166/2200 [00:10, 120.42it/s, step size=1.23e+00, acc. prob=0.908]Sample:  54%|█████▎    | 1179/2200 [00:10, 120.74it/s, step size=1.23e+00, acc. prob=0.908]Sample:  54%|█████▍    | 1195/2200 [00:10, 130.74it/s, step size=1.23e+00, acc. prob=0.908]Sample:  55%|█████▍    | 1209/2200 [00:10, 122.52it/s, step size=1.23e+00, acc. prob=0.908]Sample:  56%|█████▌    | 1223/2200 [00:10, 125.92it/s, step size=1.23e+00, acc. prob=0.907]Sample:  56%|█████▌    | 1237/2200 [00:10, 125.98it/s, step size=1.23e+00, acc. prob=0.908]Sample:  57%|█████▋    | 1250/2200 [00:10, 120.08it/s, step size=1.23e+00, acc. prob=0.907]Sample:  57%|█████▋    | 1263/2200 [00:11, 121.79it/s, step size=1.23e+00, acc. prob=0.908]Sample:  58%|█████▊    | 1277/2200 [00:11, 124.44it/s, step size=1.23e+00, acc. prob=0.907]Sample:  59%|█████▊    | 1291/2200 [00:11, 127.20it/s, step size=1.23e+00, acc. prob=0.908]Sample:  59%|█████▉    | 1307/2200 [00:11, 132.38it/s, step size=1.23e+00, acc. prob=0.907]Sample:  60%|██████    | 1321/2200 [00:11, 134.53it/s, step size=1.23e+00, acc. prob=0.907]Sample:  61%|██████    | 1336/2200 [00:11, 138.97it/s, step size=1.23e+00, acc. prob=0.907]Sample:  61%|██████▏   | 1350/2200 [00:11, 129.83it/s, step size=1.23e+00, acc. prob=0.907]Sample:  62%|██████▏   | 1365/2200 [00:11, 133.97it/s, step size=1.23e+00, acc. prob=0.907]Sample:  63%|██████▎   | 1379/2200 [00:11, 130.86it/s, step size=1.23e+00, acc. prob=0.907]Sample:  63%|██████▎   | 1393/2200 [00:12, 128.73it/s, step size=1.23e+00, acc. prob=0.908]Sample:  64%|██████▍   | 1406/2200 [00:12, 124.57it/s, step size=1.23e+00, acc. prob=0.908]Sample:  64%|██████▍   | 1419/2200 [00:12, 120.23it/s, step size=1.23e+00, acc. prob=0.908]Sample:  65%|██████▌   | 1432/2200 [00:12, 115.61it/s, step size=1.23e+00, acc. prob=0.909]Sample:  66%|██████▌   | 1444/2200 [00:12, 114.63it/s, step size=1.23e+00, acc. prob=0.909]Sample:  66%|██████▌   | 1457/2200 [00:12, 115.67it/s, step size=1.23e+00, acc. prob=0.909]Sample:  67%|██████▋   | 1469/2200 [00:12, 115.91it/s, step size=1.23e+00, acc. prob=0.909]Sample:  67%|██████▋   | 1481/2200 [00:12, 111.72it/s, step size=1.23e+00, acc. prob=0.909]Sample:  68%|██████▊   | 1493/2200 [00:12, 110.64it/s, step size=1.23e+00, acc. prob=0.909]Sample:  68%|██████▊   | 1505/2200 [00:13, 112.93it/s, step size=1.23e+00, acc. prob=0.909]Sample:  69%|██████▉   | 1519/2200 [00:13, 119.25it/s, step size=1.23e+00, acc. prob=0.909]Sample:  70%|██████▉   | 1531/2200 [00:13, 116.29it/s, step size=1.23e+00, acc. prob=0.909]Sample:  70%|███████   | 1543/2200 [00:13, 115.70it/s, step size=1.23e+00, acc. prob=0.909]Sample:  71%|███████   | 1555/2200 [00:13, 112.12it/s, step size=1.23e+00, acc. prob=0.909]Sample:  71%|███████   | 1567/2200 [00:13, 111.52it/s, step size=1.23e+00, acc. prob=0.910]Sample:  72%|███████▏  | 1580/2200 [00:13, 114.15it/s, step size=1.23e+00, acc. prob=0.910]Sample:  72%|███████▏  | 1592/2200 [00:13, 114.51it/s, step size=1.23e+00, acc. prob=0.910]Sample:  73%|███████▎  | 1607/2200 [00:13, 123.92it/s, step size=1.23e+00, acc. prob=0.910]Sample:  74%|███████▎  | 1620/2200 [00:13, 119.48it/s, step size=1.23e+00, acc. prob=0.911]Sample:  74%|███████▍  | 1633/2200 [00:14, 119.74it/s, step size=1.23e+00, acc. prob=0.911]Sample:  75%|███████▍  | 1648/2200 [00:14, 124.86it/s, step size=1.23e+00, acc. prob=0.911]Sample:  76%|███████▌  | 1661/2200 [00:14, 125.25it/s, step size=1.23e+00, acc. prob=0.911]Sample:  76%|███████▌  | 1674/2200 [00:14, 126.23it/s, step size=1.23e+00, acc. prob=0.910]Sample:  77%|███████▋  | 1687/2200 [00:14, 126.60it/s, step size=1.23e+00, acc. prob=0.910]Sample:  77%|███████▋  | 1700/2200 [00:14, 124.34it/s, step size=1.23e+00, acc. prob=0.910]Sample:  78%|███████▊  | 1713/2200 [00:14, 123.13it/s, step size=1.23e+00, acc. prob=0.910]Sample:  79%|███████▊  | 1728/2200 [00:14, 130.47it/s, step size=1.23e+00, acc. prob=0.910]Sample:  79%|███████▉  | 1742/2200 [00:14, 126.27it/s, step size=1.23e+00, acc. prob=0.910]Sample:  80%|███████▉  | 1755/2200 [00:15, 126.61it/s, step size=1.23e+00, acc. prob=0.910]Sample:  80%|████████  | 1768/2200 [00:15, 126.64it/s, step size=1.23e+00, acc. prob=0.910]Sample:  81%|████████  | 1781/2200 [00:15, 118.41it/s, step size=1.23e+00, acc. prob=0.910]Sample:  82%|████████▏ | 1793/2200 [00:15, 117.55it/s, step size=1.23e+00, acc. prob=0.910]Sample:  82%|████████▏ | 1805/2200 [00:15, 116.92it/s, step size=1.23e+00, acc. prob=0.910]Sample:  83%|████████▎ | 1820/2200 [00:15, 124.55it/s, step size=1.23e+00, acc. prob=0.910]Sample:  83%|████████▎ | 1834/2200 [00:15, 127.15it/s, step size=1.23e+00, acc. prob=0.910]Sample:  84%|████████▍ | 1847/2200 [00:15, 126.88it/s, step size=1.23e+00, acc. prob=0.910]Sample:  85%|████████▍ | 1860/2200 [00:15, 124.54it/s, step size=1.23e+00, acc. prob=0.909]Sample:  85%|████████▌ | 1873/2200 [00:16, 119.91it/s, step size=1.23e+00, acc. prob=0.910]Sample:  86%|████████▌ | 1886/2200 [00:16, 121.38it/s, step size=1.23e+00, acc. prob=0.910]Sample:  86%|████████▋ | 1899/2200 [00:16, 119.25it/s, step size=1.23e+00, acc. prob=0.910]Sample:  87%|████████▋ | 1912/2200 [00:16, 120.58it/s, step size=1.23e+00, acc. prob=0.910]Sample:  88%|████████▊ | 1925/2200 [00:16, 120.51it/s, step size=1.23e+00, acc. prob=0.910]Sample:  88%|████████▊ | 1938/2200 [00:16, 117.22it/s, step size=1.23e+00, acc. prob=0.910]Sample:  89%|████████▊ | 1950/2200 [00:16, 117.67it/s, step size=1.23e+00, acc. prob=0.910]Sample:  89%|████████▉ | 1962/2200 [00:16, 117.00it/s, step size=1.23e+00, acc. prob=0.910]Sample:  90%|████████▉ | 1974/2200 [00:16, 114.58it/s, step size=1.23e+00, acc. prob=0.911]Sample:  90%|█████████ | 1988/2200 [00:17, 121.81it/s, step size=1.23e+00, acc. prob=0.911]Sample:  91%|█████████ | 2002/2200 [00:17, 126.70it/s, step size=1.23e+00, acc. prob=0.910]Sample:  92%|█████████▏| 2015/2200 [00:17, 127.10it/s, step size=1.23e+00, acc. prob=0.911]Sample:  92%|█████████▏| 2028/2200 [00:17, 126.47it/s, step size=1.23e+00, acc. prob=0.911]Sample:  93%|█████████▎| 2041/2200 [00:17, 119.46it/s, step size=1.23e+00, acc. prob=0.911]Sample:  93%|█████████▎| 2054/2200 [00:17, 114.98it/s, step size=1.23e+00, acc. prob=0.911]Sample:  94%|█████████▍| 2066/2200 [00:17, 109.45it/s, step size=1.23e+00, acc. prob=0.910]Sample:  94%|█████████▍| 2078/2200 [00:17, 105.75it/s, step size=1.23e+00, acc. prob=0.911]Sample:  95%|█████████▍| 2089/2200 [00:17, 105.41it/s, step size=1.23e+00, acc. prob=0.910]Sample:  96%|█████████▌| 2104/2200 [00:18, 114.77it/s, step size=1.23e+00, acc. prob=0.910]Sample:  96%|█████████▋| 2118/2200 [00:18, 118.77it/s, step size=1.23e+00, acc. prob=0.909]Sample:  97%|█████████▋| 2130/2200 [00:18, 116.99it/s, step size=1.23e+00, acc. prob=0.909]Sample:  98%|█████████▊| 2146/2200 [00:18, 125.86it/s, step size=1.23e+00, acc. prob=0.910]Sample:  98%|█████████▊| 2160/2200 [00:18, 127.61it/s, step size=1.23e+00, acc. prob=0.910]Sample:  99%|█████████▉| 2173/2200 [00:18, 125.47it/s, step size=1.23e+00, acc. prob=0.910]Sample:  99%|█████████▉| 2186/2200 [00:18, 126.40it/s, step size=1.23e+00, acc. prob=0.911]Sample: 100%|██████████| 2200/2200 [00:18, 117.36it/s, step size=1.23e+00, acc. prob=0.911]\n\n\n\n# Check model convergence\nintercept_df = pd.DataFrame({'intercept' : mcmc.get_samples()['intercept'].to('cpu')})\nintercept_df.hist()\n\narray([[&lt;Axes: title={'center': 'intercept'}&gt;]], dtype=object)\n\n\n\n\n\n\n\n\n\nThat looks pretty normal now. We’ll need to turn these back into probabilities using the signmoid function.\n\n# Extract samples\nsamples = mcmc.get_samples()['intercept'].to('cpu')\n\n# Plot probabilities\ninferred_probs_df = pd.DataFrame({'probs' : torch.sigmoid(samples)})\ninferred_probs_df.hist()\n\n\ninferred_prob = torch.sigmoid(samples).mean().item()\n\nprint(f\"Inferred probability of a thumb up: {inferred_prob:.4f}\")\n\n# Compute confidence interval\nconfidence_interval_vals = torch.quantile(samples, torch.tensor([0.025, 0.975]))\nconfidence_interval = torch.sigmoid(confidence_interval_vals)\nprint(f\"95% confidence interval: {confidence_interval.tolist()}\")\n\nInferred probability of a thumb up: 0.7327\n95% confidence interval: [0.1406741887331009, 0.9958332180976868]\n\n\n\n\n\n\n\n\n\nOur original estimate using the beta distribution was 0.6649 with a 95% confidence interval: [0.1617867797613144, 0.9869610667228699]. That’s a bit different from this, but the choice of prior influences the outcome. Overall though, that looks like a pretty good estimate of the analytical solution despite only having a single sample.\nWhat happens if we increased the number of samples to 100? How accurate can we get with a moderate amount of data? Let’s say we want to model something with a 90% upvote rate, so 90 thumbs up and 10 thumbs down.\n\n# Data: 90 thumbs up, 10 thumbs down\ndata = torch.cat((torch.ones(90), torch.zeros(10)))\n\n# Rerun model with reduced steps\nnuts_kernel = NUTS(beta_model)\nmcmc = MCMC(nuts_kernel, num_samples=2000, warmup_steps=200)\nmcmc.run(data)\n\n# Extract samples\nsamples = mcmc.get_samples()\nprob_thumb_up_samples = samples['prob_thumb_up']\n\n# Make histogram of samples\nsamples_df = pd.DataFrame(samples)\nsamples_df.hist()\n\n# Compute confidence interval\nconfidence_interval = torch.quantile(prob_thumb_up_samples, torch.tensor([0.025, 0.975]))\n\nprint(f\"Estimated probability of thumb up: {prob_thumb_up_samples.mean().item():.4f}\")\nprint(f\"95% confidence interval: {confidence_interval.tolist()}\")\n\nWarmup:   0%|          | 0/2200 [00:00, ?it/s]Warmup:   1%|▏         | 28/2200 [00:00, 267.89it/s, step size=1.64e-01, acc. prob=0.765]Warmup:   3%|▎         | 65/2200 [00:00, 323.13it/s, step size=3.72e-01, acc. prob=0.786]Warmup:   5%|▍         | 103/2200 [00:00, 333.68it/s, step size=2.77e-01, acc. prob=0.779]Warmup:   6%|▌         | 137/2200 [00:00, 326.09it/s, step size=1.09e+00, acc. prob=0.787]Warmup:   8%|▊         | 170/2200 [00:00, 302.44it/s, step size=1.28e+00, acc. prob=0.782]Warmup:   9%|▉         | 201/2200 [00:00, 294.67it/s, step size=1.24e+00, acc. prob=0.699]Sample:  11%|█         | 241/2200 [00:00, 325.48it/s, step size=1.24e+00, acc. prob=0.917]Sample:  13%|█▎        | 277/2200 [00:00, 334.84it/s, step size=1.24e+00, acc. prob=0.911]Sample:  15%|█▍        | 320/2200 [00:00, 363.37it/s, step size=1.24e+00, acc. prob=0.911]Sample:  16%|█▌        | 357/2200 [00:01, 361.53it/s, step size=1.24e+00, acc. prob=0.914]Sample:  18%|█▊        | 396/2200 [00:01, 368.89it/s, step size=1.24e+00, acc. prob=0.914]Sample:  20%|█▉        | 434/2200 [00:01, 363.52it/s, step size=1.24e+00, acc. prob=0.916]Sample:  22%|██▏       | 475/2200 [00:01, 374.98it/s, step size=1.24e+00, acc. prob=0.916]Sample:  23%|██▎       | 515/2200 [00:01, 380.10it/s, step size=1.24e+00, acc. prob=0.917]Sample:  25%|██▌       | 555/2200 [00:01, 383.61it/s, step size=1.24e+00, acc. prob=0.913]Sample:  27%|██▋       | 597/2200 [00:01, 393.29it/s, step size=1.24e+00, acc. prob=0.912]Sample:  29%|██▉       | 637/2200 [00:01, 387.24it/s, step size=1.24e+00, acc. prob=0.916]Sample:  31%|███       | 678/2200 [00:01, 392.68it/s, step size=1.24e+00, acc. prob=0.915]Sample:  33%|███▎      | 718/2200 [00:02, 379.22it/s, step size=1.24e+00, acc. prob=0.918]Sample:  34%|███▍      | 757/2200 [00:02, 380.69it/s, step size=1.24e+00, acc. prob=0.919]Sample:  36%|███▌      | 797/2200 [00:02, 385.18it/s, step size=1.24e+00, acc. prob=0.921]Sample:  38%|███▊      | 836/2200 [00:02, 377.67it/s, step size=1.24e+00, acc. prob=0.921]Sample:  40%|███▉      | 874/2200 [00:02, 358.66it/s, step size=1.24e+00, acc. prob=0.922]Sample:  41%|████▏     | 911/2200 [00:02, 350.98it/s, step size=1.24e+00, acc. prob=0.921]Sample:  43%|████▎     | 953/2200 [00:02, 367.28it/s, step size=1.24e+00, acc. prob=0.921]Sample:  45%|████▌     | 994/2200 [00:02, 376.22it/s, step size=1.24e+00, acc. prob=0.920]Sample:  47%|████▋     | 1032/2200 [00:02, 367.79it/s, step size=1.24e+00, acc. prob=0.919]Sample:  49%|████▊     | 1071/2200 [00:02, 370.97it/s, step size=1.24e+00, acc. prob=0.921]Sample:  50%|█████     | 1109/2200 [00:03, 366.65it/s, step size=1.24e+00, acc. prob=0.921]Sample:  52%|█████▏    | 1146/2200 [00:03, 356.33it/s, step size=1.24e+00, acc. prob=0.922]Sample:  54%|█████▍    | 1184/2200 [00:03, 362.05it/s, step size=1.24e+00, acc. prob=0.922]Sample:  56%|█████▌    | 1221/2200 [00:03, 362.26it/s, step size=1.24e+00, acc. prob=0.923]Sample:  57%|█████▋    | 1258/2200 [00:03, 357.17it/s, step size=1.24e+00, acc. prob=0.923]Sample:  59%|█████▉    | 1298/2200 [00:03, 367.41it/s, step size=1.24e+00, acc. prob=0.923]Sample:  61%|██████    | 1338/2200 [00:03, 375.89it/s, step size=1.24e+00, acc. prob=0.922]Sample:  63%|██████▎   | 1377/2200 [00:03, 380.02it/s, step size=1.24e+00, acc. prob=0.922]Sample:  64%|██████▍   | 1416/2200 [00:03, 378.51it/s, step size=1.24e+00, acc. prob=0.921]Sample:  66%|██████▌   | 1454/2200 [00:04, 361.91it/s, step size=1.24e+00, acc. prob=0.921]Sample:  68%|██████▊   | 1492/2200 [00:04, 366.55it/s, step size=1.24e+00, acc. prob=0.921]Sample:  70%|██████▉   | 1529/2200 [00:04, 357.32it/s, step size=1.24e+00, acc. prob=0.921]Sample:  71%|███████   | 1567/2200 [00:04, 360.72it/s, step size=1.24e+00, acc. prob=0.922]Sample:  73%|███████▎  | 1608/2200 [00:04, 372.81it/s, step size=1.24e+00, acc. prob=0.921]Sample:  75%|███████▍  | 1648/2200 [00:04, 379.63it/s, step size=1.24e+00, acc. prob=0.922]Sample:  77%|███████▋  | 1695/2200 [00:04, 403.74it/s, step size=1.24e+00, acc. prob=0.922]Sample:  79%|███████▉  | 1736/2200 [00:04, 374.74it/s, step size=1.24e+00, acc. prob=0.922]Sample:  81%|████████  | 1778/2200 [00:04, 385.25it/s, step size=1.24e+00, acc. prob=0.922]Sample:  83%|████████▎ | 1817/2200 [00:04, 381.17it/s, step size=1.24e+00, acc. prob=0.922]Sample:  84%|████████▍ | 1856/2200 [00:05, 380.97it/s, step size=1.24e+00, acc. prob=0.922]Sample:  86%|████████▋ | 1898/2200 [00:05, 391.10it/s, step size=1.24e+00, acc. prob=0.922]Sample:  88%|████████▊ | 1938/2200 [00:05, 376.16it/s, step size=1.24e+00, acc. prob=0.921]Sample:  90%|████████▉ | 1979/2200 [00:05, 385.78it/s, step size=1.24e+00, acc. prob=0.920]Sample:  92%|█████████▏| 2018/2200 [00:05, 375.08it/s, step size=1.24e+00, acc. prob=0.920]Sample:  93%|█████████▎| 2056/2200 [00:05, 368.07it/s, step size=1.24e+00, acc. prob=0.920]Sample:  95%|█████████▌| 2094/2200 [00:05, 369.37it/s, step size=1.24e+00, acc. prob=0.921]Sample:  97%|█████████▋| 2136/2200 [00:05, 382.80it/s, step size=1.24e+00, acc. prob=0.920]Sample:  99%|█████████▉| 2175/2200 [00:05, 376.20it/s, step size=1.24e+00, acc. prob=0.921]Sample: 100%|██████████| 2200/2200 [00:05, 368.32it/s, step size=1.24e+00, acc. prob=0.921]\n\n\nEstimated probability of thumb up: 0.8923\n95% confidence interval: [0.827656626701355, 0.9420859217643738]\n\n\n\n\n\n\n\n\n\nAn estimate of 0.89 for a 90% upvoted piece of content is pretty good, and the spread on the estimate is much tighter than before.\nThis is still a really simple model, though. For more complicated models, MCMC might be prohibitively slow. For real life scenarios, it’s inlikely to be doing something with this few samples and this simple of a model. In these cases, you might want to try variational inference (VI) instead of MCMC. VI allows you to turn the sampling problem into an optimization problem, where you optimize the parameters of a distirbution directly. Here I’m going to use the Adam optomizer, which is short for adaptive gradient descent. It’s popular in lots of mcahine learning applications due to its speed and ability to handle sparse gradients. We can check to see how well Adam is optimizing by checking to see if the evidence lower bound (ELBO) converges. We can pump up our data a bit, too.\n\n# Define the guide (variational posterior)\ndef guide(data):\n    # Variational parameters for the intercept\n    mean_q = pyro.param(\"mean_q\", torch.tensor(0.0, device=device))\n    scale_q = pyro.param(\"scale_q\", torch.tensor(1.0, device=device), constraint=dist.constraints.positive)\n    \n    pyro.sample(\"intercept\", dist.Normal(mean_q, scale_q))\n\n# Data: 900 thumbs up, 100 thumbs down, and make sure its in the right place\ndata = torch.cat((torch.ones(900), torch.zeros(100))).to(device)\n\n# Set up the optimizer and inference algorithm\noptimizer = Adam({\"lr\": torch.tensor(0.001, device=device)})  # Learning rate\nsvi = SVI(logistic_model, guide, optimizer, loss=Trace_ELBO())\n\n# Perform optimization\nnum_steps = 10000\nlosses = []\nfor step in range(num_steps):\n    loss = svi.step(data)  # Perform one step of optimization\n    losses.append(loss)\n    if step % 1000 == 0:\n        print(f\"Step {step} - Loss: {loss}\")\n\n# Inspect learned parameters\nmean_q = pyro.param(\"mean_q\").item()\nscale_q = pyro.param(\"scale_q\").item()\nprint(f\"Posterior Mean: {mean_q}, Posterior Std: {scale_q}\")\n\n# Plot ELBO convergence\nimport matplotlib.pyplot as plt\nplt.plot(losses)\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"ELBO Loss\")\nplt.title(\"ELBO Convergence\")\nplt.show()\n\nStep 0 - Loss: 530.4799635410309\nStep 1000 - Loss: 450.63120698928833\nStep 2000 - Loss: 603.2254050970078\nStep 3000 - Loss: 363.6408305466175\nStep 4000 - Loss: 337.36681163311005\nStep 5000 - Loss: 328.7159623503685\nStep 6000 - Loss: 327.6683760881424\nStep 7000 - Loss: 328.99758183956146\nStep 8000 - Loss: 328.5293536186218\nStep 9000 - Loss: 328.31051325798035\nPosterior Mean: 2.205451250076294, Posterior Std: 0.15027137100696564\n\n\n\n\n\n\n\n\n\nThat elbow plot clearly converges, so it looks like the optimizer worked well. All that’s left is seeing if the estimate we got for the posterior probability of the intercept is accurate. To do that, we’ll need to transform our\n\nimport math\n\n# Convert posterior mean (log-odds) to probability\nposterior_mean = pyro.param(\"mean_q\").item()\n\n# Probability from logistic function\nposterior_probability = 1 / (1 + math.exp(-posterior_mean))\n\nprint(posterior_probability)\n\n0.900737967976173\n\n\nA perfect estimate!"
  },
  {
    "objectID": "posts/20240105/index.html",
    "href": "posts/20240105/index.html",
    "title": "Getting Around Istanbul",
    "section": "",
    "text": "Summary:\n\nThe metro and ferries are the best public transit options\nGoogle Maps works, but Moovit has much better coverage for ferry routes\nAvoid relying on taxis unless absolutely necessary\nUber calls taxis and you will likely have an excessive wait\nAvoid minibuses (light blue buses on Google Maps)\n\nIf you don’t like my guide, you can read more about getting to and from the airport here\nIf you don’t like my guide, you can read more about public transit here\n\n\nGeneral Information\nIstanbul has a very well-developed public transit system. It spans the Bosphorous Strait to connect both sides of city and connects both passenger airports (IST and SAW) to the city center. Public transit, especially the ferries and metro, are the best way to get around the city due to rush hour traffic. The metro system covers all of the main touristic areas of the city and is easy to use. On weeknights most public transit shuts down around midnight, so plan on staying near your hotel if you want to go out late. Google Maps does a good job of providing routes and even has several privately owned airport bus companies listed. Almost all of the public transit can be accessed using an IstanbulKart that can be purchased from and reloaded at yellow kiosks in metro stations. In tourist areas, you’re more likely to find kiosks that have an English option. Each connection you use will incur a new charge, so make sure your IstanbulKart is loaded up. And importantly, most IstanbulKart machines cannot take bills larger than 100 TL! Each leg of a journey will probably cost ~10TL\n\n\nModes of Transit\n\nMetro\n\n\n\nIstanbul metro map\n\n\nThe metro system in Istanbul is clean and easy to use. It functions just like the metro lines in most major cities. Expect trains every 5-10 min in most cases. Trains run from 6:00 AM to midnight, but there is uninterrupted 24-hour service with trains every 30 min Friday and Saturday night. This should be one of your primary ways of getting around the city. There is only one metro line that crosses the Bosphorous Strait, the Marmaray (gray line). This line is more expensive.\n\n\nFerries\nFerries in Istanbul are fully integrated into the public transit network, so you can use your IstanbulKart to board them. They are very convenient, and should also be one of your primary modes of transportation. They are automatically included in Google Maps routes and marked with a blue boat sign. At many of the ferry stations, there will be several wharfs with different ferries. For the busiest ferries, there can even be two docks running the same route at staggered intervals. Make sure you are getting on the right one by checking the signs! Public ferry schedules can be found here: https://sehirhatlari.istanbul/en/timetables . There are also private ferries (which can still be accessed using your IstanbulKart), but these are not included in Google Maps. Roughly half of the ferries moving on major routes are operated by private companies. If you see a ferry on Google Maps that’s running on the :00 and :30 marks, there is probably another dock nearby with a private ferry that leaves on the :15 and :45 marks. Private ferries are included in the Moovit app.\n\n\nBuses\nThere are three types of buses in Istanbul that might be included in your route.\n\nStandard city buses: These are big yellow and black buses, and they have a yellow bus symbol on Google Maps. You pay when you get on. Most buses will stop running before midnight.\nMetrobuses: These have a beige bus symbol on Google Maps. These buses have their own lanes and move pretty quickly. You pay when you enter the station as if it were a metro.\nMinibuses: These are tiny death traps with no ventilation. They are marked with a light blue bus symbol on Google Maps. I recommend not using them because they only take cash and you have to tell them your destination.\n\n\n\nTaxis\nAvoid taxis unless absolutely necessary. They are notorious for scamming people, often do not speak English, and will not know how to get to your destination. Many taxis are cash only. If you have to take a taxi, make sure the meter is running as soon as you get in the cab, and pull up the route on your phone so that you see if they make any detours. There may be additional fees associated with crossing the bridges, but these should be small (~20 TL).\n\n\nGetting to/from the Airport\nBoth Istanbul airports (IST and SAW) are a ways outside the city, so it can take some time to get to the city center where your hotels probably are.\nThere are several ways you can get to/from the airport. The easiest way is going to be getting your hotel to call you an airport shuttle. Note that the wedding venue (A11 Hotel Bosphorus) provides an airport shuttle. You also might be able to save yourself a few dollars by booking a shuttle yourself with a private shuttle service such as https://airporttransfer.vip/ . This should cost ~€40. I’ve seen good reviews for this service, but I haven’t used it personally. Driving directly to your hotel should take ~45 min, but traffic can add up to an hour if you’re landing during rush hour.\nAnother way is to take an airport bus. There are private companies that offer reliable service between the airport and popular destinations in Istanbul. These run 24/7, but at 30-60 min intervals. The most popular is Havaist, which leaves from the -2 floor of the IST airport https://istanbul-international-airport.com/transportation/bus/ . If you are staying in Uskudar, you can take the Havaist bus to Kadikoy, followed by the metro from Kadikoy to Uskudar. This is two stops, but requires changing lines at the first stop. The total cost of this trip will be ~$7, not including buying an IstanbulKart to use the public transit. You can also take the Havaist bus to Besiktas and take a ferry from Besiktas to Uskudar.\nThe airports also connects directly to the metro system. You can connect to either the historic areas of Istanbul or Uskudar, but it will require several transfers, and take slightly longer than the Havaist bus. You can get to Uskudar by taking the M11-M7-M2-Marmaray route. Most other areas of the city—such as Galata and Eminonu—are accessible via the M2 line without crossing the Bosphorous using the Marmaray. Some of the stations are quite large, and it may not be desirable to lug around a ton of bags."
  },
  {
    "objectID": "posts/20241215-testing-pyro/testing_pyro.html",
    "href": "posts/20241215-testing-pyro/testing_pyro.html",
    "title": "Learning Pyro for Better Content Sorting",
    "section": "",
    "text": "I’ve been looking for a reason to learn to use the Pyro library in Python for Markov Chain Monte Carlo (MCMC) simulations, and I’ve finally found one. This is me documenting how to learn to use it for future reference, and hopefully you’ll be able to get something out of it too.\nHave you ever sorted content on a website by “Top Ranked” or something similar, only do be inundated with a bunch of posts with a 100% rating but only a tiny number of reviews? It can make it frustrating or impossible to find “the best” of something when it’s hiding below hundreds or thousands of other things. If you’re going to rank by the average score for a piece of content, it’s inevitable that you’ll end up with some items near the top of the list that shouldn’t be. This happens because low sample sizes (e.g., a small number of reviews or upvotes) lead to a wide variety of estimates of the content’s “true” rating.\nThankfully, lots of smart companies have come up with better ways to rank their content! Reddit, for example, used to use a confidence interval around the score of the of a post (see the cpdef double _confidence function) to account for uncertainty in its “true” score. This has the effect of penalizing upvoted posts that have a low score with only a handful of upvotes, but it will also boost downvoted posts in the same situation. As the number of votes increases, the confidence in the post’s true rating also increases.\nOther companies seem to have an issue with this. For example, Google Maps has pretty limited options on what you can sort by, and instead only allows you to filter based on certain ranges of average review score. This still leaves the top results polluted by new, fake, or closed restaurants when you’re going out to eat. This isn’t a good system, and I frequently find myself with bad results. You have to go to an entirely different website to get decent results.\nFinding a better estimate of a restaurant’s “true rating” is a good use case for Pyro, a probabilistic programming language thats uses PyTorch. Pyro make it easy to find a restaurant’s “true rating” (posterior probability) given its current ratings (likelihood) and some assumptions about what its true rating is likely to be before any reviews are given (prior probability). These problems can be solved using MCMC simulations to estimate the posterior probability. PyTorch is supposed to make these sorts of problems easy, so I’m going to use it to estimate the posterior probabilities of some content and learn how to use Pyro along the way.\nInstalling PyTorch isn’t as simple as most other python packages, so I recommend looking at their website to do it (https://pytorch.org/). Beyond that, you’ll also need pandas and the pyro package (https://pyro.ai/).\n\n# import packages\n\nimport torch\nimport pyro\nimport pyro.distributions as dist\nimport pandas as pd\n\nfrom pyro.infer import MCMC, NUTS, SVI, Trace_ELBO\nfrom pyro.optim import Adam\n\nIf you installed PyTorch correctly and you have an nVidia GPU, you can run this to see if your GPU is available for PyTorch to use. This workbook should still work even if this isn’t true, instead running the simulations on your CPU. The first few will run on the default device, but I’ll switch to my GPU later.\n\nif torch.cuda.is_available():\n    print(\"CUDA is available. GPU is ready to be used.\")\nelse:\n    print(\"CUDA is not available. Running on CPU.\")\n\nCUDA is available. GPU is ready to be used.\n\n\nAs a toy example, let’s consider a restaurant or some other piece of content that has a single upvote in an upvote/downvote rating system. This will run on the default device regardless of your GPU setup.\n\n# Data: 1 thumb up out of 1 observation\ndata = torch.tensor([1.])\n\nWe first need to define a model that PyTorch will simulate.\n\nThe pyro.sample function here is given two arguments: the name of your sample and the prior distribution. In this case, we use the Beta(1,1) distribution as our prior. This draws a sample from the Beta(1,1) distribution for our prior. The beta distribution is a common prior when estimating probabilities because of both the posterior and the prior are defined over [0,1]. It is also a conjugate prior for the Binomial distribution, which is the distribution for a series of up-down votes. The Beta(1,1) is a uniform distribution, which indicates we have no information about the prior probability. In practice, Betas tend to be used more than Uniforms due to their flexibility, but the results should be the same regardless. It is also easy to interpret in our case because the Beta(a,b) parameters a and b correspond to pseudocounts for upvotes and downvotes for our content. This will come in handy later.\nThe pyro.plate function indicates that all of the observations are IID, and again takes two arguments: a name and the number of observations. We then use the pyro.sample function again, but this time the probability of success (in this case, an upvoted restaurants) is given by the the probability from the first step, and our observed data is used as our evidence.\n\n\n# Define the model\ndef beta_model(data):\n    # Prior distribution for the probability of a thumb up\n    prob_thumb_up = pyro.sample(\"prob_thumb_up\", dist.Beta(1, 1))\n    # Observing the data\n    with pyro.plate(\"data\", len(data)):\n        pyro.sample(\"obs\", dist.Bernoulli(prob_thumb_up), obs=data)\n\nTo simulate our posterior, we use NUTS (No U-turn Sampler). We then run our MCMC method to extract all of the posterior simulations. The NUTS sampler starts off slow and not giving useful information at the beginning, so we throw out the first warmup_steps.\n\n# Run sampling\nnuts_kernel = NUTS(beta_model)\nmcmc = MCMC(nuts_kernel, num_samples=100, warmup_steps=200)\nmcmc.run(data)\n\nSample: 100%|██████████| 300/300 [00:00, 315.72it/s, step size=1.05e+00, acc. prob=0.872]\n\n\nNow that our 100 samples have run in ~1 second, we can see what our posterior distribution looks like.\n\n# Extract samples\nsamples = mcmc.get_samples()\nprob_thumb_up_samples = samples['prob_thumb_up']\nprint(samples)\n\n# Make histogram of samples\nsamples_df = pd.DataFrame(samples)\nsamples_df.hist()\n\n{'prob_thumb_up': tensor([0.9948, 0.9911, 0.9932, 0.8157, 0.9069, 0.9725, 0.9605, 0.7183, 0.6688,\n        0.4664, 0.4664, 0.1412, 0.4164, 0.4164, 0.4718, 0.4669, 0.1655, 0.1651,\n        0.9353, 0.9179, 0.9453, 0.6563, 0.9291, 0.9523, 0.9919, 0.9892, 0.8332,\n        0.8525, 0.7400, 0.7400, 0.8004, 0.8386, 0.8639, 0.8639, 0.8468, 0.9864,\n        0.7228, 0.2010, 0.8474, 0.6574, 0.6523, 0.6671, 0.6671, 0.7251, 0.8520,\n        0.8915, 0.2909, 0.4525, 0.2638, 0.4680, 0.5942, 0.3475, 0.3475, 0.8394,\n        0.8396, 0.7719, 0.7719, 0.3881, 0.3250, 0.5279, 0.6887, 0.0769, 0.9948,\n        0.9938, 0.9592, 0.9085, 0.1504, 0.9345, 0.5709, 0.4091, 0.6242, 0.9275,\n        0.1048, 0.9889, 0.9549, 0.9138, 0.7142, 0.7142, 0.8877, 0.8169, 0.8869,\n        0.7573, 0.7573, 0.8330, 0.7542, 0.8275, 0.6461, 0.6482, 0.6646, 0.6646,\n        0.5550, 0.5550, 0.7538, 0.5528, 0.1494, 0.4221, 0.8125, 0.7165, 0.3323,\n        0.6686])}\n\n\narray([[&lt;Axes: title={'center': 'prob_thumb_up'}&gt;]], dtype=object)\n\n\n\n\n\n\n\n\n\nOur probability distribution looks like it’s multimodal, when it should be unimodal. That probably means the model didn’t converge. Let’s try upping the number of samples we use to see if we can get a better behaved distribution.\n\n# Run sampling\nnuts_kernel = NUTS(beta_model)\nmcmc = MCMC(nuts_kernel, num_samples=10000, warmup_steps=200)\nmcmc.run(data)\n\n# Extract samples\nsamples = mcmc.get_samples()\nprob_thumb_up_samples = samples['prob_thumb_up']\nprint(samples)\n\n# Make histogram of samples\nsamples_df = pd.DataFrame(samples)\nsamples_df.hist()\n\nSample: 100%|██████████| 10200/10200 [00:28, 357.90it/s, step size=6.56e-01, acc. prob=0.898]\n\n\n{'prob_thumb_up': tensor([0.5870, 0.4914, 0.8957,  ..., 0.2461, 0.5114, 0.9622])}\n\n\narray([[&lt;Axes: title={'center': 'prob_thumb_up'}&gt;]], dtype=object)\n\n\n\n\n\n\n\n\n\nThis distribution looks much more regular. From this, we can calculate the confidence interval for our restaurant’s “true rating”.\n\n# Compute confidence interval\nconfidence_interval = torch.quantile(prob_thumb_up_samples, torch.tensor([0.025, 0.975]))\n\nprint(f\"Estimated probability of thumb up: {prob_thumb_up_samples.mean().item():.4f}\")\nprint(f\"95% confidence interval: {confidence_interval.tolist()}\")\n\nEstimated probability of thumb up: 0.6559\n95% confidence interval: [0.1598558872938156, 0.9852998852729797]\n\n\nWe should also probably check to make sure that our estimates are valid. This particular example is trivial because of our choice of distribution and prior. Because we used a conjugate prior (Beta) for our observed data (Binomial/Bernoulli), we have a closed form solution for our expected result. I mentioned before that the a and b parameters for the Beta(a,b) distribution are pseudocounts for successes (thumbs up) and failures (thumbs down), respectively. That means that if our prior distribution has the form Beta(1,1) and we have observed 1 additional success, our posterior distribution has the form Beta(2,1).\nYou can see slide 12 of David Maracek’s excellent lecture on Beta-Bernoulli distributions to see what this looks like, or you can draw the Beta(2,1) distribution yourself using this applet.\nThe shape of our simulated posterior looks almost exactly like our expected output. Success!\nHow a content rating system wants to use this information would be up to them. In Reddit’s case, it looks like they used an 80% bound to rank their posts. If you want your ranking to be much stricter, you could use the lower bound of the credible interval, which would push new entries to the bottom of the ranking. You could also increase the number of pseudocounts in your Beta prior to something like the mean value of upvotes and downvotes, which may be more appropriate if you want new content to be ranked “average” until proven otherwise.\nIn some cases, it may be more appropriate to do this in a regression framework so that you can control for other variables. I’m going to repeat this analysis using logistic regression, which models the log-odds of a success. I’ll also do it on the GPU (although for small samples and simple models this may not be necessary due to the overhead incurred by moving data to your GPU).\nFirst we’ll check to see if we can use our GPU. If we can, we’ll create our data on our GPU. All of the data needs to be in one place for this to work and it will default to your CPU, so you’ll need to explicitly put everything on your GPU. A GPU will actually be a bit slower for this use case due to the overhead incurred, but it’s a good reference for larger models.\n\n# Check for CUDA availability and prepare data\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    # Create data tensor on GPU\n    data = torch.tensor([1.]).to(device)\n    print(\"Data moved to GPU.\")\nelse:\n    device = torch.device(\"cpu\")\n    data = torch.tensor([1.]).to(device)\n    print(\"CUDA not available, running on CPU.\")\n    \n# Validate devices\ntorch.cuda.get_device_name(torch.cuda.current_device())\n\nData moved to GPU.\n\n\n'NVIDIA GeForce RTX 3050 Laptop GPU'\n\n\nNext we’ll define our model. This model is going to look different from the previous one. In our logistic regression, the intercept will give us the log-odds of our restaurant’s true probability of thumbs up. This parameter is no longer bounded on [0,1], which means the Beta distribution is no longer appropriate. There are a few options here. * A normal distribution with mean 0 (p = 0.5 means the odds are (0.5) / (1 - 0.5) = 1, and log(1) = 0. This is a common choice, but choosing the variance is tricky because there is no closed form solution, so we would have to pick one based on how informative we want it to be. * A Cauchy distribution with mean 0 (for the same reason) and scale 2.5. This is based on the recommendation of Gelman et al (2008) (http://www.stat.columbia.edu/~gelman/research/published/priors11.pdf). There is still no way to properly estimate the variance of the intercept, so our choice is again going to affect our outcomes.\n\ndef logistic_model(data):  \n    # Create parameters for the distribution on the correct device\n    mean = torch.tensor(0., device=device)  # Ensure mean is a tensor on GPU\n    scale = torch.tensor(2.5, device=device)  # Ensure standard deviation is a tensor on GPU\n    \n    # Create the Cauchy distribution with parameters on the correct device\n    prior_dist = dist.Cauchy(mean, scale)\n    \n    # Sample intercept from the distribution\n    intercept = pyro.sample(\"intercept\", prior_dist)\n    \n    # Observing the data with Bernoulli likelihood\n    with pyro.plate(\"data_plate\", len(data)):\n        pyro.sample(\"obs\", dist.Bernoulli(logits=intercept), obs=data)\n\nThen we repeat our sampling. I repeated this a few times before settling on 2000 samples for decent convergence.\n\n#NUTS sampler\nnuts_kernel = NUTS(logistic_model)\nmcmc = MCMC(nuts_kernel, num_samples=2000, warmup_steps=200)\nmcmc.run(data)\n\nSample: 100%|██████████| 2200/2200 [00:40, 54.64it/s, step size=1.72e-01, acc. prob=0.923] \n\n\nAnd again check our model convergence. To plot these, we’ll also need to transfer these back to our CPU. We expect a normal distribution for the intercept parameter if the model converged. There are better ways to check the convergence such as Effective Sample Size (ESS) or Gelman-Rubin convergence, but they require running multiple chains which is finnicky on Windows.\n\n# Check model convergence\nintercept_df = pd.DataFrame({'intercept' : mcmc.get_samples()['intercept'].to('cpu')})\nintercept_df.hist()\n\narray([[&lt;Axes: title={'center': 'intercept'}&gt;]], dtype=object)\n\n\n\n\n\n\n\n\n\nThat doesn’t look very normal! Probably because we only have one sample. It’s possible that a distribution with smaller tails, like the normal, will work better here. Let’s try again with the normal distribution.\n\ndef logistic_model(data):  \n    # Create parameters for the distribution on the correct device\n    mean = torch.tensor(0., device=device)  # Ensure mean is a tensor on GPU\n    scale = torch.tensor(2.5, device=device)  # Ensure standard deviation is a tensor on GPU\n    \n    # Create the Normal distribution with parameters on the correct device\n    prior_dist = dist.Normal(mean, scale)\n    \n    # Sample intercept from the distribution\n    intercept = pyro.sample(\"intercept\", prior_dist)\n    \n    # Observing the data with Bernoulli likelihood\n    with pyro.plate(\"data_plate\", len(data)):\n        pyro.sample(\"obs\", dist.Bernoulli(logits=intercept), obs=data)\n\n\n#NUTS sampler\nnuts_kernel = NUTS(logistic_model)\nmcmc = MCMC(nuts_kernel, num_samples=2000, warmup_steps=200)\nmcmc.run(data)\n\nSample: 100%|██████████| 2200/2200 [00:19, 112.19it/s, step size=1.21e+00, acc. prob=0.923]\n\n\n\n# Check model convergence\nintercept_df = pd.DataFrame({'intercept' : mcmc.get_samples()['intercept'].to('cpu')})\nintercept_df.hist()\n\narray([[&lt;Axes: title={'center': 'intercept'}&gt;]], dtype=object)\n\n\n\n\n\n\n\n\n\nThat looks pretty normal now. We’ll need to turn these back into probabilities using the signmoid function.\n\n# Extract samples\nsamples = mcmc.get_samples()['intercept'].to('cpu')\n\n# Plot probabilities\ninferred_probs_df = pd.DataFrame({'probs' : torch.sigmoid(samples)})\ninferred_probs_df.hist()\n\n\ninferred_prob = torch.sigmoid(samples).mean().item()\n\nprint(f\"Inferred probability of a thumb up: {inferred_prob:.4f}\")\n\n# Compute confidence interval\nconfidence_interval_vals = torch.quantile(samples, torch.tensor([0.025, 0.975]))\nconfidence_interval = torch.sigmoid(confidence_interval_vals)\nprint(f\"95% confidence interval: {confidence_interval.tolist()}\")\n\nInferred probability of a thumb up: 0.7413\n95% confidence interval: [0.1398516297340393, 0.9961099028587341]\n\n\n\n\n\n\n\n\n\nOur original estimate using the beta distribution was 0.6649 with a 95% confidence interval: [0.1617867797613144, 0.9869610667228699]. That’s a bit different from this, but the choice of prior influences the outcome. Overall though, that looks like a pretty good estimate of the analytical solution despite only having a single sample.\nWhat happens if we increased the number of samples to 100? How accurate can we get with a moderate amount of data? Let’s say we want to model something with a 90% upvote rate, so 90 thumbs up and 10 thumbs down.\n\n# Data: 90 thumbs up, 10 thumbs down\ndata = torch.cat((torch.ones(90), torch.zeros(10)))\n\n# Rerun model with reduced steps\nnuts_kernel = NUTS(beta_model)\nmcmc = MCMC(nuts_kernel, num_samples=2000, warmup_steps=200)\nmcmc.run(data)\n\n# Extract samples\nsamples = mcmc.get_samples()\nprob_thumb_up_samples = samples['prob_thumb_up']\n\n# Make histogram of samples\nsamples_df = pd.DataFrame(samples)\nsamples_df.hist()\n\n# Compute confidence interval\nconfidence_interval = torch.quantile(prob_thumb_up_samples, torch.tensor([0.025, 0.975]))\n\nprint(f\"Estimated probability of thumb up: {prob_thumb_up_samples.mean().item():.4f}\")\nprint(f\"95% confidence interval: {confidence_interval.tolist()}\")\n\nSample: 100%|██████████| 2200/2200 [00:06, 358.01it/s, step size=1.25e+00, acc. prob=0.899]\n\n\nEstimated probability of thumb up: 0.8920\n95% confidence interval: [0.8236452341079712, 0.9434589743614197]\n\n\n\n\n\n\n\n\n\nAn estimate of 0.89 for a 90% upvoted piece of content is pretty good, and the spread on the estimate is much tighter than before.\nThis is still a really simple model, though. For more complicated models, MCMC might be prohibitively slow. For real life scenarios, it’s inlikely to be doing something with this few samples and this simple of a model. In these cases, you might want to try variational inference (VI) instead of MCMC. VI allows you to turn the sampling problem into an optimization problem, where you optimize the parameters of a distirbution directly. Here I’m going to use the Adam optimizer, which is short for adaptive gradient descent. It’s popular in lots of mcahine learning applications due to its speed and ability to handle sparse gradients. We can check to see how well Adam is optimizing by checking to see if the evidence lower bound (ELBO) converges. We can pump up our data a bit, too.\n\n# Define the guide (variational posterior)\ndef guide(data):\n    # Variational parameters for the intercept\n    mean_q = pyro.param(\"mean_q\", torch.tensor(0.0, device=device))\n    scale_q = pyro.param(\"scale_q\", torch.tensor(1.0, device=device), constraint=dist.constraints.positive)\n    \n    pyro.sample(\"intercept\", dist.Normal(mean_q, scale_q))\n\n# Data: 900 thumbs up, 100 thumbs down, and make sure its in the right place\ndata = torch.cat((torch.ones(900), torch.zeros(100))).to(device)\n\n# Set up the optimizer and inference algorithm\noptimizer = Adam({\"lr\": torch.tensor(0.001, device=device)})  # Learning rate\nsvi = SVI(logistic_model, guide, optimizer, loss=Trace_ELBO())\n\n# Perform optimization\nnum_steps = 10000\nlosses = []\nfor step in range(num_steps):\n    loss = svi.step(data)  # Perform one step of optimization\n    losses.append(loss)\n    if step % 1000 == 0:\n        print(f\"Step {step} - Loss: {loss}\")\n\n# Inspect learned parameters\nmean_q = pyro.param(\"mean_q\").item()\nscale_q = pyro.param(\"scale_q\").item()\nprint(f\"Posterior Mean: {mean_q}, Posterior Std: {scale_q}\")\n\n# Plot ELBO convergence\nimport matplotlib.pyplot as plt\nplt.plot(losses)\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"ELBO Loss\")\nplt.title(\"ELBO Convergence\")\nplt.show()\n\nStep 0 - Loss: 641.3312163352966\nStep 1000 - Loss: 543.6905220746994\nStep 2000 - Loss: 568.6181250810623\nStep 3000 - Loss: 474.34459149837494\nStep 4000 - Loss: 342.52564430236816\nStep 5000 - Loss: 355.2464487552643\nStep 6000 - Loss: 343.4673839211464\nStep 7000 - Loss: 339.71015548706055\nStep 8000 - Loss: 327.89236748218536\nStep 9000 - Loss: 330.6277858018875\nPosterior Mean: 2.189785957336426, Posterior Std: 0.15260715782642365\n\n\n\n\n\n\n\n\n\nThat elbow plot clearly converges, so it looks like the optimizer worked well. All that’s left is seeing if the estimate we got for the posterior probability of the intercept is accurate. To do that, we’ll need to transform our log odds back into a probability of thumbs up.\n\nimport math\n\n# Convert posterior mean (log-odds) to probability\nposterior_mean = pyro.param(\"mean_q\").item()\n\n# Probability from logistic function\nposterior_probability = 1 / (1 + math.exp(-posterior_mean))\n\nprint(posterior_probability)\n\n0.8993285293702843\n\n\nA perfect estimate!"
  },
  {
    "objectID": "posts/20241221-rolling-blog-posts/index.html",
    "href": "posts/20241221-rolling-blog-posts/index.html",
    "title": "Making Rolling Blog Posts",
    "section": "",
    "text": "Sometimes I stumble across something while reading and I want to be able to write and post it somewhere so that I’ll be able to access it later, but that isn’t meaningful enough to make a blog post about on its own. With this setup, I can just take a note on my phone and automatically update my blog. To do this, I use a combination of Obsidian (a notes app on my phone), Quarto (the publishing system for my personal website), and Github Actions for automatic deployment. The end result is that I can take a note on my phone in markdown format and have it automatically pushed and rendered on my website in my blog.\nThe basic workflow here is to make changes on your device in the Obsidian app, push those changes to your (private) notes repository, copy the file containing those changes to your website repository, and render them.\nYou need three things to do this: 1. The Obsidian app with the Github plugin 2. A Github repository to host your Obsidian notes (it can be private so that you can make notes without others seeing them) 3. A github repository with a Quarto project"
  },
  {
    "objectID": "posts/20241221-rolling-blog-posts/index.html#step-1-setting-up-obsidian",
    "href": "posts/20241221-rolling-blog-posts/index.html#step-1-setting-up-obsidian",
    "title": "Making Rolling Blog Posts",
    "section": "Step 1: Setting up Obsidian",
    "text": "Step 1: Setting up Obsidian\n\nDownload the Obsidian app from your mobile store\nInside Obsidian, download the Git Community plugin\nOn Github, create a Personal Access Token linked to your account. This token will need to have read/write privileges for your repository, so the “repo” box needs to be checked when setting up your token. SAVE THIS TOKEN FOR LATER.\nConfigure your Git plugin inside Obsidian to use your username, email, and your new Personal Access Token.\nLink your notes repository on Github to your Obsidian app.\n\nNow, any time you make changes to your files, you can go to the bottom right hamburger menu &gt; Open command palette &gt; Git: to commit and push your changes to your Git repository. (Note: for these commands to appear, you need to be in read mode rather than write mode, which can be toggled by clicking the book/pencil in the top right corner.)"
  },
  {
    "objectID": "posts/20241221-rolling-blog-posts/index.html#step-2-setting-up-your-rolling-blog-post",
    "href": "posts/20241221-rolling-blog-posts/index.html#step-2-setting-up-your-rolling-blog-post",
    "title": "Making Rolling Blog Posts",
    "section": "Step 2: Setting up your rolling blog post",
    "text": "Step 2: Setting up your rolling blog post\n\nIn Obsidian, create a new file, then commit your changes and push the file to your repository. 2. Verify that your file appears in your github repository after pushing. It should be a markdown file with the name {name}.md. I’m using a post title machine_learning_basics to do this, which appears as machine_learning_basics.md in my repository. If it’s there, everything is working!\nIn your Quarto site, create a new blog post. I have mine set up to have my blog posts contained inside individual directories (see my example here. To set up a new blog post, I created the folder 20241219-machine-learning-basics. Inside that folder, create the file index.qmd with the following contents:\n\n---\ntitle: \"Machine Learning Basics\"\nauthor: \"Daniel Geiszler\"\ndate: \"2024-12-19\"\ncategories: [machine learning]\n---\n\n\\{{&lt; include machine_learning_basics.md &gt;}}\nThe first few lines are metadata for the blog post. The following statement populated the rendered blog post with the contents of another markdown file. Eventually, this markdown file will exist and be populated by your post’s content, but for now we can forget about it. However, if you want to preview your site using quarto preview to make sure it’s working, you’ll need to create an empty markdown file with the above file name in this directory."
  },
  {
    "objectID": "posts/20241221-rolling-blog-posts/index.html#step-3-set-up-github-actions-to-automatically-push-changes-to-your-website-repository",
    "href": "posts/20241221-rolling-blog-posts/index.html#step-3-set-up-github-actions-to-automatically-push-changes-to-your-website-repository",
    "title": "Making Rolling Blog Posts",
    "section": "Step 3: Set up Github actions to automatically push changes to your website repository",
    "text": "Step 3: Set up Github actions to automatically push changes to your website repository\n\nGo back to your Obsidian repository and add a repository secret. This prevents your personal access token allowing read/write access from being public and is a must for security purposes. Inside your repository, go to Settings &gt; Secrets and variables, then under Secrets add your personal access token from before as a new repository secret with the name PERSONAL_ACCESS_TOKEN.\ncreate a folder called .github/workflows if it doesn’t already exist. You may need to pull these changes to your Obsidian app before pushing any subsequent changes on your device.\nCreate a new yaml file titled something like “sync_blogpost.yml” with the following contents. You’ll need to change the fields marked with comments to match your fields. (Note: The inline comments change from # to // because some are yaml comments and some are javascript comments.)\n\nname: Sync Machine Learning Basics File # Change this to the name of your workflow\n\non:\n  push:\n    paths:\n      - 'machine_learning_basics.md'  # Watch for changes in this specific file in your notes repo\n\njobs:\n  update-file:\n    runs-on: ubuntu-latest\n    steps:\n    - name: Checkout source repo\n      uses: actions/checkout@v3\n\n    - name: Setup Node.js\n      uses: actions/setup-node@v3\n      with:\n        node-version: '16'\n\n    - name: Fetch SHA and Update File\n      uses: actions/github-script@v6\n      with:\n        github-token: ${{ secrets.PERSONAL_ACCESS_TOKEN }}\n        script: |\n          const fs = require('fs');\n          const path = require('path');\n          const filePath = path.join(process.env.GITHUB_WORKSPACE, 'machine_learning_basics.md'); // Change this to the name of your blog post's file\n          const content = fs.readFileSync(filePath, 'utf8');\n          const encodedContent = Buffer.from(content).toString('base64');\n\n          // Fetch the current SHA of the file to be updated\n          const response = await github.rest.repos.getContent({\n            owner: 'danielgeiszler', // Change this to your github username\n            repo: 'personalwebsite', // Change this to your website's repository\n            path: 'posts/20241219-machine-learning-basics/machine_learning_basics.md', // Change this to the path inside your website repository\n            ref: 'main'  // Make sure to use the correct branch here\n          });\n\n          const sha = response.data.sha;\n\n          // Update the file with the new content and the current SHA\n          const updateResponse = await github.rest.repos.createOrUpdateFileContents({\n            owner: 'danielgeiszler', // Change this to your github username\n            repo: 'personalwebsite', // Change this to your website's repository\n            path: 'posts/20241219-machine-learning-basics/machine_learning_basics.md', // Change this to the path inside your website repository\n            message: 'Update machine learning basics', // Change this to the message displayed for your action\n            content: encodedContent,\n            sha: sha,  // Provide the SHA fetched from the previous step\n            branch: 'main',  // Make sure this is the correct branch\n            committer: {\n              name: process.env.GITHUB_ACTOR,\n              email: `${process.env.GITHUB_ACTOR}@users.noreply.github.com`\n            },\n            author: {\n              name: process.env.GITHUB_ACTOR,\n              email: `${process.env.GITHUB_ACTOR}@users.noreply.github.com`\n            }\n          });\n\n          console.log(\"File updated\", updateResponse.data);\nThat’s it! Now you can write notes on your phone and have them automatically added to your blog post."
  },
  {
    "objectID": "posts/20241221-rolling-blog-posts/rolling_blog_posts.html",
    "href": "posts/20241221-rolling-blog-posts/rolling_blog_posts.html",
    "title": "Daniel Geiszler",
    "section": "",
    "text": "Sometimes I stumble across something while reading and I want to be able to write and post it somewhere so that I’ll be able to access it later, but that isn’t meaningful enough to make a blog post about on its own. With this setup, I can just take a note on my phone and automatically update my blog. To do this, I use a combination of Obsidian (a notes app on my phone), Quarto (the publishing system for my personal website), and Github Actions for automatic deployment. The end result is that I can take a note on my phone in markdown format and have it automatically pushed and rendered on my website in my blog.\nThe basic workflow here is to make changes on your device in the Obsidian app, push those changes to your (private) notes repository, copy the file containing those changes to your website repository, and render them.\nYou need three things to do this: 1. The Obsidian app with the Github plugin 2. A Github repository to host your Obsidian notes (it can be private so that you can make notes without others seeing them) 3. A github repository with a Quarto project"
  },
  {
    "objectID": "posts/20241221-rolling-blog-posts/rolling_blog_posts.html#step-1-setting-up-obsidian",
    "href": "posts/20241221-rolling-blog-posts/rolling_blog_posts.html#step-1-setting-up-obsidian",
    "title": "Daniel Geiszler",
    "section": "Step 1: Setting up Obsidian",
    "text": "Step 1: Setting up Obsidian\n\nDownload the Obsidian app from your mobile store\nInside Obsidian, download the Git Community plugin\nOn Github, create a Personal Access Token linked to your account. This token will need to have read/write privileges for your repository, so the “repo” box needs to be checked when setting up your token. SAVE THIS TOKEN FOR LATER.\nConfigure your Git plugin inside Obsidian to use your username, email, and your new Personal Access Token.\nLink your notes repository on Github to your Obsidian app.\n\nNow, any time you make changes to your files, you can go to the bottom right hamburger menu &gt; Open command palette &gt; Git: to commit and push your changes to your Git repository. (Note: for these commands to appear, you need to be in read mode rather than write mode, which can be toggled by clicking the book/pencil in the top right corner.)"
  },
  {
    "objectID": "posts/20241221-rolling-blog-posts/rolling_blog_posts.html#step-2-setting-up-your-rolling-blog-post",
    "href": "posts/20241221-rolling-blog-posts/rolling_blog_posts.html#step-2-setting-up-your-rolling-blog-post",
    "title": "Daniel Geiszler",
    "section": "Step 2: Setting up your rolling blog post",
    "text": "Step 2: Setting up your rolling blog post\n\nIn Obsidian, create a new file, then commit your changes and push the file to your repository. 2. Verify that your file appears in your github repository after pushing. It should be a markdown file with the name {name}.md. I’m using a post title machine_learning_basics to do this, which appears as machine_learning_basics.md in my repository. If it’s there, everything is working!\nIn your Quarto site, create a new blog post. I have mine set up to have my blog posts contained inside individual directories (see my example here. To set up a new blog post, I created the folder 20241219-machine-learning-basics. Inside that folder, create the file index.qmd with the following contents (without the backslah to escape to include command}:\n\n---\ntitle: \"Machine Learning Basics\"\nauthor: \"Daniel Geiszler\"\ndate: \"2024-12-19\"\ncategories: [machine learning]\n---\n\n\\{{&lt; include machine_learning_basics.md &gt;}}\nThe first few lines are metadata for the blog post. The following statement populated the rendered blog post with the contents of another markdown file. Eventually, this markdown file will exist and be populated by your post’s content, but for now we can forget about it. However, if you want to preview your site using quarto preview to make sure it’s working, you’ll need to create an empty markdown file with the above file name in this directory."
  },
  {
    "objectID": "posts/20241221-rolling-blog-posts/rolling_blog_posts.html#step-3-set-up-github-actions-to-automatically-push-changes-to-your-website-repository",
    "href": "posts/20241221-rolling-blog-posts/rolling_blog_posts.html#step-3-set-up-github-actions-to-automatically-push-changes-to-your-website-repository",
    "title": "Daniel Geiszler",
    "section": "Step 3: Set up Github actions to automatically push changes to your website repository",
    "text": "Step 3: Set up Github actions to automatically push changes to your website repository\n\nGo back to your Obsidian repository and add a repository secret. This prevents your personal access token allowing read/write access from being public and is a must for security purposes. Inside your repository, go to Settings &gt; Secrets and variables, then under Secrets add your personal access token from before as a new repository secret with the name PERSONAL_ACCESS_TOKEN.\ncreate a folder called .github/workflows if it doesn’t already exist. You may need to pull these changes to your Obsidian app before pushing any subsequent changes on your device.\nCreate a new yaml file titled something like “sync_blogpost.yml” with the following contents. You’ll need to change the fields marked with comments to match your fields. (Note: The inline comments change from # to // because some are yaml comments and some are javascript comments.)\n\nname: Sync Machine Learning Basics File # Change this to the name of your workflow\n\non:\n  push:\n    paths:\n      - 'machine_learning_basics.md'  # Watch for changes in this specific file in your notes repo\n\njobs:\n  update-file:\n    runs-on: ubuntu-latest\n    steps:\n    - name: Checkout source repo\n      uses: actions/checkout@v3\n\n    - name: Setup Node.js\n      uses: actions/setup-node@v3\n      with:\n        node-version: '16'\n\n    - name: Fetch SHA and Update File\n      uses: actions/github-script@v6\n      with:\n        github-token: ${{ secrets.PERSONAL_ACCESS_TOKEN }}\n        script: |\n          const fs = require('fs');\n          const path = require('path');\n          const filePath = path.join(process.env.GITHUB_WORKSPACE, 'machine_learning_basics.md'); // Change this to the name of your blog post's file\n          const content = fs.readFileSync(filePath, 'utf8');\n          const encodedContent = Buffer.from(content).toString('base64');\n\n          // Fetch the current SHA of the file to be updated\n          const response = await github.rest.repos.getContent({\n            owner: 'danielgeiszler', // Change this to your github username\n            repo: 'personalwebsite', // Change this to your website's repository\n            path: 'posts/20241219-machine-learning-basics/machine_learning_basics.md', // Change this to the path inside your website repository\n            ref: 'main'  // Make sure to use the correct branch here\n          });\n\n          const sha = response.data.sha;\n\n          // Update the file with the new content and the current SHA\n          const updateResponse = await github.rest.repos.createOrUpdateFileContents({\n            owner: 'danielgeiszler', // Change this to your github username\n            repo: 'personalwebsite', // Change this to your website's repository\n            path: 'posts/20241219-machine-learning-basics/machine_learning_basics.md', // Change this to the path inside your website repository\n            message: 'Update machine learning basics', // Change this to the message displayed for your action\n            content: encodedContent,\n            sha: sha,  // Provide the SHA fetched from the previous step\n            branch: 'main',  // Make sure this is the correct branch\n            committer: {\n              name: process.env.GITHUB_ACTOR,\n              email: `${process.env.GITHUB_ACTOR}@users.noreply.github.com`\n            },\n            author: {\n              name: process.env.GITHUB_ACTOR,\n              email: `${process.env.GITHUB_ACTOR}@users.noreply.github.com`\n            }\n          });\n\n          console.log(\"File updated\", updateResponse.data);\nThat’s it! Now you can write notes on your phone and have them automatically added to your blog post."
  },
  {
    "objectID": "posts/20241219-machine-learning-basics/machine_learning_basics.html",
    "href": "posts/20241219-machine-learning-basics/machine_learning_basics.html",
    "title": "Daniel Geiszler",
    "section": "",
    "text": "This is a running list of machine learning architectures, terms, and concepts. I write them down here for quick reference in the future. If you see an issue, please let me know!"
  },
  {
    "objectID": "posts/20241219-machine-learning-basics/machine_learning_basics.html#decision-trees",
    "href": "posts/20241219-machine-learning-basics/machine_learning_basics.html#decision-trees",
    "title": "Daniel Geiszler",
    "section": "Decision Trees",
    "text": "Decision Trees\nDecision trees segregate data based on rules until a stopping condition is met, usually that subgroups are pure or a maximum depth is reached. Trees can predict categorical data (classification trees) and numerical data (regression trees). Advantages include fast predictions, intelligibility, and low compute requirements. Disadvantages include being prone to overfitting and instability (i.e., adding more data can cause the entire tree to be reconstructed).\nBranches are decided based on splitting criteria. Examples of splitting criteria include:\nGini impurity: measure how frequently a randomly chosen entry would be mislabeled based on the branch rule\nInformation gain: compared the entropy before and after splitting to see how much information is gained after the split\nMean squared error (regression trees): rules are chosen minimize variance in predicted values\nTrees can be “regularized” via pruning to prevent overfitting. This can happen in two ways:\nPre-pruning (early stopping): preventing new branches from being formed after a certain threshold or depth\nPost-pruning (cost-complexity pruning): post-box removal of branches that don’t produce any gains in validation data\n\nRandom Forests\nRandom forests solve a single decision tree’s problem with overfitting by producing an ensemble of decision trees. Decision trees are fit for random subsets of the data, then averaged (regression) or vote on the outcome (classification) during prediction. The process of averaging trees that are trained in parallel is called “bagging”.\n\n\nBoosted Trees\nBoosted trees compensate for the weakness of an initial tree by building sequential trees to correct errors in previous trees. The process of averaging trees that are trained sequentially is called boosting. Some key algorithms related to boosting:\nAdaBoost: increases the weight of incorrectly predicted data in subsequent trees\nGradient boosting: a specific type of boosting where a differentiable loss function is used to fit sequential models on the residuals of the previous model\nXGBoost: a popular library implementing gradient-boosted decision trees that is optimized for efficiency"
  },
  {
    "objectID": "posts/20241219-machine-learning-basics/machine_learning_basics.html#regularization",
    "href": "posts/20241219-machine-learning-basics/machine_learning_basics.html#regularization",
    "title": "Daniel Geiszler",
    "section": "Regularization",
    "text": "Regularization\nRegularization is the process by which we penalize model complexity, done to reduce overfitting and make models more generalizable. This can be split into two broad categories, explicit regularization—where a regularizing term, be it a penalty, prior, or constraint, is added to directly the optimization problem, and implicit regularization, which broadly encompasses other methods of preventing overfitting.\nThese are some of the most commonly types of explicit regularization:\nLasso (L1 regularization): adds a penalty proportional to the absolute values of the model parameters, making some parameters go to zero (get dropped out) during optimization. It can be used to perform feature selection if you suspect there are irrelevant variables.\nRidge regression (L2 regularization): adds a penalty proportional to the sum of squares of the model parameters, shrinking their coefficients to zero but generally without dropping them out of the optimization altogether. It can be used to reduce the weights of highly correlated variables in tandem, whereas L1 regularization might select a single feature and drop the others. It is generally more stable than L1 regularization because it doesn’t remove parameters.\nElastic Net: combines L1 and L2 regularization, simultaneously shrinking coefficients while performing feature selection. The relative weight of the L1 and L2 contributions to regularization are controlled by the \\(\\alpha\\) parameter."
  },
  {
    "objectID": "posts/20250125-retrieval-augmented-generation/RagSystem.html",
    "href": "posts/20250125-retrieval-augmented-generation/RagSystem.html",
    "title": "Retrieval Agumented Generation with DeepSeek",
    "section": "",
    "text": "Retrieval-Augmented Generation (RAG) systems can be really useful when you would like to use LLMs but require their output to be well-sourced or you need to draw from locally stored, proprietary data that doesn’t exist within the training set. In practice, a lot of LLMs in the real world are used this way so that users can add and remove information from their knowledge base easily.\nThis walks through how to create a simple RAG system using the DeepSeek API. As a toy example to make sure that it’s working as intended, we’re going to give it some bad information about the Changzhou dialect of Chinese that definitely doesn’t exist in its training set (it’s what I happened to have open at the time). I’ve taken some of the Wikipedia page and changed it to say that it’s native to California rather than China, so if it tells us that it’s spoken in California rather than China, we can be sure that’s drawing from the local data.\nThis notebook can be found on GitHub. There’s also a program to run this system from the command line. To start with this notebook, you’ll need python version 3.12.X due to some versioning conflicts at the time of writing with 3.13.X. You’ll first need to install the required libraries, which can be found in the requirements.txt file from the repository. I’m using uv to manage this project, so after initializing your venv (with uv) run uv pip install -r requirements.txt. Once you’ve done that, you’ll need to create two more files. First, create a .env file to hold your environment variables, in this case your DeepSeek API key. It should only have one line reading DEEPSEEK_API_KEY=your_key_here. Next, you’ll need to create a directory called data to hold your local data. In this case, you can place the changzhou.txt file inside.\nOnce that’s done, we can load our environment. Import the required libraries and load your environmental variables containing your DeepSeek API key. nltk is a natural language toolkit that we’ll use under the hood for tokenization, and it requires downloading some modules separately.\n\nimport os\nfrom dotenv import load_dotenv\nimport nltk\nimport gradio as gr\nfrom openai import OpenAI\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.chains import RetrievalQA\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.runnables import Runnable\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_community.document_loaders import TextLoader\nfrom tqdm.auto import tqdm\nimport traceback\nfrom pathlib import Path\n\n\nload_dotenv()\nnltk.download('punkt')\nnltk.download('punkt_tab')\n\nC:\\Users\\danny\\BlogPosts\\RagSystemFresh\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\danny\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package punkt_tab to\n[nltk_data]     C:\\Users\\danny\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n\n\nTrue\n\n\nThe first thing we’ll need to so is create a Runnable class to handle our API queries. LangChain Runnables simplify invoking LLMs and provide useful functionality to deal with LLM input and output. It needs to contain our API key, the DeepSeek url, specifications for the query, and methods for dealing with the response.\n\nclass DeepSeekRunnable(Runnable):\n    def __init__(self):\n        self.client = OpenAI(\n            api_key=os.getenv(\"DEEPSEEK_API_KEY\"),\n            base_url=\"https://api.deepseek.com/v1\",\n        )\n    \n    def invoke(self, input: dict, config: dict = None, **kwargs):\n        \"\"\"Handle LangChain compatibility\"\"\"\n        try:\n            query = input.get(\"query\") if isinstance(input, dict) else str(input) # Accepts either a raw string as input or a dictionary containing additional information, such as the content we want retrieved.\n            \n            response = self.client.chat.completions.create(\n                model=\"deepseek-chat\", # Specifies the chat model\n                messages=[{\"role\": \"user\", \"content\": query}], # Structures the the conversation history. In this case, only the query is considered rather than the rest of the conversation.\n                temperature=0.3, # Determines how creative you want the model to be in its responses. Higher temperature means more creativity.\n                **kwargs\n            )\n            \n            return response.choices[0].message.content # Extracts the message content from the response\n            \n        except Exception as e:\n            return f\"Error: {str(e)}\"\n\nThen we need to preprocess the data we have in our data folder. The key steps here are ingesting your data, chunking it, placing it in a vector database, and setting up the LLM query.\nDocuments are split recursively, meaning it first splits on paragraphs, then sentences, then words (it splits on different whitespace separators to approximate this), which allows the model to preserve the context around words. Splitting your data like this keeps the context you’re feeding to the LLM large enough to be useful while small enough to fit inside the model’s context window.\nAfter this, we need to embed the documents in semantic space so that documents relevant to each other are in the same neighborhood. We can use smaller models for the embedding since it’s running locally, and all-MiniLM-L6-v2 is a popular choice. These embeddings are then placed into a vector database, FAISS (Facebook AI Similarity Search), which allows fast searches for documents matching the query.\nWe then need to structure our query to the server. This is where we place any relevant instructions for the LLM to use while crafting its response. There a few moving pieces here: the template, the context, and tracking answer origins. The Template gives explicit instructions to the LLM. The Context is the local documents that are being fed into the LLM. The prompt and these source documents are “stuffed” into a single prompt that the LLM actually recieves. We also need to return the source documents so that we can trace the origins of the answer, and while for larger document corpora it can actually cite the sources you won’t notice a difference here.\n\ndef safe_load_documents(directory: str):\n    documents = []\n    errors = []\n    \n    # Get all .txt files, excluding hidden files/directories\n    txt_files = [\n        f for f in Path(directory).rglob(\"*.txt\") \n        if not any(part.startswith(\".\") for part in f.parts)\n    ]\n    print(f\"Found {len(txt_files)} files to process\")\n    \n    for file in tqdm(txt_files, desc=\"Loading files\"):\n        try:\n            loader = TextLoader(str(file), autodetect_encoding=True)\n            docs = loader.load()\n            documents.extend(docs)\n        except Exception as e:\n            errors.append((str(file), str(e)))\n            continue\n            \n    print(f\"Success: {len(documents)} docs | Failed: {len(errors)}\")\n    if errors:\n        print(\"First 5 errors:\")\n        for file, error in errors[:5]:\n            print(f\" - {file}: {error}\")\n    \n    return documents\n    \n\ndef initialize_rag():\n    try:\n        # 1. Load and split documents\n        documents = safe_load_documents(\"./data\")\n        \n        text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=500,\n            chunk_overlap=50\n        )\n        chunks = text_splitter.split_documents(documents)\n\n        # 2. Create vector store with updated embeddings\n        embed_model = HuggingFaceEmbeddings(\n            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n        )\n        vector_db = FAISS.from_documents(chunks, embed_model)\n        retriever = vector_db.as_retriever(search_kwargs={\"k\": 3})\n\n        # 3. Create RAG chain\n        template = \"\"\"Use the context below to answer. If unsure, say \"I don't know\". \n        \n        Context: {context}\n        Question: {question}\n        Answer:\"\"\"\n        \n        prompt = PromptTemplate(\n            template=template,\n            input_variables=[\"context\", \"question\"]\n        )\n\n        return RetrievalQA.from_chain_type(\n            llm=DeepSeekRunnable(),\n            chain_type=\"stuff\",\n            retriever=retriever,\n            chain_type_kwargs={\"prompt\": prompt},\n            return_source_documents=True,\n            input_key=\"query\"\n        ).with_config(run_name=\"DeepSeekRAG\")\n        \n    except Exception as e:\n        print(f\"Initialization failed: {str(e)}\")\n        exit(1)\n\n# Initialize system\nrag_chain = initialize_rag()\n\nFound 1 files to process\nSuccess: 1 docs | Failed: 0\n\n\nLoading files: 100%|██████████| 1/1 [00:00&lt;00:00, 82.81it/s]\n\n\nFinally, we’ll need to define an interface to work with this LLM. Gradio makes this super simple. Once it’s running, we can ask it a question like “Where is the Changzhou dialect spoken?” to see if it’s using the local information.\n\n# WARNING: This flag exists to render the notebook properly when not being executed. Set it to false if running the notebook yourself.\nis_quarto_render = True # \n# This if block only exists to render the content of the document, it can be disregarded when running.\nif is_quarto_render:\n    from IPython.display import Image, display\n    display(Image(filename='DeepSeekRag_1.png'))\nelse:\n    # Only run Gradio interface when not in Quarto\n    def ask(question):\n        try:\n            response = rag_chain.invoke({\"query\": question})\n            return response['result']\n        except Exception as e:\n            return f\"Error: {str(e)}\"\n\n    # Launch Gradio interface\n    gr.Interface(\n        fn=ask,\n        inputs=gr.Textbox(label=\"Question\"),\n        outputs=gr.Textbox(label=\"Answer\"),\n        title=\"DeepSeek Document Assistant\"\n    ).launch()\n\n\n\n\n\n\n\n\nThe response I get is “The Changzhou dialect is spoken in the city of Changzhou and surrounding areas in China. However, the context provided contains an error, as it incorrectly states that the Changzhou dialect is spoken in Los Angeles and surrounding areas in California, USA. This is not accurate. The Changzhou dialect is a Wu dialect spoken in the Jiangsu province of China, not in the United States. Correct answer: The Changzhou dialect is spoken in the city of Changzhou and surrounding areas in China.”\nI think I would prefer an LLM that relies more on the retrieved knowledge and less on prior knowledge. It should be telling us ONLY what is in its local data–that the Changzhou dialect is spoken in California–and should not be giving us information about the real location which is in its training data. We can adjust this my adjusting the template in the initialize_rag() function below. Our template currently says to use the context to answer. Let’s see if we can convince the LLM to rely more on local data. We’ll give it an attitude adjustment and ask it the same question.\n\ndef initialize_rag():\n    try:\n        # 1. Load and split documents\n        documents = safe_load_documents(\"./data\")\n        \n        text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=500,\n            chunk_overlap=50\n        )\n        chunks = text_splitter.split_documents(documents)\n\n        # 2. Create vector store with updated embeddings\n        embed_model = HuggingFaceEmbeddings(\n            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n        )\n        vector_db = FAISS.from_documents(chunks, embed_model)\n        retriever = vector_db.as_retriever(search_kwargs={\"k\": 3})\n\n        # 3. Create RAG chain\n        template = \"\"\"Use the context below to answer. If unsure, say \"I don't know\". Only use local information in your answers, \n        ignore what you have learned previously. This is a VERY specialized use case and it's critical that the information you're\n        using is as relevant and up-to-date as possible, so we can't rely on anything outdated that you may have seen before. The\n        information being provided to you is absolutely correct, so there is no need to question it.\n        \n        Context: {context}\n        Question: {question}\n        Answer:\"\"\"\n        \n        prompt = PromptTemplate(\n            template=template,\n            input_variables=[\"context\", \"question\"]\n        )\n\n        return RetrievalQA.from_chain_type(\n            llm=DeepSeekRunnable(),\n            chain_type=\"stuff\",\n            retriever=retriever,\n            chain_type_kwargs={\"prompt\": prompt},\n            return_source_documents=True,\n            input_key=\"query\"\n        ).with_config(run_name=\"DeepSeekRAG\")\n        \n    except Exception as e:\n        print(f\"Initialization failed: {str(e)}\")\n        exit(1)\n\n# Initialize system\nrag_chain = initialize_rag()\n\nFound 1 files to process\nSuccess: 1 docs | Failed: 0\n\n\nLoading files: 100%|██████████| 1/1 [00:00&lt;00:00, 248.85it/s]\n\n\n\n# WARNING: This flag exists to render the notebook properly when not being executed. Set it to false if running the notebook yourself.\nis_quarto_render = True # \n# This if block only exists to render the content of the document, it can be disregarded when running.\nif is_quarto_render:\n    from IPython.display import Image, display\n    display(Image(filename='DeepSeekRag_2.png'))\nelse:\n    # Only run Gradio interface when not in Quarto\n    def ask(question):\n        try:\n            response = rag_chain.invoke({\"query\": question})\n            return response['result']\n        except Exception as e:\n            return f\"Error: {str(e)}\"\n\n    # Launch Gradio interface\n    gr.Interface(\n        fn=ask,\n        inputs=gr.Textbox(label=\"Question\"),\n        outputs=gr.Textbox(label=\"Answer\"),\n        title=\"DeepSeek Document Assistant\"\n    ).launch()\n\n\n\n\n\n\n\n\nThat’s much better, and it looks like it works exactly as anticipated using RAG.\nThis is just a toy example of how to set up a RAG system. There are a few limitations and things you could change to make it more applicable in practice:\n\nIncreasing the number of documents returned will give it more context for the question, but be wary of how big your context window is.\nYou don’t need to regenerate embeddings every time you run this. It would be more efficient to store and load them.\nYou can alter whether you use previous responses in your context window if you need to retain a memory of what you asked previously.\nThese documents are being sent to and processed on the DeepSeek servers, which means that it isn’t ideal for sensitive data.\n\nHappy RAGging!"
  },
  {
    "objectID": "posts/20250125-retrieval-augmented-generation/RagSystemExecuted.html",
    "href": "posts/20250125-retrieval-augmented-generation/RagSystemExecuted.html",
    "title": "Retrieval Augmented Generation with DeepSeek",
    "section": "",
    "text": "Retrieval Augmented Generation (RAG) systems can be really useful when you would like to use LLMs but require their output to be well-sourced or you need to draw from locally stored, proprietary data that doesn’t exist within the training set. In practice, a lot of LLMs in the real world are used this way so that users can add and remove informaiton easily. This walks through how to create a simple retrieval augmented generation (RAG) system using the DeepSeek api. As a toy example to make sure that it’s working as intended, we’re going to give it some bad information about the Changzhou dialect of Chinese (it’s what I happened to have open at the time) that definitely doesn’t exist in its training set. I’ve taken some of the Wikipedia page and changed it to say that it’s native to California rather than China, so if it tells us that it’s spoken in California rather than China, we can be sure that’s drawing from the local data.\nThe full system can be found on github at XXX. There’s also a slightly modified script to run it from the command line. To start, you’ll need python version 3.12.X due to some versioning conflicts at the time of writing with 3.13.X. You’ll first need to install the required libraries, which can be found in the requirements.txt(XXX) file from the repository. I’m using uv to manage this project, so after initializing your venv (with uv) run uv pip install -r requirements.txt. Once you’ve done that, you’ll need to create two more files. First, create a .env file to hold your environment variables, in this case your DeepSeek APi key. It should only have one line reading DEEPSEEK_API_KEY=your_key_here. Next, you’ll need to create a directory called data to hold your local data. In this case, you can place the changzhou.txtXXX file inside.\nOnce that’s done, we can load our environment. Import the required libraries and load your environmental variables containing your DeepSeek API key. nltk is a natural language toolkit that we’ll use under the hood for tokenization, and it requires downloading some modules separately.\n\nimport os\nfrom dotenv import load_dotenv\nimport nltk\nimport gradio as gr\nfrom openai import OpenAI\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.chains import RetrievalQA\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.runnables import Runnable\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_community.document_loaders import TextLoader\nfrom tqdm.auto import tqdm\nimport traceback\nfrom pathlib import Path\n\n\nload_dotenv()\nnltk.download('punkt')\nnltk.download('punkt_tab')\n\nC:\\Users\\danny\\BlogPosts\\RagSystemFresh\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\danny\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package punkt_tab to\n[nltk_data]     C:\\Users\\danny\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n\n\nTrue\n\n\nThe first thing we’ll need to so is create a Runnable class to handle our API queries. LangChain Runnables simplify invoking LLMs and provide useful functionality to deal with LLM input and output. It needs to contain our API key, the DeepSeek url, specifications for the query, and methods for dealing with the response.\n\nclass DeepSeekRunnable(Runnable):\n    def __init__(self):\n        self.client = OpenAI(\n            api_key=os.getenv(\"DEEPSEEK_API_KEY\"),\n            base_url=\"https://api.deepseek.com/v1\",\n        )\n    \n    def invoke(self, input: dict, config: dict = None, **kwargs):\n        \"\"\"Handle LangChain compatibility\"\"\"\n        try:\n            query = input.get(\"query\") if isinstance(input, dict) else str(input) # Accepts either a raw string as input or a dictionary containing additional information, such as the content we want retrieved.\n            \n            response = self.client.chat.completions.create(\n                model=\"deepseek-chat\", # Specifies the chat model\n                messages=[{\"role\": \"user\", \"content\": query}], # Structures the the conversation history. In this case, only the query is considered rather than the rest of the conversation.\n                temperature=0.3, # Determines how creative you want the model to be in its responses. Higher temperature means more creativity.\n                **kwargs\n            )\n            \n            return response.choices[0].message.content # Extracts the message content from the response\n            \n        except Exception as e:\n            return f\"Error: {str(e)}\"\n\nThen we need to preprocess the data we have in our data folder. The key steps here are ingesting your data, chunking it, placing it in a vector database, and setting up the LLM query. Documents are split recursively, meaning it first splits on paragraphs, then sentences, then words (it splits on different whitespace separators to approximate this), which allows the model to preserve the context around words. Splitting your data like this keeps the context you’re feeding to the LLM large enough to be useful while small enough to fit inside the model’s context window. After this, we need to embed the documents in semantic space so that documents relevant to each other are nearby. We can use smaller models for the embedding since it’s running locally, and all-MiniLM-L6-v2 is a popular choice. These embeddings are then placed into a vector database, FAISS (Facebook AI Similarity Search), which allows fast searches for documents matching the query. We then need to structure our query to the server. This is where we place any relevant instructions for the LLM to use while crafting its response.\nThere a few moving pieces here: the template, the context, and tracking answer origins. The Template gives explicit instructions to the LLM. The Context is the local documents that are being fed into the LLM. The prompt and these documents are “stuffed” into a single prompt that the LLM actually recieves. We also need to return the source documents so that we can trace the origins of the answer, although our input is so small here that it won’t make a difference.\n\ndef safe_load_documents(directory: str):\n    documents = []\n    errors = []\n    \n    # Get all .txt files, excluding hidden files/directories\n    txt_files = [\n        f for f in Path(directory).rglob(\"*.txt\") \n        if not any(part.startswith(\".\") for part in f.parts)\n    ]\n    print(f\"Found {len(txt_files)} files to process\")\n    \n    for file in tqdm(txt_files, desc=\"Loading files\"):\n        try:\n            loader = TextLoader(str(file), autodetect_encoding=True)\n            docs = loader.load()\n            documents.extend(docs)\n        except Exception as e:\n            errors.append((str(file), str(e)))\n            continue\n            \n    print(f\"Success: {len(documents)} docs | Failed: {len(errors)}\")\n    if errors:\n        print(\"First 5 errors:\")\n        for file, error in errors[:5]:\n            print(f\" - {file}: {error}\")\n    \n    return documents\n    \n\ndef initialize_rag():\n    try:\n        # 1. Load and split documents\n        documents = safe_load_documents(\"./data\")\n        \n        text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=500,\n            chunk_overlap=50\n        )\n        chunks = text_splitter.split_documents(documents)\n\n        # 2. Create vector store with updated embeddings\n        embed_model = HuggingFaceEmbeddings(\n            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n        )\n        vector_db = FAISS.from_documents(chunks, embed_model)\n        retriever = vector_db.as_retriever(search_kwargs={\"k\": 3})\n\n        # 3. Create RAG chain\n        template = \"\"\"Use the context below to answer. If unsure, say \"I don't know\". \n        \n        Context: {context}\n        Question: {question}\n        Answer:\"\"\"\n        \n        prompt = PromptTemplate(\n            template=template,\n            input_variables=[\"context\", \"question\"]\n        )\n\n        return RetrievalQA.from_chain_type(\n            llm=DeepSeekRunnable(),\n            chain_type=\"stuff\",\n            retriever=retriever,\n            chain_type_kwargs={\"prompt\": prompt},\n            return_source_documents=True,\n            input_key=\"query\"\n        ).with_config(run_name=\"DeepSeekRAG\")\n        \n    except Exception as e:\n        print(f\"Initialization failed: {str(e)}\")\n        exit(1)\n\nFinally, we’ll need to define an interface to work with this LLM. Gradio makes this super simple. Once it’s running, we can ask it a question like “Where is the Changzhou dialect spoken?” to see if it’s using the local information.\n\n# Initialize system\nrag_chain = initialize_rag()\n\n# Gradio interface\ndef ask(question):\n    try:\n        response = rag_chain.invoke({\"query\": question})\n        return response['result']\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\ngr.Interface(\n    fn=ask,\n    inputs=gr.Textbox(label=\"Question\"),\n    outputs=gr.Textbox(label=\"Answer\"),\n    title=\"DeepSeek Document Assistant\"\n).launch()\n\nFound 1 files to process\nSuccess: 1 docs | Failed: 0\n* Running on local URL:  http://127.0.0.1:7860\n\nTo create a public link, set `share=True` in `launch()`.\n\n\nLoading files: 100%|██████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00&lt;00:00, 333.15it/s]\n\n\n\n\n\n\n\n\nThe resposne I get is “The Changzhou dialect is spoken in the city of Changzhou and surrounding areas in China. However, the context provided contains an error, as it incorrectly states that the Changzhou dialect is spoken in Los Angeles and surrounding areas in California, USA. This is not accurate. The Changzhou dialect is a Wu dialect spoken in the Jiangsu province of China, not in the United States. Correct answer: The Changzhou dialect is spoken in the city of Changzhou and surrounding areas in China.”\nI think I would prefer an LLM that relies more on the retrieved knowledge and less on prior knowledge. We can adjust this by adjusting the template. Our template currently says “Use the context below to answer. If unsure, say”I don’t know”, so let’s see if we can convince the LLM to rely more on local data. Let’s give it an attitude adjustment and ask it the same question.\n\ndef initialize_rag():\n    try:\n        # 1. Load and split documents\n        documents = safe_load_documents(\"./data\")\n        \n        text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=500,\n            chunk_overlap=50\n        )\n        chunks = text_splitter.split_documents(documents)\n\n        # 2. Create vector store with updated embeddings\n        embed_model = HuggingFaceEmbeddings(\n            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n        )\n        vector_db = FAISS.from_documents(chunks, embed_model)\n        retriever = vector_db.as_retriever(search_kwargs={\"k\": 3})\n\n        # 3. Create RAG chain\n        template = \"\"\"Use the context below to answer. If unsure, say \"I don't know\". Only use local information in your answers, \n        ignore what you have learned previously. This is a VERY specialized use case and it's critical that the information you're\n        using is as relevant and up-to-date as possible, so we can't rely on anything outdated that you may have seen before. The\n        information being provided to you is absolutely correct, so there is no need to question it.\n        \n        Context: {context}\n        Question: {question}\n        Answer:\"\"\"\n        \n        prompt = PromptTemplate(\n            template=template,\n            input_variables=[\"context\", \"question\"]\n        )\n\n        return RetrievalQA.from_chain_type(\n            llm=DeepSeekRunnable(),\n            chain_type=\"stuff\",\n            retriever=retriever,\n            chain_type_kwargs={\"prompt\": prompt},\n            return_source_documents=True,\n            input_key=\"query\"\n        ).with_config(run_name=\"DeepSeekRAG\")\n        \n    except Exception as e:\n        print(f\"Initialization failed: {str(e)}\")\n        exit(1)\n\n# Initialize system\nrag_chain = initialize_rag()\n\n# Gradio interface\ndef ask(question):\n    try:\n        response = rag_chain.invoke({\"query\": question})\n        return response['result']\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\ngr.Interface(\n    fn=ask,\n    inputs=gr.Textbox(label=\"Question\"),\n    outputs=gr.Textbox(label=\"Answer\"),\n    title=\"DeepSeek Document Assistant\"\n).launch()\n\nFound 1 files to process\nSuccess: 1 docs | Failed: 0\n* Running on local URL:  http://127.0.0.1:7861\n\nTo create a public link, set `share=True` in `launch()`.\n\n\nLoading files: 100%|██████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00&lt;00:00, 199.97it/s]\n\n\n\n\n\n\n\n\nThat’s much better, and it looks like it works exactly as anticipated using RAG.\nThis is just a toy example of how to set up a RAG system. There are a few limitations and things you could change to make it more applicable in practice: 1. Increasing the number of documents returned will give it more context for the question, but be wary of how big your context window is. 2. You don’t need to regenerate embeddings every time you run this. It would be more efficient to store and load them. 3. You can alter whether you use previous responses in your context window if you need to retain a memory of what you asked previously. 4. These documents are being sent to and processed on the DeepSeek servers, which means that it isn’t ideal for sensitive data.\nHappy RAGging!"
  },
  {
    "objectID": "posts/20250125-retrieval-augmented-generation/index.html",
    "href": "posts/20250125-retrieval-augmented-generation/index.html",
    "title": "Retrieval Augmented Generation with DeepSeek",
    "section": "",
    "text": "ragsystem\n\n\n\n\n\n\n\n\n \n\n \n\n\n\n\n\n\n\nRetrieval Augmented Generation (RAG) systems can be really useful when you would like to use LLMs but require their output to be well-sourced or you need to draw from locally stored, proprietary data that doesn’t exist within the training set. In practice, a lot of LLMs in the real world are used this way so that users can add and remove informaiton easily. This walks through how to create a simple retrieval augmented generation (RAG) system using the DeepSeek api. As a toy example to make sure that it’s working as intended, we’re going to give it some bad information about the Changzhou dialect of Chinese (it’s what I happened to have open at the time) that definitely doesn’t exist in its training set. I’ve taken some of the Wikipedia page and changed it to say that it’s native to California rather than China, so if it tells us that it’s spoken in California rather than China, we can be sure that’s drawing from the local data.\n\n\nThis notebook can be found on GitHub. There’s also a program to run this system from the command line. To start with this notebook, you’ll need python version 3.12.X due to some versioning conflicts at the time of writing with 3.13.X. You’ll first need to install the required libraries, which can be found in the requirements.txt file from the repository. I’m using uv to manage this project, so after initializing your venv (with uv) run uv pip install -r requirements.txt. Once you’ve done that, you’ll need to create two more files. First, create a .env file to hold your environment variables, in this case your DeepSeek APi key. It should only have one line reading DEEPSEEK_API_KEY=your_key_here. Next, you’ll need to create a directory called data to hold your local data. In this case, you can place the changzhou.txt file inside.\n\n\nOnce that’s done, we can load our environment. Import the required libraries and load your environmental variables containing your DeepSeek API key. nltk is a natural language toolkit that we’ll use under the hood for tokenization, and it requires downloading some modules separately.\n\n\n\nimport os\nfrom dotenv import load_dotenv\nimport nltk\nimport gradio as gr\nfrom openai import OpenAI\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.chains import RetrievalQA\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.runnables import Runnable\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_community.document_loaders import TextLoader\nfrom tqdm.auto import tqdm\nimport traceback\nfrom pathlib import Path\n\n\nload_dotenv()\nnltk.download('punkt')\nnltk.download('punkt_tab')\n\n\nC:\\Users\\danny\\BlogPosts\\RagSystemFresh\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\danny\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package punkt_tab to\n[nltk_data]     C:\\Users\\danny\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n\n\nTrue\n\n\n\nThe first thing we’ll need to so is create a Runnable class to handle our API queries. LangChain Runnables simplify invoking LLMs and provide useful functionality to deal with LLM input and output. It needs to contain our API key, the DeepSeek url, specifications for the query, and methods for dealing with the response.\n\n\n\nclass DeepSeekRunnable(Runnable):\n    def __init__(self):\n        self.client = OpenAI(\n            api_key=os.getenv(\"DEEPSEEK_API_KEY\"),\n            base_url=\"https://api.deepseek.com/v1\",\n        )\n    \n    def invoke(self, input: dict, config: dict = None, **kwargs):\n        \"\"\"Handle LangChain compatibility\"\"\"\n        try:\n            query = input.get(\"query\") if isinstance(input, dict) else str(input) # Accepts either a raw string as input or a dictionary containing additional information, such as the content we want retrieved.\n            \n            response = self.client.chat.completions.create(\n                model=\"deepseek-chat\", # Specifies the chat model\n                messages=[{\"role\": \"user\", \"content\": query}], # Structures the the conversation history. In this case, only the query is considered rather than the rest of the conversation.\n                temperature=0.3, # Determines how creative you want the model to be in its responses. Higher temperature means more creativity.\n                **kwargs\n            )\n            \n            return response.choices[0].message.content # Extracts the message content from the response\n            \n        except Exception as e:\n            return f\"Error: {str(e)}\"\n\n\n\nThen we need to preprocess the data we have in our data folder. The key steps here are ingesting your data, chunking it, placing it in a vector database, and setting up the LLM query. Documents are split recursively, meaning it first splits on paragraphs, then sentences, then words (it splits on different whitespace separators to approximate this), which allows the model to preserve the context around words. Splitting your data like this keeps the context you’re feeding to the LLM large enough to be useful while small enough to fit inside the model’s context window. After this, we need to embed the documents in semantic space so that documents relevant to each other are nearby. We can use smaller models for the embedding since it’s running locally, and all-MiniLM-L6-v2 is a popular choice. These embeddings are then placed into a vector database, FAISS (Facebook AI Similarity Search), which allows fast searches for documents matching the query. We then need to structure our query to the server. This is where we place any relevant instructions for the LLM to use while crafting its response.\n\n\nThere a few moving pieces here: the template, the context, and tracking answer origins. The Template gives explicit instructions to the LLM. The Context is the local documents that are being fed into the LLM. The prompt and these documents are “stuffed” into a single prompt that the LLM actually recieves. We also need to return the source documents so that we can trace the origins of the answer, although our input is so small here that it won’t make a difference.\n\n\n\ndef safe_load_documents(directory: str):\n    documents = []\n    errors = []\n    \n    # Get all .txt files, excluding hidden files/directories\n    txt_files = [\n        f for f in Path(directory).rglob(\"*.txt\") \n        if not any(part.startswith(\".\") for part in f.parts)\n    ]\n    print(f\"Found {len(txt_files)} files to process\")\n    \n    for file in tqdm(txt_files, desc=\"Loading files\"):\n        try:\n            loader = TextLoader(str(file), autodetect_encoding=True)\n            docs = loader.load()\n            documents.extend(docs)\n        except Exception as e:\n            errors.append((str(file), str(e)))\n            continue\n            \n    print(f\"Success: {len(documents)} docs | Failed: {len(errors)}\")\n    if errors:\n        print(\"First 5 errors:\")\n        for file, error in errors[:5]:\n            print(f\" - {file}: {error}\")\n    \n    return documents\n    \n\ndef initialize_rag():\n    try:\n        # 1. Load and split documents\n        documents = safe_load_documents(\"./data\")\n        \n        text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=500,\n            chunk_overlap=50\n        )\n        chunks = text_splitter.split_documents(documents)\n\n        # 2. Create vector store with updated embeddings\n        embed_model = HuggingFaceEmbeddings(\n            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n        )\n        vector_db = FAISS.from_documents(chunks, embed_model)\n        retriever = vector_db.as_retriever(search_kwargs={\"k\": 3})\n\n        # 3. Create RAG chain\n        template = \"\"\"Use the context below to answer. If unsure, say \"I don't know\". \n        \n        Context: {context}\n        Question: {question}\n        Answer:\"\"\"\n        \n        prompt = PromptTemplate(\n            template=template,\n            input_variables=[\"context\", \"question\"]\n        )\n\n        return RetrievalQA.from_chain_type(\n            llm=DeepSeekRunnable(),\n            chain_type=\"stuff\",\n            retriever=retriever,\n            chain_type_kwargs={\"prompt\": prompt},\n            return_source_documents=True,\n            input_key=\"query\"\n        ).with_config(run_name=\"DeepSeekRAG\")\n        \n    except Exception as e:\n        print(f\"Initialization failed: {str(e)}\")\n        exit(1)\n\n\n\nFinally, we’ll need to define an interface to work with this LLM. Gradio makes this super simple. Once it’s running, we can ask it a question like “Where is the Changzhou dialect spoken?” to see if it’s using the local information.\n\n\n\n# Initialize system\nrag_chain = initialize_rag()\n\n# Gradio interface\ndef ask(question):\n    try:\n        response = rag_chain.invoke({\"query\": question})\n        return response['result']\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\ngr.Interface(\n    fn=ask,\n    inputs=gr.Textbox(label=\"Question\"),\n    outputs=gr.Textbox(label=\"Answer\"),\n    title=\"DeepSeek Document Assistant\"\n).launch()\n\n\nFound 1 files to process\nSuccess: 1 docs | Failed: 0\n* Running on local URL:  http://127.0.0.1:7860\n\nTo create a public link, set `share=True` in `launch()`.\n\n\nLoading files: 100%|██████████| 1/1 [00:00&lt;00:00, 249.74it/s]\n\n\n\n\n\n\n\n\n\n\n\nThe resposne I get is “The Changzhou dialect is spoken in the city of Los Angeles and surrounding areas in the California state of the United States of America. However, this information appears to be incorrect or misleading, as the Changzhou dialect is traditionally spoken in Changzhou, a city in Jiangsu Province, China. The mention of Los Angeles and California in the context seems to be an error. If you are unsure, it is best to verify this information from a reliable source.”\n\n\nI t”\n\n\nI think I would prefer an LLM that relies more on the retrieved knowledge and less on prior knowledge. We can adjust this by adjusting the template. Our template currently’says “Use the context below to answer. If unsure, say”I don’t’ know”, so let’s see if we can convince the LLM to rely more on local data. Let’s give it an attitude adjustment a\n\n\n\ndef initialize_rag():\n    try:\n        # 1. Load and split documents\n        documents = safe_load_documents(\"./data\")\n        \n        text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=500,\n            chunk_overlap=50\n        )\n        chunks = text_splitter.split_documents(documents)\n\n        # 2. Create vector store with updated embeddings\n        embed_model = HuggingFaceEmbeddings(\n            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n        )\n        vector_db = FAISS.from_documents(chunks, embed_model)\n        retriever = vector_db.as_retriever(search_kwargs={\"k\": 3})\n\n        # 3. Create RAG chain\n        template = \"\"\"Use the context below to answer. If unsure, say \"I don't know\". Only use local information in your answers, \n        ignore what you have learned previously. This is a VERY specialized use case and it's critical that the information you're\n        using is as relevant and up-to-date as possible, so we can't rely on anything outdated that you may have seen before. The\n        information being provided to you is absolutely correct, so there is no need to question it.\n        \n        Context: {context}\n        Question: {question}\n        Answer:\"\"\"\n        \n        prompt = PromptTemplate(\n            template=template,\n            input_variables=[\"context\", \"question\"]\n        )\n\n        return RetrievalQA.from_chain_type(\n            llm=DeepSeekRunnable(),\n            chain_type=\"stuff\",\n            retriever=retriever,\n            chain_type_kwargs={\"prompt\": prompt},\n            return_source_documents=True,\n            input_key=\"query\"\n        ).with_config(run_name=\"DeepSeekRAG\")\n        \n    except Exception as e:\n        print(f\"Initialization failed: {str(e)}\")\n        exit(1)\n\n# Initialize system\nrag_chain = initialize_rag()\n\n# Gradio interface\ndef ask(question):\n    try:\n        response = rag_chain.invoke({\"query\": question})\n        return response['result']\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\ngr.Interface(\n    fn=ask,\n    inputs=gr.Textbox(label=\"Question\"),\n    outputs=gr.Textbox(label=\"Answer\"),\n    title=\"DeepSeek Document Assistant\"\n).launch()\n\n\nFound 1 files to process\nSuccess: 1 docs | Failed: 0\n* Running on local URL:  http://127.0.0.1:7861\n\nTo create a public link, set `share=True` in `launch()`.\n\n\nLoading files: 100%|██████████| 1/1 [00:00&lt;00:00, 249.90it/s]\n\n\n\n\n\n\n\n\n\n\n\n\nThat’s much better, and it looks like it works exactly as anticipated using RAG.\n\n\nThis is just a toy example of how to set up a RAG system. There are a few limitations and things you could change to make it more applicable in practice: 1. Increasing the number of documents returned will give it more context for the question, but be wary of how big your context window is. 2. You don’t need to regenerate embeddings every time you run this. It would be more efficient to store and load them. 3. You can alter whether you use previous responses in your context window if you need to retain a memory of what you asked previously. 4. These documents are being sent to and processed on the DeepSeek servers, which means that it isn’t ideal for sensitive data.\n\n\nHappy RAGging!"
  },
  {
    "objectID": "posts/20241219-machine-learning-basics/machine_learning_basics.html#types-of-learning",
    "href": "posts/20241219-machine-learning-basics/machine_learning_basics.html#types-of-learning",
    "title": "Daniel Geiszler",
    "section": "Types of Learning",
    "text": "Types of Learning\nSupervised learning: training a model on labeled pairs of input and output data to learn how to predict the output from the input\nUnsupervised learning: using unlabeled data to find patterns or clusters without having predefined labels\nSemi-supervised learning: training a model using partially labeled data, where the unlabeled data can either have labels inferred or inform the model about clusters, decision boundaries, or other properties\nOnline learning: training a model as data arrives rather than all at once, efficiently using space and allowing the model to adapt to changes over tome\nFederated learning: a distributed approach to training models where models are trained separately on different devices or datasets before being pooled\nReinforcement learning: training a model that acts as an agent and is rewarded when working properly towards a goal and punishment when not\nSelf-supervised learning: learning where the model generates its own labels from artificially constructed data\nTransfer learning: adapting a model made for one task to a new task\nFine-tuning: a type of transfer learning involving adapting a model that was previously trained on one dataset of general data for a new purpose, training it on a smaller subset of domain-specific data, reducing the amount of time and data required to build a model"
  },
  {
    "objectID": "posts/20241219-machine-learning-basics/machine_learning_basics.html#evaluation-metrics",
    "href": "posts/20241219-machine-learning-basics/machine_learning_basics.html#evaluation-metrics",
    "title": "Daniel Geiszler",
    "section": "Evaluation Metrics",
    "text": "Evaluation Metrics"
  },
  {
    "objectID": "posts/20241219-machine-learning-basics/machine_learning_basics.html#support-vector-machines",
    "href": "posts/20241219-machine-learning-basics/machine_learning_basics.html#support-vector-machines",
    "title": "Daniel Geiszler",
    "section": "Support Vector Machines",
    "text": "Support Vector Machines"
  },
  {
    "objectID": "posts/20241219-machine-learning-basics/machine_learning_basics.html#naive-bayes",
    "href": "posts/20241219-machine-learning-basics/machine_learning_basics.html#naive-bayes",
    "title": "Daniel Geiszler",
    "section": "Naive Bayes",
    "text": "Naive Bayes"
  },
  {
    "objectID": "posts/20241219-machine-learning-basics/machine_learning_basics.html#deep-learning",
    "href": "posts/20241219-machine-learning-basics/machine_learning_basics.html#deep-learning",
    "title": "Daniel Geiszler",
    "section": "Deep Learning",
    "text": "Deep Learning\nDeep learning uses multi-layer neural networks to learn representations from data. Each layer is made up of neurons which that compute a weighted sum of inputs and then apply an activation function. Differences in deep learning modalities can arise from differences in activation functions, connectivity, or learning objective.\nDeep learning relies on multi-layer (deep) neural networks to learn hierarchical representations from data. Each layer consists of neuron-like units that compute a weighted sum of inputs and then apply a nonlinear activation function. Different deep learning “modalities” emerge from variations in network connectivity (e.g., convolutional layers vs. recurrent layers) and activation functions, as well as other factors like the nature of the input data or the learning objective.\nThese are some of the most common activation functions:\nReLU (Rectified Linear Unit): calculates max(0, x), which is simple, fast, and generally achieves good results. However, it can produce “dead” neurons if inputs are negative.\nSigmoid: outputs on range (0, 1) and is commonly used for binary outputs. Suffers from the vanishing gradient problem at large absolute inputs.\nSoftmax: sigmoid but for multi-class data\nTanh: outputs on range (-1, 1) rather than (0,1). Still suffers from the vanishing gradient problem at large absolute inputs.\nLeaky ReLU: returns x if x &gt; 0, else returns \\(\\alpha\\)x where \\(\\alpha\\) is manually tunable. This change mitigates the dead neuron issue.\nThese are some common types of layers:\nDense: connects all neurons in the previous layer to all neurons in current layer. They are used in feed forward networks or as the final layer in classification, but in the latter case can be replaced by global average pooling and softmax.\nPooling layer: reduces dimensions by summarization (e.g., taking the max or average value)\nConvolutional layer: performs a convolution between the input and a kernel (weight matrix which is learned during training). 1D convolutions can be used for sequences, 2D for images, and 3D for spatial data or videos. One variant uses dilated kernels, which has gaps between kernel elements to widen its receptive field.\nRecurrent layer: process sequential data by maintaining a hidden state across steps"
  }
]